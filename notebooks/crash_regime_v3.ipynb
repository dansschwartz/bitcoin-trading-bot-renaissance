{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "name": "crash_regime_v3.ipynb"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Setup, imports, mount Drive, define crash periods\n",
    "# =============================================================================\n",
    "\n",
    "import subprocess, sys\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install dependencies\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q',\n",
    "                       'lightgbm', 'yfinance'])\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Directories\n",
    "DRIVE_DIR = '/content/drive/MyDrive/renaissance-bot-training/crash_models_v3'\n",
    "LOCAL_DIR = '/content/data'\n",
    "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
    "\n",
    "# Crash period definitions\n",
    "CRASH_PERIODS = [\n",
    "    ('2018-01-07', '2018-12-15', 'ICO Bust'),\n",
    "    ('2021-11-10', '2022-11-21', 'Terra/Luna + FTX + Fed'),\n",
    "    ('2025-10-06', '2026-02-28', 'Current macro crash'),\n",
    "]\n",
    "\n",
    "print('[OK] Setup complete')\n",
    "print(f'Drive dir: {DRIVE_DIR}')\n",
    "print(f'Local dir: {LOCAL_DIR}')\n",
    "print()\n",
    "print('Crash periods:')\n",
    "for i, (s, e, name) in enumerate(CRASH_PERIODS, 1):\n",
    "    print(f'  Crash {i}: {s} to {e} -- {name}')\n",
    "print()\n",
    "print(f'[OK] LightGBM version: {lgb.__version__}')\n",
    "print(f'[OK] Current time (UTC): {datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Download BTC and ETH 5-minute candles from Binance\n",
    "# =============================================================================\n",
    "\n",
    "BASE_URL = \"https://data-api.binance.vision/api/v3/klines\"\n",
    "\n",
    "KLINE_COLS = [\n",
    "    'open_time', 'open', 'high', 'low', 'close', 'volume',\n",
    "    'close_time', 'quote_volume', 'trades',\n",
    "    'taker_buy_base', 'taker_buy_quote', 'ignore'\n",
    "]\n",
    "\n",
    "NUMERIC_COLS = ['open', 'high', 'low', 'close', 'volume',\n",
    "                'quote_volume', 'taker_buy_base', 'taker_buy_quote']\n",
    "\n",
    "\n",
    "def download_binance_klines(symbol, start_date, interval='5m'):\n",
    "    \"\"\"Download klines from Binance data-api with pagination.\"\"\"\n",
    "    start_ts = int(pd.Timestamp(start_date).timestamp() * 1000)\n",
    "    now_ts = int(datetime.utcnow().timestamp() * 1000)\n",
    "\n",
    "    all_rows = []\n",
    "    current_ts = start_ts\n",
    "    total_fetched = 0\n",
    "    retries = 0\n",
    "    max_retries = 5\n",
    "\n",
    "    print(f'Downloading {symbol} {interval} from {start_date} ...')\n",
    "\n",
    "    while current_ts < now_ts:\n",
    "        params = {\n",
    "            'symbol': symbol,\n",
    "            'interval': interval,\n",
    "            'startTime': current_ts,\n",
    "            'limit': 1000\n",
    "        }\n",
    "        try:\n",
    "            resp = requests.get(BASE_URL, params=params, timeout=30)\n",
    "            if resp.status_code in (451, 403):\n",
    "                retries += 1\n",
    "                if retries > max_retries:\n",
    "                    print(f'[WARN] Too many 451/403 errors, stopping at {total_fetched} candles')\n",
    "                    break\n",
    "                print(f'[WARN] HTTP {resp.status_code}, retry {retries}/{max_retries}, sleeping 5s ...')\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            if not data:\n",
    "                break\n",
    "\n",
    "            retries = 0  # reset on success\n",
    "            all_rows.extend(data)\n",
    "            total_fetched += len(data)\n",
    "\n",
    "            # Progress\n",
    "            if total_fetched % 100000 < 1000:\n",
    "                dt_str = pd.Timestamp(data[-1][0], unit='ms').strftime('%Y-%m-%d')\n",
    "                print(f'  {total_fetched:>9,} candles ... last date: {dt_str}')\n",
    "\n",
    "            # Advance past the last candle\n",
    "            current_ts = data[-1][0] + 1\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            retries += 1\n",
    "            if retries > max_retries:\n",
    "                print(f'[WARN] Too many timeouts, stopping at {total_fetched} candles')\n",
    "                break\n",
    "            print(f'[WARN] Timeout, retry {retries}/{max_retries}, sleeping 5s ...')\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            if retries > max_retries:\n",
    "                print(f'[WARN] Error: {e}, stopping at {total_fetched} candles')\n",
    "                break\n",
    "            print(f'[WARN] Error: {e}, retry {retries}/{max_retries}, sleeping 5s ...')\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "\n",
    "    print(f'[OK] Total: {total_fetched:,} candles for {symbol}')\n",
    "\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame(columns=KLINE_COLS)\n",
    "\n",
    "    df = pd.DataFrame(all_rows, columns=KLINE_COLS)\n",
    "    for col in NUMERIC_COLS:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df['trades'] = pd.to_numeric(df['trades'], errors='coerce').astype(int)\n",
    "    df['timestamp'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    df = df.drop_duplicates(subset=['open_time']).sort_values('open_time').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Download BTC\n",
    "btc_df = download_binance_klines('BTCUSDT', '2017-09-01')\n",
    "print(f'BTC shape: {btc_df.shape}, date range: {btc_df[\"timestamp\"].min()} to {btc_df[\"timestamp\"].max()}')\n",
    "\n",
    "# Download ETH\n",
    "eth_df = download_binance_klines('ETHUSDT', '2017-09-01')\n",
    "print(f'ETH shape: {eth_df.shape}, date range: {eth_df[\"timestamp\"].min()} to {eth_df[\"timestamp\"].max()}')\n",
    "\n",
    "# Save to local and Drive\n",
    "for name, df in [('btc_5m_full', btc_df), ('eth_5m_full', eth_df)]:\n",
    "    local_path = os.path.join(LOCAL_DIR, f'{name}.csv')\n",
    "    drive_path = os.path.join(DRIVE_DIR, f'{name}.csv')\n",
    "    df.to_csv(local_path, index=False)\n",
    "    df.to_csv(drive_path, index=False)\n",
    "    print(f'[OK] Saved {name}: {len(df):,} rows -> {drive_path}')\n",
    "\n",
    "print()\n",
    "print('[DONE] BTC and ETH candle download complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Download macro data (daily) + Fear & Greed\n",
    "# =============================================================================\n",
    "\n",
    "MACRO_TICKERS = {\n",
    "    '^GSPC': 'spx',\n",
    "    '^VIX': 'vix',\n",
    "    'DX-Y.NYB': 'dxy',\n",
    "    '^IXIC': 'ndx',\n",
    "    '^TNX': 'us10y',\n",
    "    'GC=F': 'gold',\n",
    "}\n",
    "\n",
    "macro_frames = {}\n",
    "\n",
    "for ticker, short_name in MACRO_TICKERS.items():\n",
    "    print(f'Downloading {ticker} ({short_name}) ...')\n",
    "    try:\n",
    "        data = yf.download(ticker, start='2017-01-01', progress=False)\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            data.columns = data.columns.get_level_values(0)\n",
    "        if len(data) == 0:\n",
    "            print(f'  [WARN] No data for {ticker}')\n",
    "            continue\n",
    "        # Keep Close column renamed\n",
    "        series = data['Close'].copy()\n",
    "        series.name = short_name\n",
    "        macro_frames[short_name] = series\n",
    "        print(f'  [OK] {short_name}: {len(series)} days, {series.index.min().date()} to {series.index.max().date()}')\n",
    "    except Exception as e:\n",
    "        print(f'  [WARN] Failed to download {ticker}: {e}')\n",
    "\n",
    "# Merge all macro into one DataFrame\n",
    "if macro_frames:\n",
    "    macro_df = pd.DataFrame(macro_frames)\n",
    "    macro_df.index.name = 'date'\n",
    "    macro_df = macro_df.ffill().bfill()\n",
    "    print(f'\\n[OK] Macro DataFrame: {macro_df.shape}')\n",
    "    print(macro_df.tail(3))\n",
    "else:\n",
    "    macro_df = pd.DataFrame()\n",
    "    print('[WARN] No macro data downloaded')\n",
    "\n",
    "# Fear & Greed Index\n",
    "print('\\nDownloading Fear & Greed Index ...')\n",
    "try:\n",
    "    fng_resp = requests.get('https://api.alternative.me/fng/?limit=0', timeout=30)\n",
    "    fng_data = fng_resp.json()['data']\n",
    "    fng_df = pd.DataFrame(fng_data)\n",
    "    fng_df['date'] = pd.to_datetime(fng_df['timestamp'].astype(int), unit='s')\n",
    "    fng_df['fng_value'] = pd.to_numeric(fng_df['value'], errors='coerce')\n",
    "    fng_df = fng_df[['date', 'fng_value']].sort_values('date').reset_index(drop=True)\n",
    "    fng_df = fng_df.drop_duplicates(subset=['date'])\n",
    "    print(f'[OK] Fear & Greed: {len(fng_df)} days')\n",
    "except Exception as e:\n",
    "    print(f'[WARN] Fear & Greed download failed: {e}')\n",
    "    fng_df = pd.DataFrame(columns=['date', 'fng_value'])\n",
    "\n",
    "# Save\n",
    "for name, df in [('macro_daily', macro_df), ('fear_greed', fng_df)]:\n",
    "    local_path = os.path.join(LOCAL_DIR, f'{name}.csv')\n",
    "    drive_path = os.path.join(DRIVE_DIR, f'{name}.csv')\n",
    "    df.to_csv(local_path)\n",
    "    df.to_csv(drive_path)\n",
    "    print(f'[OK] Saved {name} -> {drive_path}')\n",
    "\n",
    "print()\n",
    "print('[DONE] Macro and Fear & Greed download complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Download Binance derivatives data (OPTIONAL)\n",
    "# =============================================================================\n",
    "\n",
    "FAPI_BASE = \"https://fapi.binance.com\"\n",
    "\n",
    "\n",
    "def download_paginated(url, params_base, time_key='startTime', limit_key='limit',\n",
    "                       limit=1000, max_pages=200, label='data'):\n",
    "    \"\"\"Generic paginated downloader for Binance futures endpoints.\"\"\"\n",
    "    all_data = []\n",
    "    params = dict(params_base)\n",
    "    params[limit_key] = limit\n",
    "\n",
    "    for page in range(max_pages):\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, timeout=30)\n",
    "            if resp.status_code in (451, 403):\n",
    "                print(f'  [WARN] HTTP {resp.status_code} on page {page} for {label}, stopping')\n",
    "                break\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            if not data:\n",
    "                break\n",
    "            all_data.extend(data)\n",
    "\n",
    "            # Find max timestamp to paginate forward\n",
    "            if isinstance(data[-1], dict):\n",
    "                last_ts = None\n",
    "                for key in ['fundingTime', 'timestamp', 'time']:\n",
    "                    if key in data[-1]:\n",
    "                        last_ts = int(data[-1][key])\n",
    "                        break\n",
    "                if last_ts is None:\n",
    "                    break\n",
    "                params[time_key] = last_ts + 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            time.sleep(0.15)\n",
    "        except Exception as e:\n",
    "            print(f'  [WARN] Error on page {page} for {label}: {e}')\n",
    "            break\n",
    "\n",
    "    print(f'  [OK] {label}: {len(all_data)} records')\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# 1. Funding Rate (8h intervals, paginate from 2020-01-01)\n",
    "print('Downloading funding rate ...')\n",
    "funding_start = int(pd.Timestamp('2020-01-01').timestamp() * 1000)\n",
    "funding_raw = download_paginated(\n",
    "    f'{FAPI_BASE}/fapi/v1/fundingRate',\n",
    "    {'symbol': 'BTCUSDT', 'startTime': funding_start},\n",
    "    time_key='startTime', limit=1000, max_pages=300, label='funding_rate'\n",
    ")\n",
    "if funding_raw:\n",
    "    funding_df = pd.DataFrame(funding_raw)\n",
    "    funding_df['timestamp'] = pd.to_datetime(funding_df['fundingTime'], unit='ms')\n",
    "    funding_df['fundingRate'] = pd.to_numeric(funding_df['fundingRate'], errors='coerce')\n",
    "    funding_df = funding_df.sort_values('timestamp').drop_duplicates(subset=['fundingTime']).reset_index(drop=True)\n",
    "else:\n",
    "    funding_df = pd.DataFrame(columns=['timestamp', 'fundingRate'])\n",
    "\n",
    "# 2. Open Interest (4h intervals)\n",
    "print('Downloading open interest ...')\n",
    "oi_start = int(pd.Timestamp('2020-06-01').timestamp() * 1000)\n",
    "oi_raw = download_paginated(\n",
    "    f'{FAPI_BASE}/futures/data/openInterestHist',\n",
    "    {'symbol': 'BTCUSDT', 'period': '4h', 'startTime': oi_start},\n",
    "    time_key='startTime', limit=500, max_pages=300, label='open_interest'\n",
    ")\n",
    "if oi_raw:\n",
    "    oi_df = pd.DataFrame(oi_raw)\n",
    "    oi_df['timestamp'] = pd.to_datetime(oi_df['timestamp'], unit='ms')\n",
    "    for col in ['sumOpenInterest', 'sumOpenInterestValue']:\n",
    "        if col in oi_df.columns:\n",
    "            oi_df[col] = pd.to_numeric(oi_df[col], errors='coerce')\n",
    "    oi_df = oi_df.sort_values('timestamp').reset_index(drop=True)\n",
    "else:\n",
    "    oi_df = pd.DataFrame(columns=['timestamp', 'sumOpenInterest', 'sumOpenInterestValue'])\n",
    "\n",
    "# 3. Long/Short Account Ratio (1h)\n",
    "print('Downloading long/short ratio ...')\n",
    "ls_start = int(pd.Timestamp('2020-06-01').timestamp() * 1000)\n",
    "ls_raw = download_paginated(\n",
    "    f'{FAPI_BASE}/futures/data/globalLongShortAccountRatio',\n",
    "    {'symbol': 'BTCUSDT', 'period': '1h', 'startTime': ls_start},\n",
    "    time_key='startTime', limit=500, max_pages=300, label='long_short_ratio'\n",
    ")\n",
    "if ls_raw:\n",
    "    ls_df = pd.DataFrame(ls_raw)\n",
    "    ls_df['timestamp'] = pd.to_datetime(ls_df['timestamp'], unit='ms')\n",
    "    ls_df['longShortRatio'] = pd.to_numeric(ls_df['longShortRatio'], errors='coerce')\n",
    "    ls_df = ls_df.sort_values('timestamp').reset_index(drop=True)\n",
    "else:\n",
    "    ls_df = pd.DataFrame(columns=['timestamp', 'longShortRatio'])\n",
    "\n",
    "# 4. Taker Buy/Sell Volume (1h)\n",
    "print('Downloading taker buy/sell volume ...')\n",
    "tv_start = int(pd.Timestamp('2020-06-01').timestamp() * 1000)\n",
    "tv_raw = download_paginated(\n",
    "    f'{FAPI_BASE}/futures/data/takeBuySellVol',\n",
    "    {'symbol': 'BTCUSDT', 'period': '1h', 'startTime': tv_start},\n",
    "    time_key='startTime', limit=500, max_pages=300, label='taker_volume'\n",
    ")\n",
    "if tv_raw:\n",
    "    tv_df = pd.DataFrame(tv_raw)\n",
    "    tv_df['timestamp'] = pd.to_datetime(tv_df['timestamp'], unit='ms')\n",
    "    for col in ['buyVol', 'sellVol', 'buySellRatio']:\n",
    "        if col in tv_df.columns:\n",
    "            tv_df[col] = pd.to_numeric(tv_df[col], errors='coerce')\n",
    "    tv_df = tv_df.sort_values('timestamp').reset_index(drop=True)\n",
    "else:\n",
    "    tv_df = pd.DataFrame(columns=['timestamp', 'buyVol', 'sellVol', 'buySellRatio'])\n",
    "\n",
    "# Save all derivatives\n",
    "for name, df in [('funding_rate', funding_df), ('open_interest', oi_df),\n",
    "                  ('long_short_ratio', ls_df), ('taker_volume', tv_df)]:\n",
    "    local_path = os.path.join(LOCAL_DIR, f'{name}.csv')\n",
    "    drive_path = os.path.join(DRIVE_DIR, f'{name}.csv')\n",
    "    df.to_csv(local_path, index=False)\n",
    "    df.to_csv(drive_path, index=False)\n",
    "    print(f'[OK] Saved {name}: {len(df)} rows -> {drive_path}')\n",
    "\n",
    "print()\n",
    "print('[DONE] Derivatives download complete (any failures above are non-fatal)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: Label crash periods and merge all data\n",
    "# =============================================================================\n",
    "\n",
    "# 1. Label crash periods in BTC data\n",
    "btc = btc_df.copy()\n",
    "btc['is_crash'] = False\n",
    "for start_str, end_str, name in CRASH_PERIODS:\n",
    "    start = pd.Timestamp(start_str)\n",
    "    end = pd.Timestamp(end_str)\n",
    "    mask = (btc['timestamp'] >= start) & (btc['timestamp'] <= end)\n",
    "    btc.loc[mask, 'is_crash'] = True\n",
    "    count = mask.sum()\n",
    "    print(f'Crash period {start_str} to {end_str} ({name}): {count:,} candles')\n",
    "\n",
    "crash_data = btc[btc['is_crash']].copy().reset_index(drop=True)\n",
    "print(f'\\nTotal crash candles: {len(crash_data):,}')\n",
    "print(f'Total non-crash candles: {(~btc[\"is_crash\"]).sum():,}')\n",
    "\n",
    "# 2. Create date column for daily merges\n",
    "crash_data['date'] = crash_data['timestamp'].dt.date\n",
    "crash_data['date'] = pd.to_datetime(crash_data['date'])\n",
    "\n",
    "# 3. Merge daily macro onto crash data by date (forward-fill for weekends)\n",
    "if len(macro_df) > 0:\n",
    "    macro_merge = macro_df.copy()\n",
    "    macro_merge.index = pd.to_datetime(macro_merge.index)\n",
    "    # Reindex to all dates in crash data and forward-fill\n",
    "    all_dates = crash_data['date'].unique()\n",
    "    macro_reindexed = macro_merge.reindex(\n",
    "        macro_merge.index.union(pd.DatetimeIndex(all_dates))\n",
    "    ).sort_index().ffill().bfill()\n",
    "    macro_reindexed = macro_reindexed.loc[all_dates]\n",
    "    macro_reindexed = macro_reindexed.reset_index().rename(columns={'index': 'date'})\n",
    "    crash_data = crash_data.merge(macro_reindexed, on='date', how='left')\n",
    "    print(f'[OK] Merged macro data: {[c for c in macro_reindexed.columns if c != \"date\"]}')\n",
    "else:\n",
    "    print('[WARN] No macro data to merge')\n",
    "\n",
    "# 4. Merge Fear & Greed by date\n",
    "if len(fng_df) > 0:\n",
    "    fng_merge = fng_df.copy()\n",
    "    fng_merge['date'] = pd.to_datetime(fng_merge['date'])\n",
    "    crash_data = crash_data.merge(fng_merge[['date', 'fng_value']], on='date', how='left')\n",
    "    crash_data['fng_value'] = crash_data['fng_value'].ffill().bfill()\n",
    "    print(f'[OK] Merged Fear & Greed: {crash_data[\"fng_value\"].notna().sum():,} rows with FNG')\n",
    "else:\n",
    "    crash_data['fng_value'] = np.nan\n",
    "    print('[WARN] No Fear & Greed data')\n",
    "\n",
    "# 5. Merge funding rate via merge_asof (backward, 12h tolerance)\n",
    "if len(funding_df) > 0:\n",
    "    funding_merge = funding_df[['timestamp', 'fundingRate']].copy()\n",
    "    funding_merge = funding_merge.sort_values('timestamp').reset_index(drop=True)\n",
    "    crash_data = crash_data.sort_values('timestamp').reset_index(drop=True)\n",
    "    crash_data = pd.merge_asof(\n",
    "        crash_data, funding_merge,\n",
    "        on='timestamp', direction='backward',\n",
    "        tolerance=pd.Timedelta('12h')\n",
    "    )\n",
    "    print(f'[OK] Merged funding rate: {crash_data[\"fundingRate\"].notna().sum():,} rows')\n",
    "else:\n",
    "    crash_data['fundingRate'] = np.nan\n",
    "    print('[WARN] No funding rate data')\n",
    "\n",
    "# 6. Merge open interest via merge_asof (backward, 6h tolerance)\n",
    "if len(oi_df) > 0:\n",
    "    oi_merge = oi_df[['timestamp', 'sumOpenInterest', 'sumOpenInterestValue']].copy()\n",
    "    oi_merge = oi_merge.sort_values('timestamp').reset_index(drop=True)\n",
    "    crash_data = pd.merge_asof(\n",
    "        crash_data, oi_merge,\n",
    "        on='timestamp', direction='backward',\n",
    "        tolerance=pd.Timedelta('6h')\n",
    "    )\n",
    "    print(f'[OK] Merged open interest: {crash_data[\"sumOpenInterest\"].notna().sum():,} rows')\n",
    "else:\n",
    "    crash_data['sumOpenInterest'] = np.nan\n",
    "    crash_data['sumOpenInterestValue'] = np.nan\n",
    "    print('[WARN] No open interest data')\n",
    "\n",
    "# 7. Merge long/short ratio via merge_asof\n",
    "if len(ls_df) > 0:\n",
    "    ls_merge = ls_df[['timestamp', 'longShortRatio']].copy()\n",
    "    ls_merge = ls_merge.sort_values('timestamp').reset_index(drop=True)\n",
    "    crash_data = pd.merge_asof(\n",
    "        crash_data, ls_merge,\n",
    "        on='timestamp', direction='backward',\n",
    "        tolerance=pd.Timedelta('2h')\n",
    "    )\n",
    "    print(f'[OK] Merged long/short ratio: {crash_data[\"longShortRatio\"].notna().sum():,} rows')\n",
    "else:\n",
    "    crash_data['longShortRatio'] = np.nan\n",
    "    print('[WARN] No long/short ratio data')\n",
    "\n",
    "# 8. Merge taker volume via merge_asof\n",
    "if len(tv_df) > 0:\n",
    "    tv_merge = tv_df[['timestamp', 'buyVol', 'sellVol', 'buySellRatio']].copy()\n",
    "    tv_merge = tv_merge.sort_values('timestamp').reset_index(drop=True)\n",
    "    crash_data = pd.merge_asof(\n",
    "        crash_data, tv_merge,\n",
    "        on='timestamp', direction='backward',\n",
    "        tolerance=pd.Timedelta('2h')\n",
    "    )\n",
    "    print(f'[OK] Merged taker volume: {crash_data[\"buyVol\"].notna().sum():,} rows')\n",
    "else:\n",
    "    crash_data['buyVol'] = np.nan\n",
    "    crash_data['sellVol'] = np.nan\n",
    "    crash_data['buySellRatio'] = np.nan\n",
    "    print('[WARN] No taker volume data')\n",
    "\n",
    "# 9. Merge ETH data by exact timestamp match\n",
    "eth_merge = eth_df[['timestamp', 'close', 'volume']].copy()\n",
    "eth_merge = eth_merge.rename(columns={'close': 'eth_close', 'volume': 'eth_volume'})\n",
    "crash_data = crash_data.merge(eth_merge, on='timestamp', how='left')\n",
    "print(f'[OK] Merged ETH: {crash_data[\"eth_close\"].notna().sum():,} rows with ETH data')\n",
    "\n",
    "# 10. Print column overview with fill rates\n",
    "print(f'\\nColumn overview ({len(crash_data.columns)} columns, {len(crash_data):,} rows):')\n",
    "print('-' * 60)\n",
    "for col in crash_data.columns:\n",
    "    fill = crash_data[col].notna().mean() * 100\n",
    "    print(f'  {col:35s} fill: {fill:6.1f}%')\n",
    "\n",
    "# 11. Save\n",
    "save_path = os.path.join(DRIVE_DIR, 'crash_dataset_raw.csv')\n",
    "crash_data.to_csv(save_path, index=False)\n",
    "crash_data.to_csv(os.path.join(LOCAL_DIR, 'crash_dataset_raw.csv'), index=False)\n",
    "print(f'\\n[DONE] Saved crash_dataset_raw.csv: {len(crash_data):,} rows -> {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: Feature engineering (51 features)\n",
    "# =============================================================================\n",
    "\n",
    "df = crash_data.copy()\n",
    "df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# ---- GROUP 1: BTC Price/Volume (15 features) ----\n",
    "print('Building Group 1: BTC Price/Volume (15 features) ...')\n",
    "\n",
    "df['return_1bar'] = df['close'].pct_change(1)\n",
    "df['return_6bar'] = df['close'].pct_change(6)\n",
    "df['return_12bar'] = df['close'].pct_change(12)\n",
    "df['return_48bar'] = df['close'].pct_change(48)\n",
    "df['return_288bar'] = df['close'].pct_change(288)\n",
    "\n",
    "df['vol_12bar'] = df['return_1bar'].rolling(12).std()\n",
    "df['vol_48bar'] = df['return_1bar'].rolling(48).std()\n",
    "df['vol_ratio'] = df['vol_12bar'] / df['vol_48bar'].replace(0, np.nan)\n",
    "\n",
    "vol_mean = df['volume'].rolling(48).mean()\n",
    "df['volume_surge'] = df['volume'] / vol_mean.replace(0, np.nan)\n",
    "df['volume_trend'] = vol_mean.pct_change(12)\n",
    "\n",
    "# Consecutive red candles\n",
    "is_red = (df['close'] < df['open']).astype(int)\n",
    "streaks = is_red.copy()\n",
    "for i in range(1, len(streaks)):\n",
    "    if streaks.iloc[i] == 1:\n",
    "        streaks.iloc[i] = streaks.iloc[i-1] + 1\n",
    "df['consecutive_red'] = streaks / 12.0  # Normalize: 12 consecutive = 1.0\n",
    "\n",
    "# Drawdown from 24h high\n",
    "rolling_high_24h = df['high'].rolling(288).max()\n",
    "df['drawdown_24h'] = (df['close'] - rolling_high_24h) / rolling_high_24h.replace(0, np.nan)\n",
    "\n",
    "# RSI-14 normalized to [-1, 1]\n",
    "delta = df['close'].diff()\n",
    "gain = delta.clip(lower=0).rolling(14).mean()\n",
    "loss = (-delta.clip(upper=0)).rolling(14).mean()\n",
    "rs = gain / loss.replace(0, np.nan)\n",
    "rsi = 100 - (100 / (1 + rs))\n",
    "df['rsi_14_norm'] = (rsi - 50) / 50  # Map 0-100 to -1 to 1\n",
    "\n",
    "# Bollinger %B\n",
    "bb_mid = df['close'].rolling(20).mean()\n",
    "bb_std = df['close'].rolling(20).std()\n",
    "bb_upper = bb_mid + 2 * bb_std\n",
    "bb_lower = bb_mid - 2 * bb_std\n",
    "bb_range = (bb_upper - bb_lower).replace(0, np.nan)\n",
    "df['bb_pct_b'] = (df['close'] - bb_lower) / bb_range\n",
    "\n",
    "# VWAP distance\n",
    "vwap_num = (df['close'] * df['volume']).rolling(48).sum()\n",
    "vwap_den = df['volume'].rolling(48).sum().replace(0, np.nan)\n",
    "vwap = vwap_num / vwap_den\n",
    "df['vwap_distance'] = (df['close'] - vwap) / vwap.replace(0, np.nan)\n",
    "\n",
    "print('  [OK] 15 BTC features built')\n",
    "\n",
    "# ---- GROUP 2: Daily Macro (10 features) ----\n",
    "print('Building Group 2: Daily Macro (10 features) ...')\n",
    "\n",
    "if 'spx' in df.columns:\n",
    "    df['spx_return_1d'] = df.groupby('date')['spx'].transform('first')\n",
    "    df['spx_return_1d'] = df['spx_return_1d'].pct_change(288)  # 1 day = 288 bars\n",
    "    spx_sma_20 = df['spx'].rolling(288 * 20).mean()\n",
    "    df['spx_vs_sma'] = (df['spx'] - spx_sma_20) / spx_sma_20.replace(0, np.nan)\n",
    "else:\n",
    "    df['spx_return_1d'] = 0.0\n",
    "    df['spx_vs_sma'] = 0.0\n",
    "\n",
    "if 'vix' in df.columns:\n",
    "    df['vix_norm'] = df['vix'] / 30.0  # VIX 30 = 1.0\n",
    "    df['vix_change'] = df['vix'].pct_change(288)\n",
    "    df['vix_extreme'] = (df['vix'] > 30).astype(float)\n",
    "else:\n",
    "    df['vix_norm'] = 0.0\n",
    "    df['vix_change'] = 0.0\n",
    "    df['vix_extreme'] = 0.0\n",
    "\n",
    "if 'dxy' in df.columns:\n",
    "    df['dxy_return_1d'] = df['dxy'].pct_change(288)\n",
    "    dxy_sma = df['dxy'].rolling(288 * 20).mean()\n",
    "    df['dxy_trend'] = (df['dxy'] - dxy_sma) / dxy_sma.replace(0, np.nan)\n",
    "else:\n",
    "    df['dxy_return_1d'] = 0.0\n",
    "    df['dxy_trend'] = 0.0\n",
    "\n",
    "if 'us10y' in df.columns:\n",
    "    df['yield_level'] = df['us10y'] / 5.0  # 5% yield = 1.0\n",
    "    df['yield_change'] = df['us10y'].pct_change(288)\n",
    "else:\n",
    "    df['yield_level'] = 0.0\n",
    "    df['yield_change'] = 0.0\n",
    "\n",
    "if 'fng_value' in df.columns:\n",
    "    df['fng_norm'] = df['fng_value'] / 100.0  # 0-100 -> 0-1\n",
    "else:\n",
    "    df['fng_norm'] = 0.5\n",
    "\n",
    "print('  [OK] 10 macro features built')\n",
    "\n",
    "# ---- GROUP 3: Derivatives (9 features) ----\n",
    "print('Building Group 3: Derivatives (9 features) ...')\n",
    "\n",
    "has_derivatives = df['fundingRate'].notna().sum() > 100\n",
    "\n",
    "if has_derivatives:\n",
    "    fr = df['fundingRate'].fillna(0)\n",
    "    fr_mean = fr.rolling(288).mean()\n",
    "    fr_std = fr.rolling(288).std().replace(0, np.nan)\n",
    "    df['funding_z'] = (fr - fr_mean) / fr_std\n",
    "    df['funding_extreme_long'] = (df['funding_z'] > 2).astype(float)\n",
    "    df['funding_extreme_short'] = (df['funding_z'] < -2).astype(float)\n",
    "\n",
    "    if 'sumOpenInterest' in df.columns:\n",
    "        oi = df['sumOpenInterest'].fillna(method='ffill')\n",
    "        df['oi_change_1h'] = oi.pct_change(12)   # 12 bars = 1h\n",
    "        df['oi_change_4h'] = oi.pct_change(48)   # 48 bars = 4h\n",
    "        oi_pct = oi.pct_change(12).abs()\n",
    "        oi_thresh = oi_pct.rolling(288).quantile(0.95)\n",
    "        df['oi_spike'] = (oi_pct > oi_thresh).astype(float)\n",
    "    else:\n",
    "        df['oi_change_1h'] = 0.0\n",
    "        df['oi_change_4h'] = 0.0\n",
    "        df['oi_spike'] = 0.0\n",
    "\n",
    "    if 'longShortRatio' in df.columns:\n",
    "        lsr = df['longShortRatio'].fillna(1.0)\n",
    "        df['ls_ratio_norm'] = (lsr - 1.0) / 0.5  # 1.0 = neutral\n",
    "        df['ls_extreme_long'] = (lsr > 2.0).astype(float)\n",
    "    else:\n",
    "        df['ls_ratio_norm'] = 0.0\n",
    "        df['ls_extreme_long'] = 0.0\n",
    "\n",
    "    if 'buyVol' in df.columns and 'sellVol' in df.columns:\n",
    "        buy = df['buyVol'].fillna(0)\n",
    "        sell = df['sellVol'].fillna(0)\n",
    "        total = (buy + sell).replace(0, np.nan)\n",
    "        df['taker_imbalance'] = (buy - sell) / total\n",
    "    else:\n",
    "        df['taker_imbalance'] = 0.0\n",
    "else:\n",
    "    print('  [WARN] Insufficient derivatives data, filling with zeros')\n",
    "    for feat in ['funding_z', 'funding_extreme_long', 'funding_extreme_short',\n",
    "                 'oi_change_1h', 'oi_change_4h', 'oi_spike',\n",
    "                 'ls_ratio_norm', 'ls_extreme_long', 'taker_imbalance']:\n",
    "        df[feat] = 0.0\n",
    "\n",
    "print('  [OK] 9 derivatives features built')\n",
    "\n",
    "# ---- GROUP 4: Intraday Macro (11 features) - ALL ZEROS (placeholders) ----\n",
    "print('Building Group 4: Intraday Macro (11 placeholders) ...')\n",
    "\n",
    "intraday_macro_cols = [\n",
    "    'spx_return_5m', 'spx_return_15m', 'spx_return_1h',\n",
    "    'spx_momentum_5m', 'spx_direction_5m',\n",
    "    'vix_return_5m', 'vix_return_1h', 'vix_spike_5m',\n",
    "    'ndx_return_5m', 'ndx_return_1h',\n",
    "    'has_intraday_macro'\n",
    "]\n",
    "for col in intraday_macro_cols:\n",
    "    df[col] = 0.0\n",
    "\n",
    "print('  [OK] 11 intraday macro placeholders set to 0.0')\n",
    "\n",
    "# ---- GROUP 5: Cross-Asset (6 features) ----\n",
    "print('Building Group 5: Cross-Asset (6 features) ...')\n",
    "\n",
    "if 'eth_close' in df.columns and df['eth_close'].notna().sum() > 100:\n",
    "    df['eth_return_1bar'] = df['eth_close'].pct_change(1)\n",
    "    df['eth_return_6bar'] = df['eth_close'].pct_change(6)\n",
    "    btc_eth_ratio = df['close'] / df['eth_close'].replace(0, np.nan)\n",
    "    df['eth_btc_ratio_change'] = btc_eth_ratio.pct_change(12)\n",
    "else:\n",
    "    df['eth_return_1bar'] = 0.0\n",
    "    df['eth_return_6bar'] = 0.0\n",
    "    df['eth_btc_ratio_change'] = 0.0\n",
    "\n",
    "# BTC leads: how much did BTC move in the prior 1/2/3 bars\n",
    "df['btc_lead_1'] = df['return_1bar'].shift(1)\n",
    "df['btc_lead_2'] = df['return_1bar'].shift(2)\n",
    "df['btc_lead_3'] = df['return_1bar'].shift(3)\n",
    "\n",
    "print('  [OK] 6 cross-asset features built')\n",
    "\n",
    "# ---- LABELS (multiple horizons) ----\n",
    "print('Building labels ...')\n",
    "\n",
    "df['forward_return_1'] = df['close'].shift(-1) / df['close'] - 1  # 5 min\n",
    "df['forward_return_2'] = df['close'].shift(-2) / df['close'] - 1  # 10 min (PRIMARY)\n",
    "df['forward_return_6'] = df['close'].shift(-6) / df['close'] - 1  # 30 min\n",
    "\n",
    "df['label_binary'] = (df['forward_return_2'] > 0).astype(int)\n",
    "df['label_binary_1bar'] = (df['forward_return_1'] > 0).astype(int)\n",
    "df['label_binary_6bar'] = (df['forward_return_6'] > 0).astype(int)\n",
    "\n",
    "print('  [OK] 3 forward return horizons + 3 binary labels')\n",
    "\n",
    "# ---- Clean up ----\n",
    "print('Cleaning features ...')\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    # Group 1: BTC Price/Volume (15)\n",
    "    'return_1bar', 'return_6bar', 'return_12bar', 'return_48bar', 'return_288bar',\n",
    "    'vol_12bar', 'vol_48bar', 'vol_ratio', 'volume_surge', 'volume_trend',\n",
    "    'consecutive_red', 'drawdown_24h', 'rsi_14_norm', 'bb_pct_b', 'vwap_distance',\n",
    "    # Group 2: Daily Macro (10)\n",
    "    'spx_return_1d', 'spx_vs_sma', 'vix_norm', 'vix_change', 'vix_extreme',\n",
    "    'dxy_return_1d', 'dxy_trend', 'yield_level', 'yield_change', 'fng_norm',\n",
    "    # Group 3: Derivatives (9)\n",
    "    'funding_z', 'funding_extreme_long', 'funding_extreme_short',\n",
    "    'oi_change_1h', 'oi_change_4h', 'oi_spike',\n",
    "    'ls_ratio_norm', 'ls_extreme_long', 'taker_imbalance',\n",
    "    # Group 4: Intraday Macro (11)\n",
    "    'spx_return_5m', 'spx_return_15m', 'spx_return_1h',\n",
    "    'spx_momentum_5m', 'spx_direction_5m',\n",
    "    'vix_return_5m', 'vix_return_1h', 'vix_spike_5m',\n",
    "    'ndx_return_5m', 'ndx_return_1h',\n",
    "    'has_intraday_macro',\n",
    "    # Group 5: Cross-Asset (6)\n",
    "    'eth_return_1bar', 'eth_return_6bar', 'eth_btc_ratio_change',\n",
    "    'btc_lead_1', 'btc_lead_2', 'btc_lead_3',\n",
    "]\n",
    "\n",
    "assert len(FEATURE_COLS) == 51, f'Expected 51 features, got {len(FEATURE_COLS)}'\n",
    "\n",
    "# Replace inf with NaN, fill NaN with 0\n",
    "for col in FEATURE_COLS:\n",
    "    df[col] = df[col].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "# Drop rows where primary label is NaN\n",
    "df = df.dropna(subset=['label_binary']).reset_index(drop=True)\n",
    "\n",
    "print(f'\\nFeature summary:')\n",
    "print(f'  Total features: {len(FEATURE_COLS)}')\n",
    "print(f'  Total rows after cleaning: {len(df):,}')\n",
    "print(f'  Label distribution: UP={df[\"label_binary\"].sum():,} / DOWN={(1-df[\"label_binary\"]).sum():,.0f}')\n",
    "print(f'  Label balance: {df[\"label_binary\"].mean():.4f} (0.5 = perfectly balanced)')\n",
    "\n",
    "# Save\n",
    "save_path = os.path.join(DRIVE_DIR, 'crash_features.csv')\n",
    "df.to_csv(save_path, index=False)\n",
    "df.to_csv(os.path.join(LOCAL_DIR, 'crash_features.csv'), index=False)\n",
    "print(f'\\n[DONE] Saved crash_features.csv: {len(df):,} rows, {len(FEATURE_COLS)} features -> {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: THREE-WAY TRAINING SET COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "# Split data by crash period\n",
    "crash1_mask = (df['timestamp'] >= pd.Timestamp('2018-01-07')) & (df['timestamp'] <= pd.Timestamp('2018-12-15'))\n",
    "crash2_mask = (df['timestamp'] >= pd.Timestamp('2021-11-10')) & (df['timestamp'] <= pd.Timestamp('2022-11-21'))\n",
    "crash3_mask = (df['timestamp'] >= pd.Timestamp('2025-10-06')) & (df['timestamp'] <= pd.Timestamp('2026-02-28'))\n",
    "\n",
    "crash1 = df[crash1_mask].copy()\n",
    "crash2 = df[crash2_mask].copy()\n",
    "crash3 = df[crash3_mask].copy()\n",
    "\n",
    "print(f'Crash 1 (ICO Bust):          {len(crash1):>8,} rows')\n",
    "print(f'Crash 2 (Terra/FTX/Fed):     {len(crash2):>8,} rows')\n",
    "print(f'Crash 3 (Current):           {len(crash3):>8,} rows')\n",
    "print()\n",
    "\n",
    "# Crash 3 splits\n",
    "c3_len = len(crash3)\n",
    "c3_half = c3_len // 2\n",
    "c3_70 = int(c3_len * 0.70)\n",
    "c3_85 = int(c3_len * 0.85)\n",
    "\n",
    "crash3_first_half = crash3.iloc[:c3_half]\n",
    "crash3_second_half = crash3.iloc[c3_half:]\n",
    "\n",
    "crash3_train = crash3.iloc[:c3_70]\n",
    "crash3_val = crash3.iloc[c3_70:c3_85]\n",
    "crash3_test = crash3.iloc[c3_85:]\n",
    "\n",
    "# Base LightGBM params\n",
    "BASE_PARAMS = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 100,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    'verbose': -1,\n",
    "}\n",
    "\n",
    "# Smaller model params for Config C (less data)\n",
    "SMALL_PARAMS = dict(BASE_PARAMS)\n",
    "SMALL_PARAMS.update({\n",
    "    'num_leaves': 20,\n",
    "    'min_child_samples': 50,\n",
    "    'lambda_l1': 0.3,\n",
    "    'lambda_l2': 0.3,\n",
    "})\n",
    "\n",
    "\n",
    "def train_and_eval(train_X, train_y, val_X, val_y, test_X, test_y,\n",
    "                   params, sample_weight=None, label='model'):\n",
    "    \"\"\"Train LightGBM and return metrics.\"\"\"\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y, weight=sample_weight)\n",
    "    val_ds = lgb.Dataset(val_X, label=val_y, reference=train_ds)\n",
    "\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(30),\n",
    "        lgb.log_evaluation(0),  # suppress per-round output\n",
    "    ]\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_ds,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[val_ds],\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    val_pred = model.predict(val_X)\n",
    "    test_pred = model.predict(test_X)\n",
    "\n",
    "    val_acc = accuracy_score(val_y, (val_pred > 0.5).astype(int))\n",
    "    test_acc = accuracy_score(test_y, (test_pred > 0.5).astype(int))\n",
    "\n",
    "    try:\n",
    "        val_auc = roc_auc_score(val_y, val_pred)\n",
    "    except ValueError:\n",
    "        val_auc = 0.5\n",
    "    try:\n",
    "        test_auc = roc_auc_score(test_y, test_pred)\n",
    "    except ValueError:\n",
    "        test_auc = 0.5\n",
    "\n",
    "    pred_std = float(np.std(test_pred))\n",
    "    n_rounds = model.best_iteration if model.best_iteration else model.num_trees()\n",
    "\n",
    "    # Top 10 features\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feat_imp = sorted(zip(FEATURE_COLS, importance), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    print(f'\\n  {label}:')\n",
    "    print(f'    Val acc={val_acc:.4f}  Test acc={test_acc:.4f}  Test AUC={test_auc:.4f}  Rounds={n_rounds}')\n",
    "    print(f'    Pred std={pred_std:.4f}')\n",
    "    print(f'    Top 10 features:')\n",
    "    for fname, fval in feat_imp:\n",
    "        print(f'      {fname:30s} gain={fval:.1f}')\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'val_acc': val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'val_auc': val_auc,\n",
    "        'test_auc': test_auc,\n",
    "        'pred_std': pred_std,\n",
    "        'n_rounds': n_rounds,\n",
    "        'test_pred': test_pred,\n",
    "        'feat_imp': feat_imp,\n",
    "    }\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "# ---- Config A: ALL_CRASHES ----\n",
    "print('=' * 60)\n",
    "print('Config A: ALL_CRASHES')\n",
    "print('  Train: Crash1 (weight=0.5) + Crash2 (weight=1.0)')\n",
    "print('  Val:   Crash3 first half')\n",
    "print('  Test:  Crash3 second half')\n",
    "print('=' * 60)\n",
    "\n",
    "train_A = pd.concat([crash1, crash2], ignore_index=True)\n",
    "weight_A = np.concatenate([\n",
    "    np.full(len(crash1), 0.5),\n",
    "    np.full(len(crash2), 1.0),\n",
    "])\n",
    "\n",
    "results['A'] = train_and_eval(\n",
    "    train_A[FEATURE_COLS].values, train_A['label_binary'].values,\n",
    "    crash3_first_half[FEATURE_COLS].values, crash3_first_half['label_binary'].values,\n",
    "    crash3_second_half[FEATURE_COLS].values, crash3_second_half['label_binary'].values,\n",
    "    BASE_PARAMS, sample_weight=weight_A, label='ALL_CRASHES'\n",
    ")\n",
    "results['A']['train_size'] = len(train_A)\n",
    "\n",
    "# ---- Config B: RECENT_ONLY ----\n",
    "print('\\n' + '=' * 60)\n",
    "print('Config B: RECENT_ONLY')\n",
    "print('  Train: Crash2 (weight=1.0)')\n",
    "print('  Val:   Crash3 first half')\n",
    "print('  Test:  Crash3 second half')\n",
    "print('=' * 60)\n",
    "\n",
    "results['B'] = train_and_eval(\n",
    "    crash2[FEATURE_COLS].values, crash2['label_binary'].values,\n",
    "    crash3_first_half[FEATURE_COLS].values, crash3_first_half['label_binary'].values,\n",
    "    crash3_second_half[FEATURE_COLS].values, crash3_second_half['label_binary'].values,\n",
    "    BASE_PARAMS, label='RECENT_ONLY'\n",
    ")\n",
    "results['B']['train_size'] = len(crash2)\n",
    "\n",
    "# ---- Config C: THIS_CRASH_ONLY ----\n",
    "print('\\n' + '=' * 60)\n",
    "print('Config C: THIS_CRASH_ONLY')\n",
    "print('  Train: Crash3 first 70%')\n",
    "print('  Val:   Crash3 next 15%')\n",
    "print('  Test:  Crash3 last 15%')\n",
    "print('=' * 60)\n",
    "\n",
    "results['C'] = train_and_eval(\n",
    "    crash3_train[FEATURE_COLS].values, crash3_train['label_binary'].values,\n",
    "    crash3_val[FEATURE_COLS].values, crash3_val['label_binary'].values,\n",
    "    crash3_test[FEATURE_COLS].values, crash3_test['label_binary'].values,\n",
    "    SMALL_PARAMS, label='THIS_CRASH_ONLY'\n",
    ")\n",
    "results['C']['train_size'] = len(crash3_train)\n",
    "\n",
    "# ---- Comparison Table ----\n",
    "print('\\n\\n')\n",
    "print('TRAINING SET COMPARISON')\n",
    "print('=' * 76)\n",
    "print(f'{\"Config\":26s} {\"Train\":>8s} {\"Val Acc\":>9s} {\"Test Acc\":>9s} {\"AUC\":>7s} {\"Rnds\":>6s} {\"PStd\":>7s}')\n",
    "print('-' * 76)\n",
    "\n",
    "config_names = {\n",
    "    'A': 'ALL_CRASHES (18+21+25)',\n",
    "    'B': 'RECENT_ONLY (21+25)',\n",
    "    'C': 'THIS_CRASH_ONLY (25-26)',\n",
    "}\n",
    "\n",
    "for key in ['A', 'B', 'C']:\n",
    "    r = results[key]\n",
    "    print(f'{config_names[key]:26s} {r[\"train_size\"]:>8,} {r[\"val_acc\"]:>9.4f} {r[\"test_acc\"]:>9.4f} '\n",
    "          f'{r[\"test_auc\"]:>7.4f} {r[\"n_rounds\"]:>6d} {r[\"pred_std\"]:>7.4f}')\n",
    "\n",
    "# Find winner\n",
    "winner_key = max(results, key=lambda k: results[k]['test_acc'])\n",
    "print(f'\\n>>> WINNER: {config_names[winner_key]} (test_acc={results[winner_key][\"test_acc\"]:.4f})')\n",
    "\n",
    "# ---- Train winner at ALL THREE horizons ----\n",
    "print('\\n\\n')\n",
    "print('HORIZON COMPARISON (using winning config)')\n",
    "print('=' * 60)\n",
    "\n",
    "horizon_results = {}\n",
    "\n",
    "# Determine train/val/test splits based on winner\n",
    "if winner_key == 'A':\n",
    "    h_train = train_A\n",
    "    h_weight = weight_A\n",
    "    h_val = crash3_first_half\n",
    "    h_test = crash3_second_half\n",
    "    h_params = BASE_PARAMS\n",
    "elif winner_key == 'B':\n",
    "    h_train = crash2\n",
    "    h_weight = None\n",
    "    h_val = crash3_first_half\n",
    "    h_test = crash3_second_half\n",
    "    h_params = BASE_PARAMS\n",
    "else:  # C\n",
    "    h_train = crash3_train\n",
    "    h_weight = None\n",
    "    h_val = crash3_val\n",
    "    h_test = crash3_test\n",
    "    h_params = SMALL_PARAMS\n",
    "\n",
    "for horizon_name, label_col in [('1-bar (5min)', 'label_binary_1bar'),\n",
    "                                 ('2-bar (10min)', 'label_binary'),\n",
    "                                 ('6-bar (30min)', 'label_binary_6bar')]:\n",
    "    # Drop NaN labels for this horizon\n",
    "    train_mask = h_train[label_col].notna()\n",
    "    val_mask = h_val[label_col].notna()\n",
    "    test_mask = h_test[label_col].notna()\n",
    "\n",
    "    ht = h_train[train_mask]\n",
    "    hv = h_val[val_mask]\n",
    "    hte = h_test[test_mask]\n",
    "\n",
    "    hw = h_weight[train_mask.values] if h_weight is not None else None\n",
    "\n",
    "    res = train_and_eval(\n",
    "        ht[FEATURE_COLS].values, ht[label_col].values,\n",
    "        hv[FEATURE_COLS].values, hv[label_col].values,\n",
    "        hte[FEATURE_COLS].values, hte[label_col].values,\n",
    "        h_params, sample_weight=hw, label=horizon_name\n",
    "    )\n",
    "    horizon_results[horizon_name] = res\n",
    "\n",
    "print('\\nHorizon comparison:')\n",
    "print(f'{\"Horizon\":20s} {\"Test Acc\":>9s} {\"AUC\":>7s}')\n",
    "print('-' * 40)\n",
    "for hname, hres in horizon_results.items():\n",
    "    print(f'{hname:20s} {hres[\"test_acc\"]:>9.4f} {hres[\"test_auc\"]:>7.4f}')\n",
    "\n",
    "# ---- Save best model ----\n",
    "best = results[winner_key]\n",
    "best_model = best['model']\n",
    "\n",
    "# Save as pickle\n",
    "pkl_path = os.path.join(DRIVE_DIR, 'crash_best_lightgbm.pkl')\n",
    "with open(pkl_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f'\\n[OK] Saved model pickle: {pkl_path}')\n",
    "\n",
    "# Save as LightGBM text\n",
    "txt_path = os.path.join(DRIVE_DIR, 'crash_best_lightgbm.txt')\n",
    "best_model.save_model(txt_path)\n",
    "print(f'[OK] Saved model text: {txt_path}')\n",
    "\n",
    "# Save metadata\n",
    "meta = {\n",
    "    'winner': config_names[winner_key],\n",
    "    'winner_key': winner_key,\n",
    "    'feature_cols': FEATURE_COLS,\n",
    "    'n_features': len(FEATURE_COLS),\n",
    "    'primary_horizon': '2-bar (10min)',\n",
    "    'comparison': {},\n",
    "    'horizon_comparison': {},\n",
    "    'trained_at': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "}\n",
    "for key in ['A', 'B', 'C']:\n",
    "    r = results[key]\n",
    "    meta['comparison'][config_names[key]] = {\n",
    "        'train_size': r['train_size'],\n",
    "        'val_acc': round(r['val_acc'], 4),\n",
    "        'test_acc': round(r['test_acc'], 4),\n",
    "        'val_auc': round(r['val_auc'], 4),\n",
    "        'test_auc': round(r['test_auc'], 4),\n",
    "        'pred_std': round(r['pred_std'], 4),\n",
    "        'n_rounds': r['n_rounds'],\n",
    "    }\n",
    "for hname, hres in horizon_results.items():\n",
    "    meta['horizon_comparison'][hname] = {\n",
    "        'test_acc': round(hres['test_acc'], 4),\n",
    "        'test_auc': round(hres['test_auc'], 4),\n",
    "    }\n",
    "\n",
    "meta_path = os.path.join(DRIVE_DIR, 'crash_best_meta.json')\n",
    "with open(meta_path, 'w') as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(f'[OK] Saved metadata: {meta_path}')\n",
    "\n",
    "# Also save locally\n",
    "with open(os.path.join(LOCAL_DIR, 'crash_best_lightgbm.pkl'), 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "best_model.save_model(os.path.join(LOCAL_DIR, 'crash_best_lightgbm.txt'))\n",
    "with open(os.path.join(LOCAL_DIR, 'crash_best_meta.json'), 'w') as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print()\n",
    "print('[DONE] Model training and comparison complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: Calibration analysis\n",
    "# =============================================================================\n",
    "\n",
    "# Use best model's test predictions\n",
    "best_result = results[winner_key]\n",
    "test_pred = best_result['test_pred']\n",
    "\n",
    "# Determine test labels based on winner config\n",
    "if winner_key == 'C':\n",
    "    test_labels = crash3_test['label_binary'].values\n",
    "else:\n",
    "    test_labels = crash3_second_half['label_binary'].values\n",
    "\n",
    "# Calibration buckets\n",
    "up_buckets = [\n",
    "    ('50-52% UP', 0.50, 0.52),\n",
    "    ('52-55% UP', 0.52, 0.55),\n",
    "    ('55-60% UP', 0.55, 0.60),\n",
    "    ('60-65% UP', 0.60, 0.65),\n",
    "    ('65-70% UP', 0.65, 0.70),\n",
    "    ('70%+   UP', 0.70, 1.01),\n",
    "]\n",
    "\n",
    "down_buckets = [\n",
    "    ('50-52% DN', 0.48, 0.50),\n",
    "    ('52-55% DN', 0.45, 0.48),\n",
    "    ('55-60% DN', 0.40, 0.45),\n",
    "    ('60-65% DN', 0.35, 0.40),\n",
    "    ('65-70% DN', 0.30, 0.35),\n",
    "    ('70%+   DN', 0.00, 0.30),\n",
    "]\n",
    "\n",
    "print('CALIBRATION ANALYSIS')\n",
    "print('=' * 70)\n",
    "print(f'{\"Bucket\":14s} {\"Count\":>7s} {\"Pct\":>6s} {\"Accuracy\":>10s} {\"Avg Pred\":>10s}')\n",
    "print('-' * 70)\n",
    "\n",
    "cal_rows = []\n",
    "\n",
    "for bucket_name, lo, hi in up_buckets:\n",
    "    mask = (test_pred >= lo) & (test_pred < hi)\n",
    "    count = mask.sum()\n",
    "    if count > 0:\n",
    "        acc = accuracy_score(test_labels[mask], (test_pred[mask] > 0.5).astype(int))\n",
    "        avg_pred = test_pred[mask].mean()\n",
    "        pct = count / len(test_pred) * 100\n",
    "        print(f'{bucket_name:14s} {count:>7,} {pct:>5.1f}% {acc:>10.4f} {avg_pred:>10.4f}')\n",
    "        cal_rows.append({'bucket': bucket_name, 'count': count, 'pct': pct,\n",
    "                         'accuracy': acc, 'avg_pred': avg_pred})\n",
    "    else:\n",
    "        print(f'{bucket_name:14s} {0:>7d}   --- {\"---\":>10s} {\"---\":>10s}')\n",
    "        cal_rows.append({'bucket': bucket_name, 'count': 0, 'pct': 0,\n",
    "                         'accuracy': None, 'avg_pred': None})\n",
    "\n",
    "print()\n",
    "\n",
    "for bucket_name, lo, hi in down_buckets:\n",
    "    mask = (test_pred >= lo) & (test_pred < hi)\n",
    "    count = mask.sum()\n",
    "    if count > 0:\n",
    "        # For DOWN buckets, accuracy = how often label was 0 (DOWN)\n",
    "        acc = accuracy_score(test_labels[mask], (test_pred[mask] > 0.5).astype(int))\n",
    "        avg_pred = test_pred[mask].mean()\n",
    "        pct = count / len(test_pred) * 100\n",
    "        print(f'{bucket_name:14s} {count:>7,} {pct:>5.1f}% {acc:>10.4f} {avg_pred:>10.4f}')\n",
    "        cal_rows.append({'bucket': bucket_name, 'count': count, 'pct': pct,\n",
    "                         'accuracy': acc, 'avg_pred': avg_pred})\n",
    "    else:\n",
    "        print(f'{bucket_name:14s} {0:>7d}   --- {\"---\":>10s} {\"---\":>10s}')\n",
    "        cal_rows.append({'bucket': bucket_name, 'count': 0, 'pct': 0,\n",
    "                         'accuracy': None, 'avg_pred': None})\n",
    "\n",
    "# Summary stats\n",
    "max_conf = float(test_pred.max())\n",
    "min_conf = float(test_pred.min())\n",
    "high_conf_mask = (test_pred > 0.55) | (test_pred < 0.45)\n",
    "count_55plus = high_conf_mask.sum()\n",
    "if count_55plus > 0:\n",
    "    acc_55plus = accuracy_score(test_labels[high_conf_mask],\n",
    "                                (test_pred[high_conf_mask] > 0.5).astype(int))\n",
    "else:\n",
    "    acc_55plus = 0.0\n",
    "\n",
    "print()\n",
    "print('-' * 70)\n",
    "print(f'Max confidence (UP):   {max_conf:.4f}')\n",
    "print(f'Max confidence (DOWN): {1 - min_conf:.4f}')\n",
    "print(f'Count at 55%+:         {count_55plus:,} ({count_55plus/len(test_pred)*100:.1f}% of test set)')\n",
    "print(f'Accuracy at 55%+:      {acc_55plus:.4f}')\n",
    "\n",
    "if acc_55plus > 0.52:\n",
    "    print(f'Recommended threshold: 0.55 (accuracy {acc_55plus:.4f} > 0.52 baseline)')\n",
    "elif count_55plus > 0:\n",
    "    print(f'Recommended threshold: 0.52 (55%+ accuracy {acc_55plus:.4f} is marginal)')\n",
    "else:\n",
    "    print('Recommended threshold: 0.50 (no high-confidence predictions available)')\n",
    "\n",
    "# Save calibration\n",
    "cal_df = pd.DataFrame(cal_rows)\n",
    "cal_path = os.path.join(DRIVE_DIR, 'calibration_analysis.csv')\n",
    "cal_df.to_csv(cal_path, index=False)\n",
    "cal_df.to_csv(os.path.join(LOCAL_DIR, 'calibration_analysis.csv'), index=False)\n",
    "print(f'\\n[OK] Saved calibration_analysis.csv -> {cal_path}')\n",
    "\n",
    "# Final summary\n",
    "print()\n",
    "print()\n",
    "print('=' * 70)\n",
    "print('FINAL SUMMARY')\n",
    "print('=' * 70)\n",
    "print(f'Winner config:      {meta[\"winner\"]}')\n",
    "print(f'Features:           {len(FEATURE_COLS)} (51 features, all scale-invariant)')\n",
    "print(f'Primary horizon:    2-bar (10 minutes)')\n",
    "print(f'Test accuracy:      {best_result[\"test_acc\"]:.4f}')\n",
    "print(f'Test AUC:           {best_result[\"test_auc\"]:.4f}')\n",
    "print(f'Prediction spread:  {best_result[\"pred_std\"]:.4f}')\n",
    "print(f'Model rounds:       {best_result[\"n_rounds\"]}')\n",
    "print()\n",
    "print('Saved files:')\n",
    "print(f'  Model:       {DRIVE_DIR}/crash_best_lightgbm.pkl')\n",
    "print(f'  Model text:  {DRIVE_DIR}/crash_best_lightgbm.txt')\n",
    "print(f'  Metadata:    {DRIVE_DIR}/crash_best_meta.json')\n",
    "print(f'  Features:    {DRIVE_DIR}/crash_features.csv')\n",
    "print(f'  Calibration: {DRIVE_DIR}/calibration_analysis.csv')\n",
    "print()\n",
    "print('[DONE] Crash regime v3 training complete')"
   ]
  }
 ]
}