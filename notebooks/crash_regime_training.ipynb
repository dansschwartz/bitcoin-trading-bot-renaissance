{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Crash-Regime LightGBM Training\n",
    "## Renaissance Trading Bot -- Regime-Specific Models\n",
    "\n",
    "**What this does:**\n",
    "- Downloads BTC + ETH candles, macro data (SPX, VIX, DXY), Binance derivatives\n",
    "- Filters to crash periods only (2018, 2021-22, 2025-26)\n",
    "- Engineers 40 crash-specific features across 4 groups\n",
    "- Trains LightGBM on crash-only data with macro features\n",
    "- Saves everything to Google Drive (survives runtime disconnects)\n",
    "\n",
    "**Key insight:** BTC has 0.77-0.90 correlation with S&P 500 during crashes.\n",
    "Our current models miss this signal entirely, achieving only 51% accuracy.\n",
    "Training on crash-only data with macro features should improve this.\n",
    "\n",
    "**Runtime:** Select GPU (T4) for faster training, though LightGBM trains in ~30 seconds either way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & Google Drive Mount\n",
    "Mount Drive first so all outputs survive runtime disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Setup & Google Drive Mount\n",
    "# ============================================================\n",
    "# Mount Drive FIRST -- all outputs save here to survive disconnects\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "os.makedirs('/content/models', exist_ok=True)\n",
    "\n",
    "DRIVE_SAVE = '/content/drive/MyDrive/renaissance-bot-training/crash_models/'\n",
    "os.makedirs(DRIVE_SAVE, exist_ok=True)\n",
    "\n",
    "!pip install -q yfinance lightgbm pandas numpy scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"\\u2705 Setup complete\")\n",
    "print(f\"Drive save path: {DRIVE_SAVE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Download BTC + ETH 5-Minute Candles from Binance\n",
    "Full history from Sep 2017. Covers all three crash periods. Takes 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Download BTC + ETH 5m candles from Binance\n",
    "# ============================================================\n",
    "# Need crash periods:\n",
    "#   Crash 1: Jan 2018 -- Dec 2018\n",
    "#   Crash 2: Nov 2021 -- Nov 2022\n",
    "#   Crash 3: Oct 2025 -- Feb 2026 (current)\n",
    "\n",
    "def fetch_binance_klines(symbol, interval, start_ms, end_ms, limit=1000):\n",
    "    \"\"\"Fetch klines from Binance public API. No auth needed.\"\"\"\n",
    "    url = \"https://api.binance.com/api/v3/klines\"\n",
    "    all_data = []\n",
    "    current = start_ms\n",
    "    retries = 0\n",
    "\n",
    "    while current < end_ms:\n",
    "        params = {\n",
    "            'symbol': symbol,\n",
    "            'interval': interval,\n",
    "            'startTime': current,\n",
    "            'endTime': end_ms,\n",
    "            'limit': limit,\n",
    "        }\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, timeout=30)\n",
    "            if resp.status_code == 429:\n",
    "                print(\"  Rate limited, waiting 60s...\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"  HTTP {resp.status_code}, retrying...\")\n",
    "                retries += 1\n",
    "                if retries > 5:\n",
    "                    break\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "            data = resp.json()\n",
    "            if not data or not isinstance(data, list):\n",
    "                break\n",
    "\n",
    "            all_data.extend(data)\n",
    "            current = data[-1][0] + 1\n",
    "            retries = 0\n",
    "\n",
    "            if len(all_data) % 50000 == 0:\n",
    "                print(f\"  ... {len(all_data):,} candles so far\")\n",
    "\n",
    "            if len(data) < limit:\n",
    "                break\n",
    "\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}, retrying...\")\n",
    "            retries += 1\n",
    "            if retries > 5:\n",
    "                break\n",
    "            time.sleep(5)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def klines_to_df(raw):\n",
    "    \"\"\"Convert Binance klines to clean DataFrame.\"\"\"\n",
    "    df = pd.DataFrame(raw, columns=[\n",
    "        'open_time', 'open', 'high', 'low', 'close', 'volume',\n",
    "        'close_time', 'quote_volume', 'trades', 'taker_buy_base',\n",
    "        'taker_buy_quote', 'ignore'\n",
    "    ])\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume', 'quote_volume',\n",
    "                'taker_buy_base', 'taker_buy_quote']:\n",
    "        df[col] = df[col].astype(float)\n",
    "    df['trades'] = df['trades'].astype(int)\n",
    "    df['timestamp'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    df = df.drop_duplicates(subset=['open_time']).sort_values('timestamp').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "start_ms = int(datetime(2017, 9, 1).timestamp() * 1000)\n",
    "end_ms = int(datetime.now(datetime.UTC).timestamp() * 1000)\n",
    "\n",
    "# Download BTC\n",
    "print(\"Downloading BTC 5m candles from Binance (this takes 5-10 minutes)...\")\n",
    "raw = fetch_binance_klines('BTCUSDT', '5m', start_ms, end_ms)\n",
    "btc_5m = klines_to_df(raw)\n",
    "btc_5m.to_csv('/content/data/btc_5m_full.csv', index=False)\n",
    "print(f\"\\u2705 BTC: {len(btc_5m):,} candles ({btc_5m['timestamp'].min().date()} \\u2192 {btc_5m['timestamp'].max().date()})\")\n",
    "\n",
    "# Download ETH (for cross-asset features)\n",
    "print(\"\\nDownloading ETH 5m candles...\")\n",
    "raw = fetch_binance_klines('ETHUSDT', '5m', start_ms, end_ms)\n",
    "eth_5m = klines_to_df(raw)\n",
    "eth_5m.to_csv('/content/data/eth_5m_full.csv', index=False)\n",
    "print(f\"\\u2705 ETH: {len(eth_5m):,} candles ({eth_5m['timestamp'].min().date()} \\u2192 {eth_5m['timestamp'].max().date()})\")\n",
    "\n",
    "# Save to Drive as backup\n",
    "btc_5m.to_csv(f'{DRIVE_SAVE}/btc_5m_full.csv', index=False)\n",
    "eth_5m.to_csv(f'{DRIVE_SAVE}/eth_5m_full.csv', index=False)\n",
    "print(\"\\n\\u2705 Backed up to Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Download Macro Data (S&P 500, VIX, DXY, Yields)\n",
    "Daily macro data via yfinance. BTC has 0.77-0.90 correlation with SPX in crashes -- this is the NEW signal our current models don't have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Download macro data via yfinance\n",
    "# ============================================================\n",
    "# BTC has 0.77-0.90 correlation with S&P 500 in crash periods.\n",
    "# This is the signal our models are missing.\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "macro_tickers = {\n",
    "    'spx': '^GSPC',       # S&P 500\n",
    "    'ndx': '^IXIC',       # Nasdaq Composite\n",
    "    'vix': '^VIX',        # CBOE Volatility Index\n",
    "    'dxy': 'DX-Y.NYB',    # US Dollar Index\n",
    "    'us10y': '^TNX',      # 10-Year Treasury Yield\n",
    "    'gold': 'GC=F',       # Gold Futures\n",
    "}\n",
    "\n",
    "macro_daily = {}\n",
    "for name, ticker in macro_tickers.items():\n",
    "    print(f\"Downloading {name} ({ticker})...\")\n",
    "    try:\n",
    "        data = yf.download(ticker, start='2017-01-01', progress=False)\n",
    "        if len(data) > 0:\n",
    "            # Handle MultiIndex columns from yfinance\n",
    "            if isinstance(data.columns, pd.MultiIndex):\n",
    "                data.columns = data.columns.get_level_values(0)\n",
    "            macro_daily[name] = data[['Close']].rename(columns={'Close': name})\n",
    "            print(f\"  \\u2705 {name}: {len(data):,} days ({data.index.min().date()} \\u2192 {data.index.max().date()})\")\n",
    "        else:\n",
    "            print(f\"  \\u26a0\\ufe0f {name}: no data returned\")\n",
    "    except Exception as e:\n",
    "        print(f\"  \\u274c {name}: {e}\")\n",
    "\n",
    "# Merge into one DataFrame\n",
    "macro_df = pd.concat(macro_daily.values(), axis=1)\n",
    "macro_df.index = pd.to_datetime(macro_df.index)\n",
    "macro_df = macro_df.ffill()  # Forward-fill weekends/holidays\n",
    "macro_df = macro_df.bfill()  # Back-fill any leading NaNs\n",
    "macro_df.to_csv('/content/data/macro_daily.csv')\n",
    "print(f\"\\n\\u2705 Macro data: {len(macro_df):,} days\")\n",
    "print(f\"   Columns: {list(macro_df.columns)}\")\n",
    "print(f\"   Date range: {macro_df.index.min().date()} \\u2192 {macro_df.index.max().date()}\")\n",
    "\n",
    "# Fear & Greed Index (daily, from alternative.me \\u2014 free API)\n",
    "print(\"\\nDownloading Fear & Greed Index...\")\n",
    "try:\n",
    "    fng_resp = requests.get(\"https://api.alternative.me/fng/?limit=0\", timeout=30)\n",
    "    fng_data = fng_resp.json()['data']\n",
    "    fng_df = pd.DataFrame(fng_data)\n",
    "    fng_df['timestamp'] = pd.to_datetime(fng_df['timestamp'].astype(int), unit='s')\n",
    "    fng_df['fng'] = fng_df['value'].astype(int)\n",
    "    fng_df = fng_df[['timestamp', 'fng']].set_index('timestamp').sort_index()\n",
    "    fng_df.to_csv('/content/data/fear_greed.csv')\n",
    "    print(f\"  \\u2705 Fear & Greed: {len(fng_df):,} days\")\n",
    "except Exception as e:\n",
    "    print(f\"  \\u274c Fear & Greed: {e}\")\n",
    "    fng_df = None\n",
    "\n",
    "# Save to Drive\n",
    "macro_df.to_csv(f'{DRIVE_SAVE}/macro_daily.csv')\n",
    "print(\"\\u2705 Backed up to Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Download Binance Derivatives Data\n",
    "Funding rate (8h), Open Interest (5m), Long/Short ratio (5m), Taker Buy/Sell volume (5m). Available from ~mid 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Download Binance Futures derivatives data\n",
    "# ============================================================\n",
    "# Funding rate, Open Interest, Long/Short ratio, Taker buy/sell\n",
    "# Free endpoints, no API key needed.\n",
    "# Available from approximately mid-2019.\n",
    "\n",
    "def fetch_binance_futures(endpoint, symbol, period=None, limit=500,\n",
    "                          start_ms=None, end_ms=None):\n",
    "    \"\"\"Fetch data from Binance Futures API.\"\"\"\n",
    "    base = \"https://fapi.binance.com\"\n",
    "    all_data = []\n",
    "    current = start_ms\n",
    "    retries = 0\n",
    "\n",
    "    while True:\n",
    "        params = {'symbol': symbol, 'limit': limit}\n",
    "        if period and 'fundingRate' not in endpoint:\n",
    "            params['period'] = period\n",
    "        if current:\n",
    "            params['startTime'] = current\n",
    "        if end_ms:\n",
    "            params['endTime'] = end_ms\n",
    "\n",
    "        try:\n",
    "            resp = requests.get(f\"{base}{endpoint}\", params=params, timeout=30)\n",
    "            if resp.status_code == 429:\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "            if resp.status_code != 200:\n",
    "                retries += 1\n",
    "                if retries > 3:\n",
    "                    break\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "            data = resp.json()\n",
    "            if not data or not isinstance(data, list):\n",
    "                break\n",
    "\n",
    "            all_data.extend(data)\n",
    "            retries = 0\n",
    "\n",
    "            # Find timestamp key for pagination\n",
    "            if 'fundingTime' in data[-1]:\n",
    "                current = data[-1]['fundingTime'] + 1\n",
    "            elif 'timestamp' in data[-1]:\n",
    "                current = data[-1]['timestamp'] + 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            if len(data) < limit:\n",
    "                break\n",
    "\n",
    "            if len(all_data) % 10000 == 0:\n",
    "                print(f\"    ... {len(all_data):,} records\")\n",
    "\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            if retries > 3:\n",
    "                print(f\"    Giving up after {retries} retries: {e}\")\n",
    "                break\n",
    "            time.sleep(5)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "start_ms = int(datetime(2019, 9, 1).timestamp() * 1000)\n",
    "end_ms = int(datetime.now(datetime.UTC).timestamp() * 1000)\n",
    "\n",
    "derivatives = {}\n",
    "\n",
    "# 1. Funding Rate (8-hourly)\n",
    "print(\"Downloading BTC funding rate (8h intervals)...\")\n",
    "raw = fetch_binance_futures('/fapi/v1/fundingRate', 'BTCUSDT',\n",
    "                            start_ms=start_ms, end_ms=end_ms)\n",
    "if raw:\n",
    "    fr_df = pd.DataFrame(raw)\n",
    "    fr_df['timestamp'] = pd.to_datetime(fr_df['fundingTime'], unit='ms')\n",
    "    fr_df['funding_rate'] = fr_df['fundingRate'].astype(float)\n",
    "    derivatives['funding_rate'] = fr_df[['timestamp', 'funding_rate']]\n",
    "    fr_df[['timestamp', 'funding_rate']].to_csv('/content/data/btc_funding_rate.csv', index=False)\n",
    "    print(f\"  \\u2705 Funding rate: {len(fr_df):,} records\")\n",
    "else:\n",
    "    print(\"  \\u26a0\\ufe0f No funding rate data\")\n",
    "\n",
    "# 2. Open Interest (5m)\n",
    "print(\"Downloading BTC open interest (5m)...\")\n",
    "raw = fetch_binance_futures('/futures/data/openInterestHist', 'BTCUSDT',\n",
    "                            period='5m', start_ms=start_ms, end_ms=end_ms)\n",
    "if raw:\n",
    "    oi_df = pd.DataFrame(raw)\n",
    "    oi_df['timestamp'] = pd.to_datetime(oi_df['timestamp'], unit='ms')\n",
    "    oi_df['open_interest'] = oi_df['sumOpenInterest'].astype(float)\n",
    "    oi_df['oi_value'] = oi_df['sumOpenInterestValue'].astype(float)\n",
    "    derivatives['open_interest'] = oi_df[['timestamp', 'open_interest', 'oi_value']]\n",
    "    oi_df[['timestamp', 'open_interest', 'oi_value']].to_csv('/content/data/btc_open_interest.csv', index=False)\n",
    "    print(f\"  \\u2705 Open interest: {len(oi_df):,} records\")\n",
    "else:\n",
    "    print(\"  \\u26a0\\ufe0f No open interest data\")\n",
    "\n",
    "# 3. Long/Short Ratio (5m)\n",
    "print(\"Downloading BTC long/short ratio (5m)...\")\n",
    "raw = fetch_binance_futures('/futures/data/globalLongShortAccountRatio',\n",
    "                            'BTCUSDT', period='5m',\n",
    "                            start_ms=start_ms, end_ms=end_ms)\n",
    "if raw:\n",
    "    ls_df = pd.DataFrame(raw)\n",
    "    ls_df['timestamp'] = pd.to_datetime(ls_df['timestamp'], unit='ms')\n",
    "    ls_df['long_short_ratio'] = ls_df['longShortRatio'].astype(float)\n",
    "    ls_df['long_account'] = ls_df['longAccount'].astype(float)\n",
    "    ls_df['short_account'] = ls_df['shortAccount'].astype(float)\n",
    "    derivatives['long_short'] = ls_df[['timestamp', 'long_short_ratio',\n",
    "                                        'long_account', 'short_account']]\n",
    "    ls_df[['timestamp', 'long_short_ratio', 'long_account', 'short_account']].to_csv(\n",
    "        '/content/data/btc_long_short.csv', index=False)\n",
    "    print(f\"  \\u2705 Long/short ratio: {len(ls_df):,} records\")\n",
    "else:\n",
    "    print(\"  \\u26a0\\ufe0f No long/short data\")\n",
    "\n",
    "# 4. Taker Buy/Sell Volume (5m)\n",
    "print(\"Downloading BTC taker buy/sell volume (5m)...\")\n",
    "raw = fetch_binance_futures('/futures/data/takeBuySellVol', 'BTCUSDT',\n",
    "                            period='5m', start_ms=start_ms, end_ms=end_ms)\n",
    "if raw:\n",
    "    tv_df = pd.DataFrame(raw)\n",
    "    tv_df['timestamp'] = pd.to_datetime(tv_df['timestamp'], unit='ms')\n",
    "    tv_df['taker_buy_vol'] = tv_df['buyVol'].astype(float)\n",
    "    tv_df['taker_sell_vol'] = tv_df['sellVol'].astype(float)\n",
    "    tv_df['taker_ratio'] = tv_df['taker_buy_vol'] / (tv_df['taker_sell_vol'] + 1e-10)\n",
    "    derivatives['taker_vol'] = tv_df[['timestamp', 'taker_buy_vol',\n",
    "                                       'taker_sell_vol', 'taker_ratio']]\n",
    "    tv_df[['timestamp', 'taker_buy_vol', 'taker_sell_vol', 'taker_ratio']].to_csv(\n",
    "        '/content/data/btc_taker_vol.csv', index=False)\n",
    "    print(f\"  \\u2705 Taker volume: {len(tv_df):,} records\")\n",
    "else:\n",
    "    print(\"  \\u26a0\\ufe0f No taker volume data\")\n",
    "\n",
    "print(f\"\\n\\u2705 Derivatives data complete. Saved {len(derivatives)} datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Label Crash Periods & Merge All Data\n",
    "Merge BTC 5m candles with macro (daily), derivatives (variable freq via merge_asof), ETH cross-asset, and Fear & Greed. Filter to crash periods only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Label crash periods and merge everything\n",
    "# ============================================================\n",
    "\n",
    "# Load BTC 5m\n",
    "btc = pd.read_csv('/content/data/btc_5m_full.csv')\n",
    "btc['timestamp'] = pd.to_datetime(btc['timestamp'])\n",
    "btc = btc.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# \\u2500\\u2500 Define crash periods \\u2500\\u2500\n",
    "# Based on BTC market cycle analysis:\n",
    "#   Crash 1: Post-2017 bubble, Jan 2018 peak \\u2192 Dec 2018 bottom\n",
    "#   Crash 2: Post-2021 bubble, Nov 2021 ATH $69K \\u2192 Nov 2022 bottom $15.5K\n",
    "#   Crash 3: Post-2025 bubble, Oct 2025 ATH $126K \\u2192 ongoing (~$65K)\n",
    "CRASH_PERIODS = [\n",
    "    ('2018-01-07', '2018-12-15'),\n",
    "    ('2021-11-10', '2022-11-21'),\n",
    "    ('2025-10-06', '2026-02-28'),\n",
    "]\n",
    "\n",
    "btc['is_crash'] = False\n",
    "for start, end in CRASH_PERIODS:\n",
    "    mask = (btc['timestamp'] >= start) & (btc['timestamp'] <= end)\n",
    "    btc.loc[mask, 'is_crash'] = True\n",
    "\n",
    "crash_data = btc[btc['is_crash']].copy()\n",
    "print(f\"Total BTC candles: {len(btc):,}\")\n",
    "print(f\"Crash candles: {len(crash_data):,} ({100*len(crash_data)/len(btc):.1f}%)\")\n",
    "for start, end in CRASH_PERIODS:\n",
    "    n = len(btc[(btc['timestamp'] >= start) & (btc['timestamp'] <= end)])\n",
    "    print(f\"  {start} \\u2192 {end}: {n:,} candles\")\n",
    "\n",
    "# \\u2500\\u2500 Merge macro data (daily \\u2192 5m via date join) \\u2500\\u2500\n",
    "macro = pd.read_csv('/content/data/macro_daily.csv', index_col=0, parse_dates=True)\n",
    "crash_data['date'] = crash_data['timestamp'].dt.strftime('%Y-%m-%d')\n",
    "macro['date'] = macro.index.strftime('%Y-%m-%d')\n",
    "crash_data = crash_data.merge(macro, on='date', how='left')\n",
    "for col in ['spx', 'ndx', 'vix', 'dxy', 'us10y', 'gold']:\n",
    "    if col in crash_data.columns:\n",
    "        crash_data[col] = crash_data[col].ffill().bfill()\n",
    "print(f\"\\n\\u2705 Merged macro data\")\n",
    "\n",
    "# \\u2500\\u2500 Merge Fear & Greed (daily \\u2192 5m via date join) \\u2500\\u2500\n",
    "try:\n",
    "    fng = pd.read_csv('/content/data/fear_greed.csv', parse_dates=['timestamp'])\n",
    "    fng['date'] = fng['timestamp'].dt.strftime('%Y-%m-%d')\n",
    "    crash_data = crash_data.merge(fng[['date', 'fng']], on='date', how='left')\n",
    "    crash_data['fng'] = crash_data['fng'].ffill().bfill().fillna(50)\n",
    "    print(f\"\\u2705 Merged Fear & Greed\")\n",
    "except Exception as e:\n",
    "    crash_data['fng'] = 50\n",
    "    print(f\"\\u26a0\\ufe0f Fear & Greed failed, using default 50: {e}\")\n",
    "\n",
    "# \\u2500\\u2500 Merge derivatives (variable freq \\u2192 5m via merge_asof) \\u2500\\u2500\n",
    "crash_data = crash_data.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "deriv_files = {\n",
    "    'funding_rate': ('/content/data/btc_funding_rate.csv', ['funding_rate']),\n",
    "    'open_interest': ('/content/data/btc_open_interest.csv', ['open_interest', 'oi_value']),\n",
    "    'long_short': ('/content/data/btc_long_short.csv', ['long_short_ratio', 'long_account', 'short_account']),\n",
    "    'taker_vol': ('/content/data/btc_taker_vol.csv', ['taker_buy_vol', 'taker_sell_vol', 'taker_ratio']),\n",
    "}\n",
    "\n",
    "for name, (path, cols) in deriv_files.items():\n",
    "    try:\n",
    "        deriv = pd.read_csv(path, parse_dates=['timestamp'])\n",
    "        deriv = deriv.sort_values('timestamp').reset_index(drop=True)\n",
    "        crash_data = pd.merge_asof(\n",
    "            crash_data, deriv[['timestamp'] + cols],\n",
    "            on='timestamp', direction='backward',\n",
    "            tolerance=pd.Timedelta('8h')  # Funding rate is 8-hourly\n",
    "        )\n",
    "        filled = crash_data[cols[0]].notna().sum()\n",
    "        print(f\"\\u2705 Merged {name}: {filled:,}/{len(crash_data):,} rows filled\")\n",
    "    except Exception as e:\n",
    "        for col in cols:\n",
    "            crash_data[col] = np.nan\n",
    "        print(f\"\\u26a0\\ufe0f {name} merge failed: {e}\")\n",
    "\n",
    "# \\u2500\\u2500 Merge ETH cross-asset data \\u2500\\u2500\n",
    "try:\n",
    "    eth = pd.read_csv('/content/data/eth_5m_full.csv')\n",
    "    eth['timestamp'] = pd.to_datetime(eth['timestamp'])\n",
    "    eth = eth.sort_values('timestamp').reset_index(drop=True)\n",
    "    eth_merge = eth[['timestamp', 'close', 'volume']].rename(\n",
    "        columns={'close': 'eth_close', 'volume': 'eth_volume'}\n",
    "    )\n",
    "    crash_data = pd.merge_asof(\n",
    "        crash_data, eth_merge,\n",
    "        on='timestamp', direction='backward',\n",
    "        tolerance=pd.Timedelta('5min')\n",
    "    )\n",
    "    filled = crash_data['eth_close'].notna().sum()\n",
    "    print(f\"\\u2705 Merged ETH: {filled:,}/{len(crash_data):,} rows filled\")\n",
    "except Exception as e:\n",
    "    crash_data['eth_close'] = np.nan\n",
    "    crash_data['eth_volume'] = np.nan\n",
    "    print(f\"\\u26a0\\ufe0f ETH merge failed: {e}\")\n",
    "\n",
    "# \\u2500\\u2500 Clean and save \\u2500\\u2500\n",
    "crash_data = crash_data.dropna(subset=['close']).reset_index(drop=True)\n",
    "crash_data.to_csv('/content/data/crash_dataset_raw.csv', index=False)\n",
    "crash_data.to_csv(f'{DRIVE_SAVE}/crash_dataset_raw.csv', index=False)\n",
    "\n",
    "print(f\"\\n\\u2705 Crash dataset: {len(crash_data):,} rows, {len(crash_data.columns)} columns\")\n",
    "print(f\"\\nColumn overview:\")\n",
    "for col in sorted(crash_data.columns):\n",
    "    non_null = crash_data[col].notna().sum()\n",
    "    pct = 100 * non_null / len(crash_data)\n",
    "    print(f\"  {col:30s} {non_null:>8,} non-null ({pct:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Engineer Crash-Specific Features\n",
    "40 features in 4 groups: BTC price/volume (15), Macro correlation (10), Derivatives (9), Cross-asset (6). All features are normalized/relative (no absolute prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 6: Build crash-specific features\n",
    "# ============================================================\n",
    "# 40 features across 4 groups, all scale-invariant\n",
    "\n",
    "df = crash_data.copy()\n",
    "\n",
    "# \\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\n",
    "# GROUP 1: BTC PRICE & VOLUME (15 features)\n",
    "# \\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\n",
    "\n",
    "# Returns at multiple horizons (relative, not absolute)\n",
    "df['return_1bar'] = df['close'].pct_change(1)          # 5 min\n",
    "df['return_6bar'] = df['close'].pct_change(6)          # 30 min\n",
    "df['return_12bar'] = df['close'].pct_change(12)        # 1 hour\n",
    "df['return_48bar'] = df['close'].pct_change(48)        # 4 hours\n",
    "df['return_288bar'] = df['close'].pct_change(288)      # 24 hours\n",
    "\n",
    "# Volatility (rolling std of returns \\u2014 already scale-invariant)\n",
    "df['vol_12bar'] = df['return_1bar'].rolling(12).std()\n",
    "df['vol_48bar'] = df['return_1bar'].rolling(48).std()\n",
    "df['vol_ratio'] = df['vol_12bar'] / (df['vol_48bar'] + 1e-10)\n",
    "\n",
    "# Volume (relative to own history \\u2014 scale-invariant)\n",
    "df['vol_sma_20'] = df['volume'].rolling(20).mean()\n",
    "df['volume_surge'] = df['volume'] / (df['vol_sma_20'] + 1e-10)\n",
    "df['volume_trend'] = df['volume'].rolling(12).mean() / (df['volume'].rolling(48).mean() + 1e-10)\n",
    "\n",
    "# Consecutive red candles (count)\n",
    "df['candle_dir'] = (df['close'] > df['open']).astype(int)\n",
    "groups = (df['candle_dir'] != df['candle_dir'].shift()).cumsum()\n",
    "df['consecutive_red'] = df.groupby(groups)['candle_dir'].cumcount()\n",
    "df.loc[df['candle_dir'] == 1, 'consecutive_red'] = 0\n",
    "\n",
    "# Drawdown from rolling 24h high (relative)\n",
    "df['rolling_high_24h'] = df['high'].rolling(288).max()\n",
    "df['drawdown_24h'] = df['close'] / df['rolling_high_24h'] - 1\n",
    "\n",
    "# RSI (normalized to [-1, 1])\n",
    "delta = df['close'].diff()\n",
    "gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "rs = gain / (loss + 1e-10)\n",
    "df['rsi_14_norm'] = (100 - (100 / (1 + rs)) - 50) / 50\n",
    "\n",
    "# Bollinger Band position (already normalized)\n",
    "bb_mid = df['close'].rolling(20).mean()\n",
    "bb_std = df['close'].rolling(20).std()\n",
    "df['bb_pct_b'] = (df['close'] - bb_mid) / (2 * bb_std + 1e-10)\n",
    "\n",
    "# VWAP distance (relative)\n",
    "# Use session-based VWAP (reset every 288 bars = 24h)\n",
    "df['session'] = np.arange(len(df)) // 288\n",
    "vwap = df.groupby('session').apply(\n",
    "    lambda g: (g['quote_volume'].cumsum() / (g['volume'].cumsum() + 1e-10))\n",
    ").reset_index(level=0, drop=True)\n",
    "df['vwap_distance'] = df['close'] / (vwap + 1e-10) - 1\n",
    "\n",
    "# \\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\n",
    "# GROUP 2: MACRO CORRELATION (10 features)\n",
    "# THE NEW SIGNAL \\u2014 BTC follows SPX in crashes\n",
    "# \\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\n",
    "\n",
    "# S&P 500\n",
    "if 'spx' in df.columns and df['spx'].notna().sum() > 100:\n",
    "    # Daily SPX return (same value for all bars in a day)\n",
    "    spx_daily = df.groupby('date')['spx'].last()\n",
    "    spx_returns = spx_daily.pct_change()\n",
    "    spx_ret_map = spx_returns.to_dict()\n",
    "    df['spx_return_1d'] = df['date'].map(spx_ret_map).fillna(0)\n",
    "\n",
    "    # SPX trend (vs 5-day SMA equivalent)\n",
    "    spx_sma = df['spx'].rolling(288 * 5, min_periods=288).mean()\n",
    "    df['spx_vs_sma'] = df['spx'] / (spx_sma + 1e-10) - 1\n",
    "else:\n",
    "    df['spx_return_1d'] = 0.0\n",
    "    df['spx_vs_sma'] = 0.0\n",
    "\n",
    "# VIX\n",
    "if 'vix' in df.columns and df['vix'].notna().sum() > 100:\n",
    "    df['vix_norm'] = (df['vix'] - 20) / 20\n",
    "    vix_daily = df.groupby('date')['vix'].last()\n",
    "    vix_change = vix_daily.pct_change()\n",
    "    vix_chg_map = vix_change.to_dict()\n",
    "    df['vix_change'] = df['date'].map(vix_chg_map).fillna(0)\n",
    "    df['vix_extreme'] = (df['vix'] > 30).astype(float)\n",
    "else:\n",
    "    df['vix_norm'] = 0.0\n",
    "    df['vix_change'] = 0.0\n",
    "    df['vix_extreme'] = 0.0\n",
    "\n",
    "# Dollar Index\n",
    "if 'dxy' in df.columns and df['dxy'].notna().sum() > 100:\n",
    "    dxy_daily = df.groupby('date')['dxy'].last()\n",
    "    dxy_returns = dxy_daily.pct_change()\n",
    "    dxy_ret_map = dxy_returns.to_dict()\n",
    "    df['dxy_return_1d'] = df['date'].map(dxy_ret_map).fillna(0)\n",
    "    dxy_sma = df['dxy'].rolling(288 * 20, min_periods=288).mean()\n",
    "    df['dxy_trend'] = df['dxy'] / (dxy_sma + 1e-10) - 1\n",
    "else:\n",
    "    df['dxy_return_1d'] = 0.0\n",
    "    df['dxy_trend'] = 0.0\n",
    "\n",
    "# Treasury Yields\n",
    "if 'us10y' in df.columns and df['us10y'].notna().sum() > 100:\n",
    "    df['yield_level'] = (df['us10y'] - 3.0) / 2.0\n",
    "    yield_daily = df.groupby('date')['us10y'].last()\n",
    "    yield_diff = yield_daily.diff()\n",
    "    yield_diff_map = yield_diff.to_dict()\n",
    "    df['yield_change'] = df['date'].map(yield_diff_map).fillna(0)\n",
    "else:\n",
    "    df['yield_level'] = 0.0\n",
    "    df['yield_change'] = 0.0\n",
    "\n",
    "# Fear & Greed\n",
    "if 'fng' in df.columns and df['fng'].notna().sum() > 100:\n",
    "    df['fng_norm'] = (df['fng'] - 50) / 50\n",
    "else:\n",
    "    df['fng_norm'] = 0.0\n",
    "\n",
    "# \\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\n",
    "# GROUP 3: DERIVATIVES (9 features)\n",
    "# \\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\n",
    "\n",
    "# Funding rate\n",
    "if 'funding_rate' in df.columns and df['funding_rate'].notna().sum() > 100:\n",
    "    df['funding_rate'] = df['funding_rate'].ffill().fillna(0)\n",
    "    fr_mean = df['funding_rate'].rolling(288 * 7, min_periods=288).mean()\n",
    "    fr_std = df['funding_rate'].rolling(288 * 7, min_periods=288).std()\n",
    "    df['funding_z'] = (df['funding_rate'] - fr_mean) / (fr_std + 1e-10)\n",
    "    df['funding_extreme_long'] = (df['funding_rate'] > 0.01).astype(float)\n",
    "    df['funding_extreme_short'] = (df['funding_rate'] < -0.01).astype(float)\n",
    "else:\n",
    "    df['funding_z'] = 0.0\n",
    "    df['funding_extreme_long'] = 0.0\n",
    "    df['funding_extreme_short'] = 0.0\n",
    "\n",
    "# Open Interest\n",
    "if 'oi_value' in df.columns and df['oi_value'].notna().sum() > 100:\n",
    "    df['oi_value'] = df['oi_value'].ffill().bfill()\n",
    "    df['oi_change_1h'] = df['oi_value'].pct_change(12)\n",
    "    df['oi_change_4h'] = df['oi_value'].pct_change(48)\n",
    "    df['oi_spike'] = (df['oi_change_1h'].abs() > 0.05).astype(float)\n",
    "else:\n",
    "    df['oi_change_1h'] = 0.0\n",
    "    df['oi_change_4h'] = 0.0\n",
    "    df['oi_spike'] = 0.0\n",
    "\n",
    "# Long/Short Ratio\n",
    "if 'long_short_ratio' in df.columns and df['long_short_ratio'].notna().sum() > 100:\n",
    "    df['ls_ratio_norm'] = df['long_short_ratio'].ffill().fillna(1.0) - 1.0\n",
    "    df['ls_extreme_long'] = (df['long_short_ratio'] > 2.0).astype(float)\n",
    "else:\n",
    "    df['ls_ratio_norm'] = 0.0\n",
    "    df['ls_extreme_long'] = 0.0\n",
    "\n",
    "# Taker Buy/Sell\n",
    "if 'taker_ratio' in df.columns and df['taker_ratio'].notna().sum() > 100:\n",
    "    df['taker_imbalance'] = df['taker_ratio'].ffill().fillna(1.0) - 1.0\n",
    "else:\n",
    "    df['taker_imbalance'] = 0.0\n",
    "\n",
    "# \\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\n",
    "# GROUP 4: CROSS-ASSET (6 features)\n",
    "# \\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\n",
    "\n",
    "if 'eth_close' in df.columns and df['eth_close'].notna().sum() > 100:\n",
    "    df['eth_return_1bar'] = df['eth_close'].pct_change(1)\n",
    "    df['eth_return_6bar'] = df['eth_close'].pct_change(6)\n",
    "    df['eth_btc_ratio'] = df['eth_close'] / (df['close'] + 1e-10)\n",
    "    df['eth_btc_ratio_change'] = df['eth_btc_ratio'].pct_change(12)\n",
    "    df['btc_lead_1'] = df['return_1bar'].shift(1)\n",
    "    df['btc_lead_2'] = df['return_1bar'].shift(2)\n",
    "    df['btc_lead_3'] = df['return_1bar'].shift(3)\n",
    "else:\n",
    "    df['eth_return_1bar'] = 0.0\n",
    "    df['eth_return_6bar'] = 0.0\n",
    "    df['eth_btc_ratio_change'] = 0.0\n",
    "    df['btc_lead_1'] = 0.0\n",
    "    df['btc_lead_2'] = 0.0\n",
    "    df['btc_lead_3'] = 0.0\n",
    "\n",
    "# \\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\n",
    "# LABELS: 6-bar forward return (30 min ahead)\n",
    "# \\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\n",
    "\n",
    "df['forward_return_6'] = df['close'].shift(-6) / df['close'] - 1\n",
    "df['label_binary'] = (df['forward_return_6'] > 0).astype(int)  # 1=up, 0=down\n",
    "df['label_soft'] = np.tanh(df['forward_return_6'] * 100)\n",
    "\n",
    "# \\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\n",
    "# FEATURE LIST (40 features)\n",
    "# \\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    # BTC price/volume (15)\n",
    "    'return_1bar', 'return_6bar', 'return_12bar', 'return_48bar', 'return_288bar',\n",
    "    'vol_12bar', 'vol_48bar', 'vol_ratio',\n",
    "    'volume_surge', 'volume_trend',\n",
    "    'consecutive_red', 'drawdown_24h',\n",
    "    'rsi_14_norm', 'bb_pct_b', 'vwap_distance',\n",
    "    # Macro (10)\n",
    "    'spx_return_1d', 'spx_vs_sma',\n",
    "    'vix_norm', 'vix_change', 'vix_extreme',\n",
    "    'dxy_return_1d', 'dxy_trend',\n",
    "    'yield_level', 'yield_change',\n",
    "    'fng_norm',\n",
    "    # Derivatives (9)\n",
    "    'funding_z', 'funding_extreme_long', 'funding_extreme_short',\n",
    "    'oi_change_1h', 'oi_change_4h', 'oi_spike',\n",
    "    'ls_ratio_norm', 'ls_extreme_long',\n",
    "    'taker_imbalance',\n",
    "    # Cross-asset (6)\n",
    "    'eth_return_1bar', 'eth_return_6bar', 'eth_btc_ratio_change',\n",
    "    'btc_lead_1', 'btc_lead_2', 'btc_lead_3',\n",
    "]\n",
    "\n",
    "# Replace inf with nan, then drop incomplete rows\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna(subset=FEATURE_COLS + ['label_binary']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n\\u2705 Feature engineering complete\")\n",
    "print(f\"Clean dataset: {len(df):,} rows with {len(FEATURE_COLS)} features\")\n",
    "print(f\"Label balance: {df['label_binary'].mean():.3f} (1=up)\")\n",
    "print(f\"\\nFeature groups:\")\n",
    "print(f\"  BTC price/volume:  15 features\")\n",
    "print(f\"  Macro correlation: 10 features\")\n",
    "print(f\"  Derivatives:        9 features\")\n",
    "print(f\"  Cross-asset:        6 features\")\n",
    "print(f\"  Total:             40 features\")\n",
    "\n",
    "# Save feature dataset to Drive\n",
    "df[FEATURE_COLS + ['timestamp', 'close', 'label_binary', 'label_soft', 'forward_return_6']].to_csv(\n",
    "    f'{DRIVE_SAVE}/crash_features.csv', index=False\n",
    ")\n",
    "print(f\"\\u2705 Feature dataset saved to Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Walk-Forward Split & Train LightGBM\n",
    "Train on crash 1+2, validate on first half of crash 3, test on second half. Crash 1 gets 0.5x weight (older market structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 7: Train crash-regime LightGBM\n",
    "# ============================================================\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import pickle\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# \\u2500\\u2500 Split by crash period (walk-forward) \\u2500\\u2500\n",
    "crash1 = df[df['timestamp'] < '2019-01-01']\n",
    "crash2 = df[(df['timestamp'] >= '2021-11-01') & (df['timestamp'] < '2023-01-01')]\n",
    "crash3 = df[df['timestamp'] >= '2025-10-01']\n",
    "\n",
    "print(f\"Crash 1 (2018):    {len(crash1):>8,} rows\")\n",
    "print(f\"Crash 2 (2021-22): {len(crash2):>8,} rows\")\n",
    "print(f\"Crash 3 (2025-26): {len(crash3):>8,} rows\")\n",
    "\n",
    "# Train on crash 1 + 2\n",
    "# Validate on first half of crash 3\n",
    "# Test on second half of crash 3\n",
    "train_data = pd.concat([crash1, crash2])\n",
    "\n",
    "if len(crash3) > 1000:\n",
    "    split_idx = len(crash3) // 2\n",
    "    val_data = crash3.iloc[:split_idx]\n",
    "    test_data = crash3.iloc[split_idx:]\n",
    "else:\n",
    "    # Fall back: split crash 2 if no crash 3 data\n",
    "    split_70 = int(len(crash2) * 0.7)\n",
    "    split_85 = int(len(crash2) * 0.85)\n",
    "    train_data = pd.concat([crash1, crash2.iloc[:split_70]])\n",
    "    val_data = crash2.iloc[split_70:split_85]\n",
    "    test_data = crash2.iloc[split_85:]\n",
    "\n",
    "# Sample weights: crash 1 gets 0.5x weight (older)\n",
    "train_weights = np.ones(len(train_data))\n",
    "crash1_mask = train_data['timestamp'] < '2019-01-01'\n",
    "train_weights[crash1_mask.values] = 0.5\n",
    "\n",
    "X_train = train_data[FEATURE_COLS].values\n",
    "y_train = train_data['label_binary'].values\n",
    "X_val = val_data[FEATURE_COLS].values\n",
    "y_val = val_data['label_binary'].values\n",
    "X_test = test_data[FEATURE_COLS].values\n",
    "y_test = test_data['label_binary'].values\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train):,} | Val: {len(X_val):,} | Test: {len(X_test):,}\")\n",
    "print(f\"Train up%: {y_train.mean():.3f} | Val up%: {y_val.mean():.3f} | Test up%: {y_test.mean():.3f}\")\n",
    "\n",
    "# \\u2500\\u2500 Train \\u2500\\u2500\n",
    "train_set = lgb.Dataset(X_train, label=y_train, weight=train_weights,\n",
    "                         feature_name=FEATURE_COLS)\n",
    "val_set = lgb.Dataset(X_val, label=y_val, feature_name=FEATURE_COLS)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 100,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    'verbose': -1,\n",
    "}\n",
    "\n",
    "print(\"\\n\\ud83d\\ude80 Training LightGBM (crash-regime)...\\n\")\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_set,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=[val_set],\n",
    "    valid_names=['val'],\n",
    "    callbacks=[lgb.early_stopping(30), lgb.log_evaluation(50)],\n",
    ")\n",
    "\n",
    "# \\u2500\\u2500 Evaluate \\u2500\\u2500\n",
    "val_probs = model.predict(X_val)\n",
    "test_probs = model.predict(X_test)\n",
    "\n",
    "val_acc = accuracy_score(y_val, (val_probs > 0.5).astype(int))\n",
    "test_acc = accuracy_score(y_test, (test_probs > 0.5).astype(int))\n",
    "val_auc = roc_auc_score(y_val, val_probs)\n",
    "test_auc = roc_auc_score(y_test, test_probs)\n",
    "pred_std = np.std(val_probs)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CRASH-REGIME LIGHTGBM RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Val accuracy:  {val_acc:.4f}  ({val_acc*100:.1f}%)\")\n",
    "print(f\"  Test accuracy: {test_acc:.4f}  ({test_acc*100:.1f}%)\")\n",
    "print(f\"  Val AUC:       {val_auc:.4f}\")\n",
    "print(f\"  Test AUC:      {test_auc:.4f}\")\n",
    "print(f\"  Pred std:      {pred_std:.4f}\")\n",
    "print(f\"  Best round:    {model.best_iteration}\")\n",
    "\n",
    "# \\u2500\\u2500 Feature importance \\u2500\\u2500\n",
    "importance = model.feature_importance(importance_type='gain')\n",
    "feat_imp = sorted(zip(FEATURE_COLS, importance), key=lambda x: -x[1])\n",
    "\n",
    "macro_feats = {'spx_return_1d','spx_vs_sma','vix_norm','vix_change','vix_extreme',\n",
    "               'dxy_return_1d','dxy_trend','yield_level','yield_change','fng_norm'}\n",
    "deriv_feats = {'funding_z','funding_extreme_long','funding_extreme_short',\n",
    "               'oi_change_1h','oi_change_4h','oi_spike','ls_ratio_norm',\n",
    "               'ls_extreme_long','taker_imbalance'}\n",
    "cross_feats = {'eth_return_1bar','eth_return_6bar','eth_btc_ratio_change',\n",
    "               'btc_lead_1','btc_lead_2','btc_lead_3'}\n",
    "\n",
    "print(f\"\\nTop 20 features by gain:\")\n",
    "for i, (feat, imp) in enumerate(feat_imp[:20], 1):\n",
    "    if feat in macro_feats:\n",
    "        group = \"MACRO\"\n",
    "    elif feat in deriv_feats:\n",
    "        group = \"DERIV\"\n",
    "    elif feat in cross_feats:\n",
    "        group = \"CROSS\"\n",
    "    else:\n",
    "        group = \"PRICE\"\n",
    "    print(f\"  {i:3d}. [{group:5s}] {feat:30s} {imp:>12,.0f}\")\n",
    "\n",
    "# Calculate group importance totals\n",
    "group_imp = {'PRICE': 0, 'MACRO': 0, 'DERIV': 0, 'CROSS': 0}\n",
    "for feat, imp in feat_imp:\n",
    "    if feat in macro_feats:\n",
    "        group_imp['MACRO'] += imp\n",
    "    elif feat in deriv_feats:\n",
    "        group_imp['DERIV'] += imp\n",
    "    elif feat in cross_feats:\n",
    "        group_imp['CROSS'] += imp\n",
    "    else:\n",
    "        group_imp['PRICE'] += imp\n",
    "\n",
    "total_imp = sum(group_imp.values())\n",
    "print(f\"\\nFeature group importance:\")\n",
    "for group, imp in sorted(group_imp.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {group:5s}: {imp:>12,.0f} ({100*imp/total_imp:.1f}%)\")\n",
    "\n",
    "# \\u2500\\u2500 Save model \\u2500\\u2500\n",
    "model.save_model('/content/models/crash_lightgbm_model.txt')\n",
    "with open('/content/models/crash_lightgbm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "meta = {\n",
    "    'model_type': 'lightgbm_crash_regime',\n",
    "    'regime': 'CRASH',\n",
    "    'val_accuracy': float(val_acc),\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'val_auc': float(val_auc),\n",
    "    'test_auc': float(test_auc),\n",
    "    'pred_std': float(pred_std),\n",
    "    'best_round': int(model.best_iteration),\n",
    "    'n_features': len(FEATURE_COLS),\n",
    "    'feature_names': FEATURE_COLS,\n",
    "    'feature_importance': {f: float(i) for f, i in feat_imp},\n",
    "    'train_rows': int(len(X_train)),\n",
    "    'val_rows': int(len(X_val)),\n",
    "    'test_rows': int(len(X_test)),\n",
    "    'crash_periods': CRASH_PERIODS,\n",
    "    'crash1_weight': 0.5,\n",
    "    'params': params,\n",
    "    'trained_at': datetime.now(datetime.UTC).isoformat(),\n",
    "}\n",
    "with open('/content/models/crash_lightgbm_meta.json', 'w') as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "# \\u2500\\u2500 SAVE TO DRIVE IMMEDIATELY \\u2500\\u2500\n",
    "for fname in os.listdir('/content/models/'):\n",
    "    shutil.copy(f'/content/models/{fname}', f'{DRIVE_SAVE}/{fname}')\n",
    "    sz = os.path.getsize(f'/content/models/{fname}') / 1024\n",
    "    print(f\"\\u2705 Saved to Drive: {fname} ({sz:.1f} KB)\")\n",
    "\n",
    "print(f\"\\n\\ud83c\\udfaf Crash-regime LightGBM trained and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Confidence Calibration Analysis\n",
    "Critical for Polymarket: when the model says 85% confident, how often is it actually right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8: Confidence calibration analysis\n",
    "# ============================================================\n",
    "# This answers: \"When the model says 85% confident, is it right 85% of the time?\"\n",
    "# Critical for Polymarket strategy which bets at 85%+ confidence.\n",
    "\n",
    "# Use TEST set (out-of-sample)\n",
    "probs = test_probs.copy()\n",
    "actuals = y_test.copy()\n",
    "\n",
    "# Model outputs probability of UP (0.0 to 1.0)\n",
    "# Confidence = how far from 50/50\n",
    "# prob=0.85 means 85% confident UP \\u2192 confidence = 0.70 (distance from 0.5, scaled)\n",
    "# prob=0.15 means 85% confident DOWN \\u2192 confidence = 0.70\n",
    "\n",
    "confidence = np.abs(probs - 0.5) * 2  # Scale to 0-1\n",
    "predicted_up = (probs > 0.5).astype(int)\n",
    "correct = (predicted_up == actuals).astype(int)\n",
    "\n",
    "# Bin by model probability (both directions)\n",
    "print(f\"{'Probability':>12} {'Direction':>10} {'Count':>8} {'Accuracy':>10} {'Bet?':>8}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "prob_bins = [\n",
    "    (0.50, 0.55, 'UP'),\n",
    "    (0.55, 0.60, 'UP'),\n",
    "    (0.60, 0.65, 'UP'),\n",
    "    (0.65, 0.70, 'UP'),\n",
    "    (0.70, 0.75, 'UP'),\n",
    "    (0.75, 0.80, 'UP'),\n",
    "    (0.80, 0.85, 'UP'),\n",
    "    (0.85, 0.90, 'UP'),\n",
    "    (0.90, 0.95, 'UP'),\n",
    "    (0.95, 1.01, 'UP'),\n",
    "]\n",
    "\n",
    "for lo, hi, direction in prob_bins:\n",
    "    mask = (probs >= lo) & (probs < hi)\n",
    "    if mask.sum() > 0:\n",
    "        acc = correct[mask].mean()\n",
    "        bet = \"\\u2705 BET\" if lo >= 0.85 else \"\\u2014\"\n",
    "        print(f\"  {lo:.0%}-{hi:.0%}      {'UP':>10} {mask.sum():>8,} {acc:>10.1%} {bet:>8}\")\n",
    "\n",
    "print()\n",
    "# Also check DOWN predictions (prob < 0.5)\n",
    "for lo, hi in [(0.05, 0.10), (0.10, 0.15), (0.15, 0.20), (0.20, 0.25),\n",
    "               (0.25, 0.30), (0.30, 0.35), (0.35, 0.40), (0.40, 0.45), (0.45, 0.50)]:\n",
    "    mask = (probs >= lo) & (probs < hi)\n",
    "    if mask.sum() > 0:\n",
    "        # For DOWN predictions, \\\"correct\\\" means actual=0 (down)\n",
    "        down_correct = (actuals[mask] == 0).mean()\n",
    "        bet = \"\\u2705 BET\" if hi <= 0.15 else \"\\u2014\"\n",
    "        print(f\"  {lo:.0%}-{hi:.0%}      {'DOWN':>10} {mask.sum():>8,} {down_correct:>10.1%} {bet:>8}\")\n",
    "\n",
    "# Summary for Polymarket thresholds\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(f\"POLYMARKET DECISION THRESHOLDS\")\n",
    "print(f\"{'='*55}\")\n",
    "\n",
    "# 85%+ confident UP (prob >= 0.85)\n",
    "mask_85_up = probs >= 0.85\n",
    "if mask_85_up.sum() > 0:\n",
    "    acc = correct[mask_85_up].mean()\n",
    "    print(f\"  85%+ UP:   {mask_85_up.sum():>6,} predictions, {acc:.1%} accuracy\")\n",
    "else:\n",
    "    print(f\"  85%+ UP:   0 predictions\")\n",
    "\n",
    "# 85%+ confident DOWN (prob <= 0.15)\n",
    "mask_85_down = probs <= 0.15\n",
    "if mask_85_down.sum() > 0:\n",
    "    acc = (actuals[mask_85_down] == 0).mean()\n",
    "    print(f\"  85%+ DOWN: {mask_85_down.sum():>6,} predictions, {acc:.1%} accuracy\")\n",
    "else:\n",
    "    print(f\"  85%+ DOWN: 0 predictions\")\n",
    "\n",
    "# Combined high confidence\n",
    "mask_85_any = (probs >= 0.85) | (probs <= 0.15)\n",
    "if mask_85_any.sum() > 0:\n",
    "    hc_correct = np.where(probs[mask_85_any] >= 0.5,\n",
    "                          actuals[mask_85_any] == 1,\n",
    "                          actuals[mask_85_any] == 0)\n",
    "    print(f\"  85%+ ANY:  {mask_85_any.sum():>6,} predictions, {hc_correct.mean():.1%} accuracy\")\n",
    "    print(f\"\\n  \\u2192 Polymarket should {'BET' if hc_correct.mean() > 0.60 else 'WAIT \\u2014 models not calibrated yet'}\")\n",
    "else:\n",
    "    print(f\"  85%+ ANY:  0 predictions \\u2014 model never reaches 85% confidence\")\n",
    "    # Check what confidence it does reach\n",
    "    max_conf = confidence.max()\n",
    "    print(f\"  Max confidence seen: {50 + max_conf*50:.1f}%\")\n",
    "    for thresh in [0.60, 0.65, 0.70, 0.75, 0.80]:\n",
    "        mask = confidence >= (thresh - 0.5) * 2\n",
    "        if mask.sum() > 10:\n",
    "            hc = np.where(probs[mask] >= 0.5, actuals[mask] == 1, actuals[mask] == 0)\n",
    "            print(f\"  {thresh:.0%}+ conf: {mask.sum():>6,} predictions, {hc.mean():.1%} accuracy\")\n",
    "\n",
    "# Save calibration data to Drive\n",
    "cal_df = pd.DataFrame({\n",
    "    'probability': probs,\n",
    "    'confidence': confidence,\n",
    "    'predicted_up': predicted_up,\n",
    "    'actual_up': actuals,\n",
    "    'correct': correct,\n",
    "})\n",
    "cal_df.to_csv(f'{DRIVE_SAVE}/calibration_analysis.csv', index=False)\n",
    "print(f\"\\n\\u2705 Calibration data saved to Drive\")\n",
    "\n",
    "# \\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\n",
    "# FINAL SUMMARY\n",
    "# \\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(f\"TRAINING COMPLETE \\u2014 CRASH REGIME LIGHTGBM\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Accuracy:       {test_acc:.1%}\")\n",
    "print(f\"  AUC:            {test_auc:.4f}\")\n",
    "print(f\"  Features:       {len(FEATURE_COLS)} (40)\")\n",
    "print(f\"  Training data:  {len(X_train):,} crash-period rows\")\n",
    "print(f\"  Model saved to: {DRIVE_SAVE}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if test_acc > 0.53:\n",
    "    print(f\"\\n\\ud83c\\udfaf ACCURACY ABOVE 53% \\u2014 Ready to deploy!\")\n",
    "    print(f\"   Download crash_lightgbm_model.pkl from Google Drive\")\n",
    "    print(f\"   and place in models/trained/ on the VPS.\")\n",
    "elif test_acc > 0.51:\n",
    "    print(f\"\\n\\ud83d\\udcca MARGINAL IMPROVEMENT \\u2014 Consider adding more features\")\n",
    "    print(f\"   Try intraday SPX data (1-min) instead of daily\")\n",
    "else:\n",
    "    print(f\"\\n\\u26a0\\ufe0f NO IMPROVEMENT \\u2014 Macro features may not help at 5-min resolution\")\n",
    "    print(f\"   Consider longer prediction horizon (1h, 4h) or different features\")"
   ]
  }
 ]
}