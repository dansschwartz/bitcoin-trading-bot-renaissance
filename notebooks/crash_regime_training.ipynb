{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Crash-Regime LightGBM Training\n",
    "## Renaissance Trading Bot -- Regime-Specific Models\n",
    "\n",
    "**What this does:**\n",
    "- Downloads BTC + ETH candles, macro data (SPX, VIX, DXY), Binance derivatives\n",
    "- Filters to crash periods only (2018, 2021-22, 2025-26)\n",
    "- Engineers 40 crash-specific features across 4 groups\n",
    "- Trains LightGBM on crash-only data with macro features\n",
    "- Saves everything to Google Drive (survives runtime disconnects)\n",
    "\n",
    "**Key insight:** BTC has 0.77-0.90 correlation with S&P 500 during crashes.\n",
    "Our current models miss this signal entirely, achieving only 51% accuracy.\n",
    "Training on crash-only data with macro features should improve this.\n",
    "\n",
    "**Runtime:** Select GPU (T4) for faster training, though LightGBM trains in ~30 seconds either way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & Google Drive Mount\n",
    "Mount Drive first so all outputs survive runtime disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Setup & Google Drive Mount\n",
    "# ============================================================\n",
    "# Mount Drive FIRST -- all outputs save here to survive disconnects\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "os.makedirs('/content/models', exist_ok=True)\n",
    "\n",
    "DRIVE_SAVE = '/content/drive/MyDrive/renaissance-bot-training/crash_models/'\n",
    "os.makedirs(DRIVE_SAVE, exist_ok=True)\n",
    "\n",
    "!pip install -q yfinance lightgbm pandas numpy scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"\\u2705 Setup complete\")\n",
    "print(f\"Drive save path: {DRIVE_SAVE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Download BTC + ETH 5-Minute Candles from Binance\n",
    "Full history from Sep 2017. Covers all three crash periods. Takes 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 2: Download BTC + ETH 5m candles from Binance\n# ============================================================\n# Need crash periods:\n#   Crash 1: Jan 2018 -- Dec 2018\n#   Crash 2: Nov 2021 -- Nov 2022\n#   Crash 3: Oct 2025 -- Feb 2026 (current)\n\ndef fetch_binance_klines(symbol, interval, start_ms, end_ms, limit=1000):\n    \"\"\"Fetch klines from Binance public API. No auth needed.\"\"\"\n    url = \"https://api.binance.com/api/v3/klines\"\n    all_data = []\n    current = start_ms\n    retries = 0\n\n    while current < end_ms:\n        params = {\n            'symbol': symbol,\n            'interval': interval,\n            'startTime': current,\n            'endTime': end_ms,\n            'limit': limit,\n        }\n        try:\n            resp = requests.get(url, params=params, timeout=30)\n            if resp.status_code == 429:\n                print(\"  Rate limited, waiting 60s...\")\n                time.sleep(60)\n                continue\n            if resp.status_code != 200:\n                print(f\"  HTTP {resp.status_code}, retrying...\")\n                retries += 1\n                if retries > 5:\n                    break\n                time.sleep(5)\n                continue\n\n            data = resp.json()\n            if not data or not isinstance(data, list):\n                break\n\n            all_data.extend(data)\n            current = data[-1][0] + 1\n            retries = 0\n\n            if len(all_data) % 50000 == 0:\n                print(f\"  ... {len(all_data):,} candles so far\")\n\n            if len(data) < limit:\n                break\n\n            time.sleep(0.1)\n\n        except Exception as e:\n            print(f\"  Error: {e}, retrying...\")\n            retries += 1\n            if retries > 5:\n                break\n            time.sleep(5)\n\n    return all_data\n\ndef klines_to_df(raw):\n    \"\"\"Convert Binance klines to clean DataFrame.\"\"\"\n    df = pd.DataFrame(raw, columns=[\n        'open_time', 'open', 'high', 'low', 'close', 'volume',\n        'close_time', 'quote_volume', 'trades', 'taker_buy_base',\n        'taker_buy_quote', 'ignore'\n    ])\n    for col in ['open', 'high', 'low', 'close', 'volume', 'quote_volume',\n                'taker_buy_base', 'taker_buy_quote']:\n        df[col] = df[col].astype(float)\n    df['trades'] = df['trades'].astype(int)\n    df['timestamp'] = pd.to_datetime(df['open_time'], unit='ms')\n    df = df.drop_duplicates(subset=['open_time']).sort_values('timestamp').reset_index(drop=True)\n    return df\n\nstart_ms = int(datetime(2017, 9, 1).timestamp() * 1000)\nend_ms = int(datetime.utcnow().timestamp() * 1000)\n\n# Download BTC\nprint(\"Downloading BTC 5m candles from Binance (this takes 5-10 minutes)...\")\nraw = fetch_binance_klines('BTCUSDT', '5m', start_ms, end_ms)\nbtc_5m = klines_to_df(raw)\nbtc_5m.to_csv('/content/data/btc_5m_full.csv', index=False)\nprint(f\"\\u2705 BTC: {len(btc_5m):,} candles ({btc_5m['timestamp'].min().date()} \\u2192 {btc_5m['timestamp'].max().date()})\")\n\n# Download ETH (for cross-asset features)\nprint(\"\\nDownloading ETH 5m candles...\")\nraw = fetch_binance_klines('ETHUSDT', '5m', start_ms, end_ms)\neth_5m = klines_to_df(raw)\neth_5m.to_csv('/content/data/eth_5m_full.csv', index=False)\nprint(f\"\\u2705 ETH: {len(eth_5m):,} candles ({eth_5m['timestamp'].min().date()} \\u2192 {eth_5m['timestamp'].max().date()})\")\n\n# Save to Drive as backup\nbtc_5m.to_csv(f'{DRIVE_SAVE}/btc_5m_full.csv', index=False)\neth_5m.to_csv(f'{DRIVE_SAVE}/eth_5m_full.csv', index=False)\nprint(\"\\n\\u2705 Backed up to Drive\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Download Macro Data (Daily + 5-Minute Intraday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 3: Download macro data -- daily + 5-minute intraday\n# ============================================================\n# BTC has 0.77-0.90 correlation with S&P 500 in crash periods.\n# Daily macro covers full history; intraday only last ~60 days (yfinance limit).\n\nimport yfinance as yf\n\n# --- Part A: Daily macro (full history, same as v1) ---\nmacro_tickers = {\n    'spx': '^GSPC',       # S&P 500\n    'ndx': '^IXIC',       # Nasdaq Composite\n    'vix': '^VIX',        # CBOE Volatility Index\n    'dxy': 'DX-Y.NYB',    # US Dollar Index\n    'us10y': '^TNX',      # 10-Year Treasury Yield\n    'gold': 'GC=F',       # Gold Futures\n}\n\nmacro_daily = {}\nfor name, ticker in macro_tickers.items():\n    print(f\"Downloading daily {name} ({ticker})...\")\n    try:\n        data = yf.download(ticker, start='2017-01-01', progress=False)\n        if len(data) > 0:\n            if isinstance(data.columns, pd.MultiIndex):\n                data.columns = data.columns.get_level_values(0)\n            macro_daily[name] = data[['Close']].rename(columns={'Close': name})\n            print(f\"  [OK] {name}: {len(data):,} days ({data.index.min().date()} to {data.index.max().date()})\")\n        else:\n            print(f\"  [WARN] {name}: no data returned\")\n    except Exception as e:\n        print(f\"  [ERR] {name}: {e}\")\n\nmacro_df = pd.concat(macro_daily.values(), axis=1)\nmacro_df.index = pd.to_datetime(macro_df.index)\nmacro_df = macro_df.ffill().bfill()\nmacro_df.to_csv('/content/data/macro_daily.csv')\nprint(f\"\\n[OK] Daily macro: {len(macro_df):,} days, columns: {list(macro_df.columns)}\")\n\n# --- Part B: 5-minute intraday macro (last ~60 days only) ---\n# yfinance allows 5m data for the last 60 days.\n# This gives us real-time SPX/VIX/NDX at BTC candle resolution\n# for the most recent crash period (crash 3).\n\nintraday_tickers = {\n    'spx_5m': '^GSPC',\n    'vix_5m': '^VIX',\n    'ndx_5m': '^IXIC',\n}\n\nintraday_frames = {}\nfor name, ticker in intraday_tickers.items():\n    print(f\"Downloading 5m intraday {name} ({ticker})...\")\n    try:\n        data = yf.download(ticker, period='60d', interval='5m', progress=False)\n        if len(data) > 0:\n            if isinstance(data.columns, pd.MultiIndex):\n                data.columns = data.columns.get_level_values(0)\n            col_name = name.replace('_5m', '_intraday')\n            intraday_frames[col_name] = data[['Close']].rename(columns={'Close': col_name})\n            print(f\"  [OK] {name}: {len(data):,} bars ({data.index.min()} to {data.index.max()})\")\n        else:\n            print(f\"  [WARN] {name}: no data returned\")\n    except Exception as e:\n        print(f\"  [ERR] {name}: {e}\")\n\nif intraday_frames:\n    intraday_df = pd.concat(intraday_frames.values(), axis=1)\n    intraday_df.index = pd.to_datetime(intraday_df.index).tz_localize(None)\n    intraday_df = intraday_df.ffill()\n    intraday_df.to_csv('/content/data/macro_intraday_5m.csv')\n    print(f\"\\n[OK] Intraday macro: {len(intraday_df):,} bars, columns: {list(intraday_df.columns)}\")\n    print(f\"     Date range: {intraday_df.index.min()} to {intraday_df.index.max()}\")\nelse:\n    intraday_df = None\n    print(\"\\n[WARN] No intraday macro data available\")\n\n# --- Part C: Fear & Greed Index (daily) ---\nprint(\"\\nDownloading Fear & Greed Index...\")\ntry:\n    fng_resp = requests.get(\"https://api.alternative.me/fng/?limit=0\", timeout=30)\n    fng_data = fng_resp.json()['data']\n    fng_df = pd.DataFrame(fng_data)\n    fng_df['timestamp'] = pd.to_datetime(fng_df['timestamp'].astype(int), unit='s')\n    fng_df['fng'] = fng_df['value'].astype(int)\n    fng_df = fng_df[['timestamp', 'fng']].set_index('timestamp').sort_index()\n    fng_df.to_csv('/content/data/fear_greed.csv')\n    print(f\"  [OK] Fear & Greed: {len(fng_df):,} days\")\nexcept Exception as e:\n    print(f\"  [ERR] Fear & Greed: {e}\")\n    fng_df = None\n\n# Save to Drive\nmacro_df.to_csv(f'{DRIVE_SAVE}/macro_daily.csv')\nif intraday_df is not None:\n    intraday_df.to_csv(f'{DRIVE_SAVE}/macro_intraday_5m.csv')\nprint(\"[OK] Backed up to Drive\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Download Binance Derivatives Data (1h Period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 4: Download Binance Futures derivatives data (1h period)\n# ============================================================\n# Funding rate, Open Interest, Long/Short ratio, Taker buy/sell.\n# IMPORTANT: OI/LS/Taker endpoints only retain ~30 days for 5m period.\n# Using 1h period gives ~180 days of history -- much better coverage.\n# Funding rate is always 8-hourly (no period param needed).\n\ndef fetch_binance_futures(endpoint, symbol, period=None, limit=500,\n                          start_ms=None, end_ms=None, max_records=50000):\n    \"\"\"Fetch data from Binance Futures API with robust pagination.\"\"\"\n    base = \"https://fapi.binance.com\"\n    all_data = []\n    current = start_ms\n    retries = 0\n\n    while True:\n        params = {'symbol': symbol, 'limit': limit}\n        if period and 'fundingRate' not in endpoint:\n            params['period'] = period\n        if current:\n            params['startTime'] = current\n        if end_ms:\n            params['endTime'] = end_ms\n\n        try:\n            resp = requests.get(f\"{base}{endpoint}\", params=params, timeout=30)\n            if resp.status_code == 429:\n                print(\"    Rate limited, waiting 60s...\")\n                time.sleep(60)\n                continue\n            if resp.status_code != 200:\n                retries += 1\n                if retries > 5:\n                    print(f\"    Giving up after {retries} retries (status {resp.status_code})\")\n                    break\n                time.sleep(5)\n                continue\n\n            data = resp.json()\n            if not data or not isinstance(data, list):\n                break\n\n            all_data.extend(data)\n            retries = 0\n\n            # Find timestamp key for pagination\n            last_row = data[-1]\n            if 'fundingTime' in last_row:\n                current = last_row['fundingTime'] + 1\n            elif 'timestamp' in last_row:\n                current = last_row['timestamp'] + 1\n            else:\n                break\n\n            if len(data) < limit:\n                break\n\n            if len(all_data) >= max_records:\n                print(f\"    Hit max_records limit ({max_records})\")\n                break\n\n            if len(all_data) % 5000 == 0:\n                print(f\"    ... {len(all_data):,} records so far\")\n\n            time.sleep(0.3)  # Slightly slower to avoid 429\n\n        except Exception as e:\n            retries += 1\n            if retries > 5:\n                print(f\"    Giving up after {retries} retries: {e}\")\n                break\n            time.sleep(5)\n\n    return all_data\n\n\n# Focus on last 180 days for derivatives (1h period has good coverage)\nend_ms = int(datetime.utcnow().timestamp() * 1000)\nstart_180d = int((datetime.utcnow() - timedelta(days=180)).timestamp() * 1000)\n# Funding rate available from 2019, use full range\nstart_funding = int(datetime(2019, 9, 1).timestamp() * 1000)\n\nderivatives = {}\n\n# 1. Funding Rate (8-hourly, no period param)\nprint(\"Downloading BTC funding rate (8h intervals, full history)...\")\nraw = fetch_binance_futures('/fapi/v1/fundingRate', 'BTCUSDT',\n                            start_ms=start_funding, end_ms=end_ms)\nif raw:\n    fr_df = pd.DataFrame(raw)\n    fr_df['timestamp'] = pd.to_datetime(fr_df['fundingTime'], unit='ms')\n    fr_df['funding_rate'] = fr_df['fundingRate'].astype(float)\n    derivatives['funding_rate'] = fr_df[['timestamp', 'funding_rate']]\n    fr_df[['timestamp', 'funding_rate']].to_csv('/content/data/btc_funding_rate.csv', index=False)\n    print(f\"  [OK] Funding rate: {len(fr_df):,} records ({fr_df['timestamp'].min().date()} to {fr_df['timestamp'].max().date()})\")\nelse:\n    print(\"  [WARN] No funding rate data\")\n\n# 2. Open Interest (1h period -- ~180 days available)\nprint(\"Downloading BTC open interest (1h, last 180 days)...\")\nraw = fetch_binance_futures('/futures/data/openInterestHist', 'BTCUSDT',\n                            period='1h', start_ms=start_180d, end_ms=end_ms)\nif raw:\n    oi_df = pd.DataFrame(raw)\n    oi_df['timestamp'] = pd.to_datetime(oi_df['timestamp'], unit='ms')\n    oi_df['open_interest'] = oi_df['sumOpenInterest'].astype(float)\n    oi_df['oi_value'] = oi_df['sumOpenInterestValue'].astype(float)\n    derivatives['open_interest'] = oi_df[['timestamp', 'open_interest', 'oi_value']]\n    oi_df[['timestamp', 'open_interest', 'oi_value']].to_csv('/content/data/btc_open_interest.csv', index=False)\n    print(f\"  [OK] Open interest: {len(oi_df):,} records ({oi_df['timestamp'].min().date()} to {oi_df['timestamp'].max().date()})\")\nelse:\n    print(\"  [WARN] No open interest data\")\n\n# 3. Long/Short Ratio (1h period)\nprint(\"Downloading BTC long/short ratio (1h, last 180 days)...\")\nraw = fetch_binance_futures('/futures/data/globalLongShortAccountRatio',\n                            'BTCUSDT', period='1h',\n                            start_ms=start_180d, end_ms=end_ms)\nif raw:\n    ls_df = pd.DataFrame(raw)\n    ls_df['timestamp'] = pd.to_datetime(ls_df['timestamp'], unit='ms')\n    ls_df['long_short_ratio'] = ls_df['longShortRatio'].astype(float)\n    ls_df['long_account'] = ls_df['longAccount'].astype(float)\n    ls_df['short_account'] = ls_df['shortAccount'].astype(float)\n    derivatives['long_short'] = ls_df[['timestamp', 'long_short_ratio',\n                                        'long_account', 'short_account']]\n    ls_df[['timestamp', 'long_short_ratio', 'long_account', 'short_account']].to_csv(\n        '/content/data/btc_long_short.csv', index=False)\n    print(f\"  [OK] Long/short ratio: {len(ls_df):,} records ({ls_df['timestamp'].min().date()} to {ls_df['timestamp'].max().date()})\")\nelse:\n    print(\"  [WARN] No long/short data\")\n\n# 4. Taker Buy/Sell Volume (1h period)\nprint(\"Downloading BTC taker buy/sell volume (1h, last 180 days)...\")\nraw = fetch_binance_futures('/futures/data/takeBuySellVol', 'BTCUSDT',\n                            period='1h', start_ms=start_180d, end_ms=end_ms)\nif raw:\n    tv_df = pd.DataFrame(raw)\n    tv_df['timestamp'] = pd.to_datetime(tv_df['timestamp'], unit='ms')\n    tv_df['taker_buy_vol'] = tv_df['buyVol'].astype(float)\n    tv_df['taker_sell_vol'] = tv_df['sellVol'].astype(float)\n    tv_df['taker_ratio'] = tv_df['taker_buy_vol'] / (tv_df['taker_sell_vol'] + 1e-10)\n    derivatives['taker_vol'] = tv_df[['timestamp', 'taker_buy_vol',\n                                       'taker_sell_vol', 'taker_ratio']]\n    tv_df[['timestamp', 'taker_buy_vol', 'taker_sell_vol', 'taker_ratio']].to_csv(\n        '/content/data/btc_taker_vol.csv', index=False)\n    print(f\"  [OK] Taker volume: {len(tv_df):,} records ({tv_df['timestamp'].min().date()} to {tv_df['timestamp'].max().date()})\")\nelse:\n    print(\"  [WARN] No taker volume data\")\n\nprint(f\"\\n[OK] Derivatives data complete. Got {len(derivatives)} datasets.\")\nfor name, df in derivatives.items():\n    print(f\"  {name}: {len(df):,} records\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Label Crash Periods & Merge All Data\n",
    "Merge BTC 5m candles with macro (daily), derivatives (variable freq via merge_asof), ETH cross-asset, and Fear & Greed. Filter to crash periods only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 5: Label crash periods and merge everything\n# ============================================================\n\n# Load BTC 5m\nbtc = pd.read_csv('/content/data/btc_5m_full.csv')\nbtc['timestamp'] = pd.to_datetime(btc['timestamp'])\nbtc = btc.sort_values('timestamp').reset_index(drop=True)\n\n# -- Define crash periods --\n# Crash 1: Post-2017 bubble, Jan 2018 peak -> Dec 2018 bottom\n# Crash 2: Post-2021 bubble, Nov 2021 ATH $69K -> Nov 2022 bottom $15.5K\n# Crash 3: Post-2025 bubble, Oct 2025 ATH $126K -> ongoing (~$65K)\nCRASH_PERIODS = [\n    ('2018-01-07', '2018-12-15'),\n    ('2021-11-10', '2022-11-21'),\n    ('2025-10-06', '2026-02-28'),\n]\n\nbtc['is_crash'] = False\nfor start, end in CRASH_PERIODS:\n    mask = (btc['timestamp'] >= start) & (btc['timestamp'] <= end)\n    btc.loc[mask, 'is_crash'] = True\n\ncrash_data = btc[btc['is_crash']].copy()\nprint(f\"Total BTC candles: {len(btc):,}\")\nprint(f\"Crash candles: {len(crash_data):,} ({100*len(crash_data)/len(btc):.1f}%)\")\nfor start, end in CRASH_PERIODS:\n    n = len(btc[(btc['timestamp'] >= start) & (btc['timestamp'] <= end)])\n    print(f\"  {start} to {end}: {n:,} candles\")\n\n# -- Merge daily macro (via date join) --\nmacro = pd.read_csv('/content/data/macro_daily.csv', index_col=0, parse_dates=True)\ncrash_data['date'] = crash_data['timestamp'].dt.strftime('%Y-%m-%d')\nmacro['date'] = macro.index.strftime('%Y-%m-%d')\ncrash_data = crash_data.merge(macro, on='date', how='left')\nfor col in ['spx', 'ndx', 'vix', 'dxy', 'us10y', 'gold']:\n    if col in crash_data.columns:\n        crash_data[col] = crash_data[col].ffill().bfill()\nprint(f\"\\n[OK] Merged daily macro data\")\n\n# -- Merge 5-minute intraday macro (NEW in v2) --\n# Only available for last ~60 days (crash 3 period).\n# Rows without intraday data get NaN -- features will be 0-filled.\nhas_intraday_macro = False\ntry:\n    intraday = pd.read_csv('/content/data/macro_intraday_5m.csv', index_col=0, parse_dates=True)\n    intraday.index = pd.to_datetime(intraday.index)\n    intraday = intraday.sort_index()\n    crash_data_sorted = crash_data.sort_values('timestamp').reset_index(drop=True)\n    crash_data_sorted['timestamp'] = pd.to_datetime(crash_data_sorted['timestamp'])\n    crash_data = pd.merge_asof(\n        crash_data_sorted, intraday.reset_index().rename(columns={'index': 'timestamp'}),\n        on='timestamp', direction='backward',\n        tolerance=pd.Timedelta('10min')  # Allow 10min tolerance for market hours gaps\n    )\n    for col in ['spx_intraday', 'vix_intraday', 'ndx_intraday']:\n        if col in crash_data.columns:\n            filled = crash_data[col].notna().sum()\n            print(f\"  [OK] {col}: {filled:,}/{len(crash_data):,} rows filled\")\n    has_intraday_macro = True\n    print(f\"[OK] Merged intraday macro (5-min resolution)\")\nexcept FileNotFoundError:\n    print(\"[WARN] No intraday macro file found -- features will be zero-filled\")\n    crash_data['spx_intraday'] = np.nan\n    crash_data['vix_intraday'] = np.nan\n    crash_data['ndx_intraday'] = np.nan\nexcept Exception as e:\n    print(f\"[WARN] Intraday macro merge failed: {e}\")\n    crash_data['spx_intraday'] = np.nan\n    crash_data['vix_intraday'] = np.nan\n    crash_data['ndx_intraday'] = np.nan\n\n# -- Merge Fear & Greed (daily -> 5m via date join) --\ntry:\n    fng = pd.read_csv('/content/data/fear_greed.csv', parse_dates=['timestamp'])\n    fng['date'] = fng['timestamp'].dt.strftime('%Y-%m-%d')\n    crash_data = crash_data.merge(fng[['date', 'fng']], on='date', how='left')\n    crash_data['fng'] = crash_data['fng'].ffill().bfill().fillna(50)\n    print(f\"[OK] Merged Fear & Greed\")\nexcept Exception as e:\n    crash_data['fng'] = 50\n    print(f\"[WARN] Fear & Greed failed, using default 50: {e}\")\n\n# -- Merge derivatives (variable freq -> 5m via merge_asof) --\ncrash_data = crash_data.sort_values('timestamp').reset_index(drop=True)\n\nderiv_files = {\n    'funding_rate': ('/content/data/btc_funding_rate.csv', ['funding_rate']),\n    'open_interest': ('/content/data/btc_open_interest.csv', ['open_interest', 'oi_value']),\n    'long_short': ('/content/data/btc_long_short.csv', ['long_short_ratio', 'long_account', 'short_account']),\n    'taker_vol': ('/content/data/btc_taker_vol.csv', ['taker_buy_vol', 'taker_sell_vol', 'taker_ratio']),\n}\n\nfor name, (path, cols) in deriv_files.items():\n    try:\n        deriv = pd.read_csv(path, parse_dates=['timestamp'])\n        deriv = deriv.sort_values('timestamp').reset_index(drop=True)\n        crash_data = pd.merge_asof(\n            crash_data, deriv[['timestamp'] + cols],\n            on='timestamp', direction='backward',\n            tolerance=pd.Timedelta('8h')  # Funding rate is 8-hourly\n        )\n        filled = crash_data[cols[0]].notna().sum()\n        print(f\"[OK] Merged {name}: {filled:,}/{len(crash_data):,} rows filled\")\n    except Exception as e:\n        for col in cols:\n            crash_data[col] = np.nan\n        print(f\"[WARN] {name} merge failed: {e}\")\n\n# -- Merge ETH cross-asset data --\ntry:\n    eth = pd.read_csv('/content/data/eth_5m_full.csv')\n    eth['timestamp'] = pd.to_datetime(eth['timestamp'])\n    eth = eth.sort_values('timestamp').reset_index(drop=True)\n    eth_merge = eth[['timestamp', 'close', 'volume']].rename(\n        columns={'close': 'eth_close', 'volume': 'eth_volume'}\n    )\n    crash_data = pd.merge_asof(\n        crash_data, eth_merge,\n        on='timestamp', direction='backward',\n        tolerance=pd.Timedelta('5min')\n    )\n    filled = crash_data['eth_close'].notna().sum()\n    print(f\"[OK] Merged ETH: {filled:,}/{len(crash_data):,} rows filled\")\nexcept Exception as e:\n    crash_data['eth_close'] = np.nan\n    crash_data['eth_volume'] = np.nan\n    print(f\"[WARN] ETH merge failed: {e}\")\n\n# -- Clean and save --\ncrash_data = crash_data.dropna(subset=['close']).reset_index(drop=True)\ncrash_data.to_csv('/content/data/crash_dataset_raw.csv', index=False)\ncrash_data.to_csv(f'{DRIVE_SAVE}/crash_dataset_raw.csv', index=False)\n\nprint(f\"\\n[OK] Crash dataset: {len(crash_data):,} rows, {len(crash_data.columns)} columns\")\nprint(f\"has_intraday_macro = {has_intraday_macro}\")\nprint(f\"\\nColumn overview:\")\nfor col in sorted(crash_data.columns):\n    non_null = crash_data[col].notna().sum()\n    pct = 100 * non_null / len(crash_data)\n    print(f\"  {col:30s} {non_null:>8,} non-null ({pct:.0f}%)\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Engineer Crash-Specific Features (51 Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 6: Build crash-specific features (v2 -- 51 features)\n# ============================================================\n# 51 features across 5 groups, all scale-invariant\n# v2 adds: 11 intraday macro features + 2-bar (10min) primary label\n\ndf = crash_data.copy()\n\n# ============================================\n# GROUP 1: BTC PRICE & VOLUME (15 features)\n# ============================================\n\n# Returns at multiple horizons (relative, not absolute)\ndf['return_1bar'] = df['close'].pct_change(1)          # 5 min\ndf['return_6bar'] = df['close'].pct_change(6)          # 30 min\ndf['return_12bar'] = df['close'].pct_change(12)        # 1 hour\ndf['return_48bar'] = df['close'].pct_change(48)        # 4 hours\ndf['return_288bar'] = df['close'].pct_change(288)      # 24 hours\n\n# Volatility (rolling std of returns -- already scale-invariant)\ndf['vol_12bar'] = df['return_1bar'].rolling(12).std()\ndf['vol_48bar'] = df['return_1bar'].rolling(48).std()\ndf['vol_ratio'] = df['vol_12bar'] / (df['vol_48bar'] + 1e-10)\n\n# Volume (relative to own history -- scale-invariant)\ndf['vol_sma_20'] = df['volume'].rolling(20).mean()\ndf['volume_surge'] = df['volume'] / (df['vol_sma_20'] + 1e-10)\ndf['volume_trend'] = df['volume'].rolling(12).mean() / (df['volume'].rolling(48).mean() + 1e-10)\n\n# Consecutive red candles (count)\ndf['candle_dir'] = (df['close'] > df['open']).astype(int)\ngroups = (df['candle_dir'] != df['candle_dir'].shift()).cumsum()\ndf['consecutive_red'] = df.groupby(groups)['candle_dir'].cumcount()\ndf.loc[df['candle_dir'] == 1, 'consecutive_red'] = 0\n\n# Drawdown from rolling 24h high (relative)\ndf['rolling_high_24h'] = df['high'].rolling(288).max()\ndf['drawdown_24h'] = df['close'] / df['rolling_high_24h'] - 1\n\n# RSI (normalized to [-1, 1])\ndelta = df['close'].diff()\ngain = delta.where(delta > 0, 0).rolling(14).mean()\nloss = (-delta.where(delta < 0, 0)).rolling(14).mean()\nrs = gain / (loss + 1e-10)\ndf['rsi_14_norm'] = (100 - (100 / (1 + rs)) - 50) / 50\n\n# Bollinger Band position (already normalized)\nbb_mid = df['close'].rolling(20).mean()\nbb_std = df['close'].rolling(20).std()\ndf['bb_pct_b'] = (df['close'] - bb_mid) / (2 * bb_std + 1e-10)\n\n# VWAP distance (relative)\ndf['session'] = np.arange(len(df)) // 288\nvwap = df.groupby('session').apply(\n    lambda g: (g['quote_volume'].cumsum() / (g['volume'].cumsum() + 1e-10))\n).reset_index(level=0, drop=True)\ndf['vwap_distance'] = df['close'] / (vwap + 1e-10) - 1\n\n# ============================================\n# GROUP 2: MACRO CORRELATION (10 features)\n# THE KEY SIGNAL -- BTC follows SPX in crashes\n# ============================================\n\n# S&P 500\nif 'spx' in df.columns and df['spx'].notna().sum() > 100:\n    spx_daily = df.groupby('date')['spx'].last()\n    spx_returns = spx_daily.pct_change()\n    spx_ret_map = spx_returns.to_dict()\n    df['spx_return_1d'] = df['date'].map(spx_ret_map).fillna(0)\n    spx_sma = df['spx'].rolling(288 * 5, min_periods=288).mean()\n    df['spx_vs_sma'] = df['spx'] / (spx_sma + 1e-10) - 1\nelse:\n    df['spx_return_1d'] = 0.0\n    df['spx_vs_sma'] = 0.0\n\n# VIX\nif 'vix' in df.columns and df['vix'].notna().sum() > 100:\n    df['vix_norm'] = (df['vix'] - 20) / 20\n    vix_daily = df.groupby('date')['vix'].last()\n    vix_change = vix_daily.pct_change()\n    vix_chg_map = vix_change.to_dict()\n    df['vix_change'] = df['date'].map(vix_chg_map).fillna(0)\n    df['vix_extreme'] = (df['vix'] > 30).astype(float)\nelse:\n    df['vix_norm'] = 0.0\n    df['vix_change'] = 0.0\n    df['vix_extreme'] = 0.0\n\n# Dollar Index\nif 'dxy' in df.columns and df['dxy'].notna().sum() > 100:\n    dxy_daily = df.groupby('date')['dxy'].last()\n    dxy_returns = dxy_daily.pct_change()\n    dxy_ret_map = dxy_returns.to_dict()\n    df['dxy_return_1d'] = df['date'].map(dxy_ret_map).fillna(0)\n    dxy_sma = df['dxy'].rolling(288 * 20, min_periods=288).mean()\n    df['dxy_trend'] = df['dxy'] / (dxy_sma + 1e-10) - 1\nelse:\n    df['dxy_return_1d'] = 0.0\n    df['dxy_trend'] = 0.0\n\n# Treasury Yields\nif 'us10y' in df.columns and df['us10y'].notna().sum() > 100:\n    df['yield_level'] = (df['us10y'] - 3.0) / 2.0\n    yield_daily = df.groupby('date')['us10y'].last()\n    yield_diff = yield_daily.diff()\n    yield_diff_map = yield_diff.to_dict()\n    df['yield_change'] = df['date'].map(yield_diff_map).fillna(0)\nelse:\n    df['yield_level'] = 0.0\n    df['yield_change'] = 0.0\n\n# Fear & Greed\nif 'fng' in df.columns and df['fng'].notna().sum() > 100:\n    df['fng_norm'] = (df['fng'] - 50) / 50\nelse:\n    df['fng_norm'] = 0.0\n\n# ============================================\n# GROUP 2B: INTRADAY MACRO (11 features, NEW in v2)\n# Only populated for last ~60 days (crash 3)\n# Older crash periods get 0-filled\n# ============================================\n\n# SPX intraday features\nif 'spx_intraday' in df.columns and df['spx_intraday'].notna().sum() > 100:\n    df['spx_return_5m'] = df['spx_intraday'].pct_change(1).fillna(0)\n    df['spx_return_30m'] = df['spx_intraday'].pct_change(6).fillna(0)\n    df['spx_return_1h'] = df['spx_intraday'].pct_change(12).fillna(0)\nelse:\n    df['spx_return_5m'] = 0.0\n    df['spx_return_30m'] = 0.0\n    df['spx_return_1h'] = 0.0\n\n# VIX intraday features\nif 'vix_intraday' in df.columns and df['vix_intraday'].notna().sum() > 100:\n    df['vix_return_5m'] = df['vix_intraday'].pct_change(1).fillna(0)\n    df['vix_return_30m'] = df['vix_intraday'].pct_change(6).fillna(0)\n    df['vix_level_5m'] = ((df['vix_intraday'] - 20) / 20).fillna(0)\nelse:\n    df['vix_return_5m'] = 0.0\n    df['vix_return_30m'] = 0.0\n    df['vix_level_5m'] = 0.0\n\n# NDX (Nasdaq) intraday features\nif 'ndx_intraday' in df.columns and df['ndx_intraday'].notna().sum() > 100:\n    df['ndx_return_5m'] = df['ndx_intraday'].pct_change(1).fillna(0)\n    df['ndx_return_30m'] = df['ndx_intraday'].pct_change(6).fillna(0)\nelse:\n    df['ndx_return_5m'] = 0.0\n    df['ndx_return_30m'] = 0.0\n\n# Cross-market signals from intraday data\n# SPX-VIX divergence: SPX up + VIX up = warning signal\ndf['spx_vix_diverge'] = (df['spx_return_5m'] * df['vix_return_5m']).fillna(0)\n\n# Macro momentum composites\ndf['macro_momentum_5m'] = ((df['spx_return_5m'] + df['ndx_return_5m']) / 2).fillna(0)\ndf['macro_momentum_30m'] = ((df['spx_return_30m'] + df['ndx_return_30m']) / 2).fillna(0)\n\n# ============================================\n# GROUP 3: DERIVATIVES (9 features)\n# ============================================\n\n# Funding rate\nif 'funding_rate' in df.columns and df['funding_rate'].notna().sum() > 100:\n    df['funding_rate'] = df['funding_rate'].ffill().fillna(0)\n    fr_mean = df['funding_rate'].rolling(288 * 7, min_periods=288).mean()\n    fr_std = df['funding_rate'].rolling(288 * 7, min_periods=288).std()\n    df['funding_z'] = (df['funding_rate'] - fr_mean) / (fr_std + 1e-10)\n    df['funding_extreme_long'] = (df['funding_rate'] > 0.01).astype(float)\n    df['funding_extreme_short'] = (df['funding_rate'] < -0.01).astype(float)\nelse:\n    df['funding_z'] = 0.0\n    df['funding_extreme_long'] = 0.0\n    df['funding_extreme_short'] = 0.0\n\n# Open Interest\nif 'oi_value' in df.columns and df['oi_value'].notna().sum() > 100:\n    df['oi_value'] = df['oi_value'].ffill().bfill()\n    df['oi_change_1h'] = df['oi_value'].pct_change(12)\n    df['oi_change_4h'] = df['oi_value'].pct_change(48)\n    df['oi_spike'] = (df['oi_change_1h'].abs() > 0.05).astype(float)\nelse:\n    df['oi_change_1h'] = 0.0\n    df['oi_change_4h'] = 0.0\n    df['oi_spike'] = 0.0\n\n# Long/Short Ratio\nif 'long_short_ratio' in df.columns and df['long_short_ratio'].notna().sum() > 100:\n    df['ls_ratio_norm'] = df['long_short_ratio'].ffill().fillna(1.0) - 1.0\n    df['ls_extreme_long'] = (df['long_short_ratio'] > 2.0).astype(float)\nelse:\n    df['ls_ratio_norm'] = 0.0\n    df['ls_extreme_long'] = 0.0\n\n# Taker Buy/Sell\nif 'taker_ratio' in df.columns and df['taker_ratio'].notna().sum() > 100:\n    df['taker_imbalance'] = df['taker_ratio'].ffill().fillna(1.0) - 1.0\nelse:\n    df['taker_imbalance'] = 0.0\n\n# ============================================\n# GROUP 4: CROSS-ASSET (6 features)\n# ============================================\n\nif 'eth_close' in df.columns and df['eth_close'].notna().sum() > 100:\n    df['eth_return_1bar'] = df['eth_close'].pct_change(1)\n    df['eth_return_6bar'] = df['eth_close'].pct_change(6)\n    df['eth_btc_ratio'] = df['eth_close'] / (df['close'] + 1e-10)\n    df['eth_btc_ratio_change'] = df['eth_btc_ratio'].pct_change(12)\n    df['btc_lead_1'] = df['return_1bar'].shift(1)\n    df['btc_lead_2'] = df['return_1bar'].shift(2)\n    df['btc_lead_3'] = df['return_1bar'].shift(3)\nelse:\n    df['eth_return_1bar'] = 0.0\n    df['eth_return_6bar'] = 0.0\n    df['eth_btc_ratio_change'] = 0.0\n    df['btc_lead_1'] = 0.0\n    df['btc_lead_2'] = 0.0\n    df['btc_lead_3'] = 0.0\n\n# ============================================\n# LABELS: Multiple horizons (v2 change)\n# Primary: 2-bar (10 min) -- matches Polymarket 15-min windows\n# Also compute 1-bar (5 min) and 6-bar (30 min) for comparison\n# ============================================\n\n# Primary label: 2-bar forward return (10 min ahead)\ndf['forward_return_2'] = df['close'].shift(-2) / df['close'] - 1\ndf['label_binary'] = (df['forward_return_2'] > 0).astype(int)  # 1=up, 0=down\ndf['label_soft'] = np.tanh(df['forward_return_2'] * 100)\n\n# Comparison labels (trained separately in Cell 7)\ndf['forward_return_1'] = df['close'].shift(-1) / df['close'] - 1\ndf['forward_return_6'] = df['close'].shift(-6) / df['close'] - 1\ndf['label_binary_1bar'] = (df['forward_return_1'] > 0).astype(int)\ndf['label_binary_6bar'] = (df['forward_return_6'] > 0).astype(int)\n\n# ============================================\n# FEATURE LIST (51 features)\n# ============================================\n\nFEATURE_COLS = [\n    # BTC price/volume (15)\n    'return_1bar', 'return_6bar', 'return_12bar', 'return_48bar', 'return_288bar',\n    'vol_12bar', 'vol_48bar', 'vol_ratio',\n    'volume_surge', 'volume_trend',\n    'consecutive_red', 'drawdown_24h',\n    'rsi_14_norm', 'bb_pct_b', 'vwap_distance',\n    # Daily macro (10)\n    'spx_return_1d', 'spx_vs_sma',\n    'vix_norm', 'vix_change', 'vix_extreme',\n    'dxy_return_1d', 'dxy_trend',\n    'yield_level', 'yield_change',\n    'fng_norm',\n    # Intraday macro (11) -- NEW in v2\n    'spx_return_5m', 'spx_return_30m', 'spx_return_1h',\n    'vix_return_5m', 'vix_return_30m', 'vix_level_5m',\n    'ndx_return_5m', 'ndx_return_30m',\n    'spx_vix_diverge',\n    'macro_momentum_5m', 'macro_momentum_30m',\n    # Derivatives (9)\n    'funding_z', 'funding_extreme_long', 'funding_extreme_short',\n    'oi_change_1h', 'oi_change_4h', 'oi_spike',\n    'ls_ratio_norm', 'ls_extreme_long',\n    'taker_imbalance',\n    # Cross-asset (6)\n    'eth_return_1bar', 'eth_return_6bar', 'eth_btc_ratio_change',\n    'btc_lead_1', 'btc_lead_2', 'btc_lead_3',\n]\n\nassert len(FEATURE_COLS) == 51, f\"Expected 51 features, got {len(FEATURE_COLS)}\"\n\n# Replace inf with nan, then drop incomplete rows\ndf = df.replace([np.inf, -np.inf], np.nan)\ndf = df.dropna(subset=FEATURE_COLS + ['label_binary']).reset_index(drop=True)\n\n# Count how many rows have real intraday macro data\nintraday_cols = ['spx_return_5m', 'spx_return_30m', 'spx_return_1h',\n                 'vix_return_5m', 'vix_return_30m', 'vix_level_5m',\n                 'ndx_return_5m', 'ndx_return_30m']\nhas_real_intraday = (df[intraday_cols].abs().sum(axis=1) > 0).sum()\n\nprint(f\"\\n[OK] Feature engineering complete (v2)\")\nprint(f\"Clean dataset: {len(df):,} rows with {len(FEATURE_COLS)} features\")\nprint(f\"Label balance (2-bar): {df['label_binary'].mean():.3f} (1=up)\")\nprint(f\"Rows with real intraday macro: {has_real_intraday:,} ({100*has_real_intraday/len(df):.1f}%)\")\nprint(f\"\\nFeature groups:\")\nprint(f\"  BTC price/volume:   15 features\")\nprint(f\"  Daily macro:        10 features\")\nprint(f\"  Intraday macro:     11 features (NEW)\")\nprint(f\"  Derivatives:         9 features\")\nprint(f\"  Cross-asset:         6 features\")\nprint(f\"  Total:              51 features\")\n\n# Save feature dataset to Drive\nsave_cols = FEATURE_COLS + ['timestamp', 'close', 'label_binary', 'label_soft',\n                             'forward_return_2', 'forward_return_1', 'forward_return_6',\n                             'label_binary_1bar', 'label_binary_6bar']\ndf[save_cols].to_csv(f'{DRIVE_SAVE}/crash_features_v2.csv', index=False)\nprint(f\"[OK] Feature dataset saved to Drive\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Walk-Forward Split & Train LightGBM (Multi-Horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 7: Train crash-regime LightGBM (v2 -- multi-horizon)\n# ============================================================\n# Trains 3 models at different horizons, plus an intraday-only model.\n# Primary: 2-bar (10 min), Comparison: 1-bar (5 min), 6-bar (30 min)\n\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport pickle\nimport json\nimport shutil\n\n# -- Split by crash period (walk-forward) --\ncrash1 = df[df['timestamp'] < '2019-01-01']\ncrash2 = df[(df['timestamp'] >= '2021-11-01') & (df['timestamp'] < '2023-01-01')]\ncrash3 = df[df['timestamp'] >= '2025-10-01']\n\nprint(f\"Crash 1 (2018):    {len(crash1):>8,} rows\")\nprint(f\"Crash 2 (2021-22): {len(crash2):>8,} rows\")\nprint(f\"Crash 3 (2025-26): {len(crash3):>8,} rows\")\n\n# Train on crash 1 + 2, validate/test on crash 3\ntrain_data = pd.concat([crash1, crash2])\n\nif len(crash3) > 1000:\n    split_idx = len(crash3) // 2\n    val_data = crash3.iloc[:split_idx]\n    test_data = crash3.iloc[split_idx:]\nelse:\n    split_70 = int(len(crash2) * 0.7)\n    split_85 = int(len(crash2) * 0.85)\n    train_data = pd.concat([crash1, crash2.iloc[:split_70]])\n    val_data = crash2.iloc[split_70:split_85]\n    test_data = crash2.iloc[split_85:]\n\n# Sample weights: crash 1 gets 0.5x weight (older data)\ntrain_weights = np.ones(len(train_data))\ncrash1_mask = train_data['timestamp'] < '2019-01-01'\ntrain_weights[crash1_mask.values] = 0.5\n\nX_train = train_data[FEATURE_COLS].values\nX_val = val_data[FEATURE_COLS].values\nX_test = test_data[FEATURE_COLS].values\n\nprint(f\"\\nTrain: {len(X_train):,} | Val: {len(X_val):,} | Test: {len(X_test):,}\")\n\n# LightGBM params (shared across all horizons)\nparams = {\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'min_child_samples': 100,\n    'lambda_l1': 0.1,\n    'lambda_l2': 0.1,\n    'verbose': -1,\n}\n\n# ============================================\n# TRAIN MODELS AT 3 HORIZONS\n# ============================================\n\nhorizon_configs = [\n    ('2bar_10min', 'label_binary',      'PRIMARY (10 min)'),\n    ('1bar_5min',  'label_binary_1bar',  'comparison (5 min)'),\n    ('6bar_30min', 'label_binary_6bar',  'comparison (30 min)'),\n]\n\nresults = {}\n\nfor horizon_name, label_col, description in horizon_configs:\n    print(f\"\\n{'='*60}\")\n    print(f\"Training: {horizon_name} -- {description}\")\n    print(f\"{'='*60}\")\n\n    y_train = train_data[label_col].values\n    y_val = val_data[label_col].values\n    y_test = test_data[label_col].values\n\n    print(f\"Train up%: {y_train.mean():.3f} | Val up%: {y_val.mean():.3f} | Test up%: {y_test.mean():.3f}\")\n\n    train_set = lgb.Dataset(X_train, label=y_train, weight=train_weights,\n                             feature_name=FEATURE_COLS)\n    val_set = lgb.Dataset(X_val, label=y_val, feature_name=FEATURE_COLS)\n\n    model = lgb.train(\n        params,\n        train_set,\n        num_boost_round=500,\n        valid_sets=[val_set],\n        valid_names=['val'],\n        callbacks=[lgb.early_stopping(30), lgb.log_evaluation(50)],\n    )\n\n    val_probs = model.predict(X_val)\n    test_probs = model.predict(X_test)\n\n    val_acc = accuracy_score(y_val, (val_probs > 0.5).astype(int))\n    test_acc = accuracy_score(y_test, (test_probs > 0.5).astype(int))\n    val_auc = roc_auc_score(y_val, val_probs)\n    test_auc = roc_auc_score(y_test, test_probs)\n    pred_std = np.std(test_probs)\n\n    results[horizon_name] = {\n        'model': model,\n        'val_acc': val_acc,\n        'test_acc': test_acc,\n        'val_auc': val_auc,\n        'test_auc': test_auc,\n        'pred_std': pred_std,\n        'test_probs': test_probs,\n        'y_test': y_test,\n        'best_round': model.best_iteration,\n    }\n\n    print(f\"\\n  Val acc:  {val_acc:.4f} ({val_acc*100:.1f}%)\")\n    print(f\"  Test acc: {test_acc:.4f} ({test_acc*100:.1f}%)\")\n    print(f\"  Val AUC:  {val_auc:.4f}\")\n    print(f\"  Test AUC: {test_auc:.4f}\")\n    print(f\"  Pred std: {pred_std:.4f}\")\n    print(f\"  Best round: {model.best_iteration}\")\n\n# ============================================\n# HORIZON COMPARISON TABLE\n# ============================================\nprint(f\"\\n\\n{'='*60}\")\nprint(f\"HORIZON COMPARISON\")\nprint(f\"{'='*60}\")\nprint(f\"{'Horizon':<16} {'Val Acc':>8} {'Test Acc':>9} {'Val AUC':>8} {'Test AUC':>9} {'Pred Std':>9}\")\nprint(f\"{'-'*60}\")\nfor name, r in results.items():\n    marker = \" <-- PRIMARY\" if '2bar' in name else \"\"\n    print(f\"{name:<16} {r['val_acc']:>8.4f} {r['test_acc']:>9.4f} {r['val_auc']:>8.4f} {r['test_auc']:>9.4f} {r['pred_std']:>9.4f}{marker}\")\n\n# ============================================\n# INTRADAY-ONLY MODEL (crash 3 only, all 51 features)\n# Uses only rows where intraday macro is populated\n# ============================================\nprint(f\"\\n\\n{'='*60}\")\nprint(f\"Training: INTRADAY-ONLY model (crash 3 subset)\")\nprint(f\"{'='*60}\")\n\nintraday_cols = ['spx_return_5m', 'spx_return_30m', 'spx_return_1h',\n                 'vix_return_5m', 'vix_return_30m', 'vix_level_5m',\n                 'ndx_return_5m', 'ndx_return_30m']\n\n# Filter to rows with real intraday data\ndf_intraday = crash3[crash3[intraday_cols].abs().sum(axis=1) > 0].copy()\n\nif len(df_intraday) >= 500:\n    split_70 = int(len(df_intraday) * 0.7)\n    split_85 = int(len(df_intraday) * 0.85)\n    id_train = df_intraday.iloc[:split_70]\n    id_val = df_intraday.iloc[split_70:split_85]\n    id_test = df_intraday.iloc[split_85:]\n\n    id_X_train = id_train[FEATURE_COLS].values\n    id_y_train = id_train['label_binary'].values\n    id_X_val = id_val[FEATURE_COLS].values\n    id_y_val = id_val['label_binary'].values\n    id_X_test = id_test[FEATURE_COLS].values\n    id_y_test = id_test['label_binary'].values\n\n    print(f\"Intraday rows: {len(df_intraday):,}\")\n    print(f\"Train: {len(id_X_train):,} | Val: {len(id_X_val):,} | Test: {len(id_X_test):,}\")\n\n    id_train_set = lgb.Dataset(id_X_train, label=id_y_train, feature_name=FEATURE_COLS)\n    id_val_set = lgb.Dataset(id_X_val, label=id_y_val, feature_name=FEATURE_COLS)\n\n    id_model = lgb.train(\n        params,\n        id_train_set,\n        num_boost_round=300,\n        valid_sets=[id_val_set],\n        valid_names=['val'],\n        callbacks=[lgb.early_stopping(20), lgb.log_evaluation(50)],\n    )\n\n    id_test_probs = id_model.predict(id_X_test)\n    id_test_acc = accuracy_score(id_y_test, (id_test_probs > 0.5).astype(int))\n    id_test_auc = roc_auc_score(id_y_test, id_test_probs)\n\n    print(f\"\\n  Intraday-only test acc: {id_test_acc:.4f} ({id_test_acc*100:.1f}%)\")\n    print(f\"  Intraday-only test AUC: {id_test_auc:.4f}\")\n\n    # Save intraday model too\n    id_model.save_model('/content/models/crash_lgbm_intraday.txt')\n    with open('/content/models/crash_lgbm_intraday.pkl', 'wb') as f:\n        pickle.dump(id_model, f)\n    print(f\"  Intraday model saved\")\nelse:\n    print(f\"Not enough intraday rows ({len(df_intraday)}) -- need >= 500. Skipping.\")\n    id_model = None\n\n# ============================================\n# PICK BEST MODEL & SAVE\n# ============================================\n\n# Best = highest test AUC among the 3 horizon models\nbest_name = max(results, key=lambda k: results[k]['test_auc'])\nbest = results[best_name]\nbest_model = best['model']\n\nprint(f\"\\n\\n{'='*60}\")\nprint(f\"BEST MODEL: {best_name}\")\nprint(f\"  Test acc: {best['test_acc']:.4f} | Test AUC: {best['test_auc']:.4f}\")\nprint(f\"{'='*60}\")\n\n# Feature importance for best model\nimportance = best_model.feature_importance(importance_type='gain')\nfeat_imp = sorted(zip(FEATURE_COLS, importance), key=lambda x: -x[1])\n\nmacro_feats = {'spx_return_1d','spx_vs_sma','vix_norm','vix_change','vix_extreme',\n               'dxy_return_1d','dxy_trend','yield_level','yield_change','fng_norm'}\nintraday_macro_feats = {'spx_return_5m','spx_return_30m','spx_return_1h',\n                         'vix_return_5m','vix_return_30m','vix_level_5m',\n                         'ndx_return_5m','ndx_return_30m',\n                         'spx_vix_diverge','macro_momentum_5m','macro_momentum_30m'}\nderiv_feats = {'funding_z','funding_extreme_long','funding_extreme_short',\n               'oi_change_1h','oi_change_4h','oi_spike','ls_ratio_norm',\n               'ls_extreme_long','taker_imbalance'}\ncross_feats = {'eth_return_1bar','eth_return_6bar','eth_btc_ratio_change',\n               'btc_lead_1','btc_lead_2','btc_lead_3'}\n\nprint(f\"\\nTop 20 features by gain:\")\nfor i, (feat, imp) in enumerate(feat_imp[:20], 1):\n    if feat in intraday_macro_feats:\n        group = \"INTRA\"\n    elif feat in macro_feats:\n        group = \"MACRO\"\n    elif feat in deriv_feats:\n        group = \"DERIV\"\n    elif feat in cross_feats:\n        group = \"CROSS\"\n    else:\n        group = \"PRICE\"\n    print(f\"  {i:3d}. [{group:5s}] {feat:30s} {imp:>12,.0f}\")\n\n# Group importance totals\ngroup_imp = {'PRICE': 0, 'MACRO': 0, 'INTRA': 0, 'DERIV': 0, 'CROSS': 0}\nfor feat, imp in feat_imp:\n    if feat in intraday_macro_feats:\n        group_imp['INTRA'] += imp\n    elif feat in macro_feats:\n        group_imp['MACRO'] += imp\n    elif feat in deriv_feats:\n        group_imp['DERIV'] += imp\n    elif feat in cross_feats:\n        group_imp['CROSS'] += imp\n    else:\n        group_imp['PRICE'] += imp\n\ntotal_imp = sum(group_imp.values())\nprint(f\"\\nFeature group importance:\")\nfor group, imp in sorted(group_imp.items(), key=lambda x: -x[1]):\n    print(f\"  {group:5s}: {imp:>12,.0f} ({100*imp/total_imp:.1f}%)\")\n\n# -- Save best model --\nbest_model.save_model('/content/models/crash_lightgbm_model.txt')\nwith open('/content/models/crash_lightgbm_model.pkl', 'wb') as f:\n    pickle.dump(best_model, f)\n\n# Also save all horizon models\nfor name, r in results.items():\n    with open(f'/content/models/crash_lgbm_{name}.pkl', 'wb') as f:\n        pickle.dump(r['model'], f)\n\nmeta = {\n    'model_type': 'lightgbm_crash_regime_v2',\n    'regime': 'CRASH',\n    'best_horizon': best_name,\n    'val_accuracy': float(best['val_acc']),\n    'test_accuracy': float(best['test_acc']),\n    'val_auc': float(best['val_auc']),\n    'test_auc': float(best['test_auc']),\n    'pred_std': float(best['pred_std']),\n    'best_round': int(best['best_round']),\n    'n_features': len(FEATURE_COLS),\n    'feature_names': FEATURE_COLS,\n    'feature_importance': {f: float(i) for f, i in feat_imp},\n    'horizon_comparison': {\n        name: {\n            'val_acc': float(r['val_acc']),\n            'test_acc': float(r['test_acc']),\n            'val_auc': float(r['val_auc']),\n            'test_auc': float(r['test_auc']),\n        }\n        for name, r in results.items()\n    },\n    'train_rows': int(len(X_train)),\n    'val_rows': int(len(X_val)),\n    'test_rows': int(len(X_test)),\n    'crash_periods': CRASH_PERIODS,\n    'crash1_weight': 0.5,\n    'params': params,\n    'trained_at': datetime.utcnow().isoformat(),\n}\nwith open('/content/models/crash_lightgbm_meta.json', 'w') as f:\n    json.dump(meta, f, indent=2)\n\n# -- SAVE TO DRIVE IMMEDIATELY --\nfor fname in os.listdir('/content/models/'):\n    shutil.copy(f'/content/models/{fname}', f'{DRIVE_SAVE}/{fname}')\n    sz = os.path.getsize(f'/content/models/{fname}') / 1024\n    print(f\"[OK] Saved to Drive: {fname} ({sz:.1f} KB)\")\n\nprint(f\"\\n[DONE] Crash-regime LightGBM v2 trained and saved!\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Confidence Calibration Analysis (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CELL 8: Confidence calibration analysis (v2)\n# ============================================================\n# Answers: \"When the model says 85% confident, is it right 85% of the time?\"\n# Uses the BEST model from the horizon comparison.\n\n# Use best model's test predictions\nprobs = results[best_name]['test_probs'].copy()\nactuals = results[best_name]['y_test'].copy()\n\n# Model outputs probability of UP (0.0 to 1.0)\n# Confidence = how far from 50/50\nconfidence = np.abs(probs - 0.5) * 2  # Scale to 0-1\npredicted_up = (probs > 0.5).astype(int)\ncorrect = (predicted_up == actuals).astype(int)\n\n# Bin by model probability (UP direction)\nprint(f\"Best model: {best_name}\")\nprint(f\"\\n{'Probability':>12} {'Direction':>10} {'Count':>8} {'Accuracy':>10} {'Bet?':>8}\")\nprint(\"-\" * 55)\n\nprob_bins = [\n    (0.50, 0.55, 'UP'),\n    (0.55, 0.60, 'UP'),\n    (0.60, 0.65, 'UP'),\n    (0.65, 0.70, 'UP'),\n    (0.70, 0.75, 'UP'),\n    (0.75, 0.80, 'UP'),\n    (0.80, 0.85, 'UP'),\n    (0.85, 0.90, 'UP'),\n    (0.90, 0.95, 'UP'),\n    (0.95, 1.01, 'UP'),\n]\n\nfor lo, hi, direction in prob_bins:\n    mask = (probs >= lo) & (probs < hi)\n    if mask.sum() > 0:\n        acc = correct[mask].mean()\n        bet = \"[BET]\" if lo >= 0.85 else \"--\"\n        print(f\"  {lo:.0%}-{hi:.0%}      {'UP':>10} {mask.sum():>8,} {acc:>10.1%} {bet:>8}\")\n\nprint()\n# Also check DOWN predictions (prob < 0.5)\nfor lo, hi in [(0.05, 0.10), (0.10, 0.15), (0.15, 0.20), (0.20, 0.25),\n               (0.25, 0.30), (0.30, 0.35), (0.35, 0.40), (0.40, 0.45), (0.45, 0.50)]:\n    mask = (probs >= lo) & (probs < hi)\n    if mask.sum() > 0:\n        down_correct = (actuals[mask] == 0).mean()\n        bet = \"[BET]\" if hi <= 0.15 else \"--\"\n        print(f\"  {lo:.0%}-{hi:.0%}      {'DOWN':>10} {mask.sum():>8,} {down_correct:>10.1%} {bet:>8}\")\n\n# Summary for Polymarket thresholds\nprint(f\"\\n{'='*55}\")\nprint(f\"POLYMARKET DECISION THRESHOLDS ({best_name})\")\nprint(f\"{'='*55}\")\n\n# 85%+ confident UP (prob >= 0.85)\nmask_85_up = probs >= 0.85\nif mask_85_up.sum() > 0:\n    acc = correct[mask_85_up].mean()\n    print(f\"  85%+ UP:   {mask_85_up.sum():>6,} predictions, {acc:.1%} accuracy\")\nelse:\n    print(f\"  85%+ UP:   0 predictions\")\n\n# 85%+ confident DOWN (prob <= 0.15)\nmask_85_down = probs <= 0.15\nif mask_85_down.sum() > 0:\n    acc = (actuals[mask_85_down] == 0).mean()\n    print(f\"  85%+ DOWN: {mask_85_down.sum():>6,} predictions, {acc:.1%} accuracy\")\nelse:\n    print(f\"  85%+ DOWN: 0 predictions\")\n\n# Combined high confidence\nmask_85_any = (probs >= 0.85) | (probs <= 0.15)\nif mask_85_any.sum() > 0:\n    hc_correct = np.where(probs[mask_85_any] >= 0.5,\n                          actuals[mask_85_any] == 1,\n                          actuals[mask_85_any] == 0)\n    print(f\"  85%+ ANY:  {mask_85_any.sum():>6,} predictions, {hc_correct.mean():.1%} accuracy\")\n    verdict = \"BET\" if hc_correct.mean() > 0.60 else \"WAIT -- models not calibrated yet\"\n    print(f\"\\n  -> Polymarket should {verdict}\")\nelse:\n    print(f\"  85%+ ANY:  0 predictions -- model never reaches 85% confidence\")\n    max_conf = confidence.max()\n    print(f\"  Max confidence seen: {50 + max_conf*50:.1f}%\")\n    for thresh in [0.60, 0.65, 0.70, 0.75, 0.80]:\n        mask = confidence >= (thresh - 0.5) * 2\n        if mask.sum() > 10:\n            hc = np.where(probs[mask] >= 0.5, actuals[mask] == 1, actuals[mask] == 0)\n            print(f\"  {thresh:.0%}+ conf: {mask.sum():>6,} predictions, {hc.mean():.1%} accuracy\")\n\n# Save calibration data to Drive\ncal_df = pd.DataFrame({\n    'probability': probs,\n    'confidence': confidence,\n    'predicted_up': predicted_up,\n    'actual_up': actuals,\n    'correct': correct,\n})\ncal_df.to_csv(f'{DRIVE_SAVE}/calibration_analysis_v2.csv', index=False)\nprint(f\"\\n[OK] Calibration data saved to Drive\")\n\n# ============================================\n# FINAL SUMMARY\n# ============================================\nprint(f\"\\n\\n{'='*60}\")\nprint(f\"TRAINING COMPLETE -- CRASH REGIME LIGHTGBM v2\")\nprint(f\"{'='*60}\")\nprint(f\"  Best horizon:   {best_name}\")\nprint(f\"  Test accuracy:  {best['test_acc']:.1%}\")\nprint(f\"  Test AUC:       {best['test_auc']:.4f}\")\nprint(f\"  Features:       {len(FEATURE_COLS)} (51)\")\nprint(f\"  Training data:  {len(X_train):,} crash-period rows\")\nprint(f\"  Model saved to: {DRIVE_SAVE}\")\nprint(f\"{'='*60}\")\n\nprint(f\"\\nHorizon comparison:\")\nfor name, r in results.items():\n    marker = \" <-- BEST\" if name == best_name else \"\"\n    print(f\"  {name:16s}: acc={r['test_acc']:.4f}  auc={r['test_auc']:.4f}{marker}\")\n\nif best['test_acc'] > 0.53:\n    print(f\"\\n[PASS] ACCURACY ABOVE 53% -- Ready to deploy!\")\n    print(f\"   Download crash_lightgbm_model.pkl from Google Drive\")\n    print(f\"   and place in models/trained/ on the VPS.\")\nelif best['test_acc'] > 0.51:\n    print(f\"\\n[MARGINAL] Consider adding more features or tuning params\")\nelse:\n    print(f\"\\n[FAIL] NO IMPROVEMENT -- Check feature quality and data coverage\")\n"
  }
 ]
}