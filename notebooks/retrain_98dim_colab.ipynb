{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_YP_oRUEJFc"
      },
      "source": [
        "# Renaissance Bot — Retrain All Models (98-dim Cross-Asset Features)\n",
        "\n",
        "This notebook retrains all 7 ML models with the new 98-dimension cross-asset feature pipeline:\n",
        "- **46 scale-invariant single-pair features**: returns, ratios, z-scores (no raw prices)\n",
        "- **15 cross-asset features**: lead signals, correlations, spreads, market-wide\n",
        "- **7 derivatives features**: funding rate, OI, long/short ratio, taker ratio, Fear & Greed\n",
        "- **Padded to 98**\n",
        "\n",
        "## Training order\n",
        "1. Upload historical CSVs via Google Drive (6 pairs x 5+ years of 5-min bars)\n",
        "2. (Optional) Upload derivatives CSVs for 7 additional features\n",
        "3. Phase 1: Train 5 base models (QT, BiLSTM, DilatedCNN, CNN, GRU)\n",
        "4. Phase 2: Train Meta-Ensemble (stacking layer over 5 base models)\n",
        "5. Phase 3: Train VAE anomaly detector\n",
        "6. Download trained `.pth` weight files\n",
        "\n",
        "## Key training fixes (v7)\n",
        "- **Soft labels**: 6-bar (30-min) forward return x 100, clipped to [-1, 1]\n",
        "- **v6 loss**: BCE(pred x 20) + 10 x separation_margin(0.10) + 5 x magnitude_floor\n",
        "- **v7 QT optimizer**: weight_decay=0 (not 1e-5), differential LR (attention 0.1x), collapse recovery\n",
        "- **Other models**: weight_decay=1e-4, LR=3e-4\n",
        "- **LR warmup**: 3-epoch linear warmup + cosine decay\n",
        "\n",
        "**Why v7?** v6 fixed the loss but QT still collapsed at epoch 5 on full data.\n",
        "Root cause: weight_decay=1e-5 per step × 10,600 batches/epoch = **10.6% total\n",
        "weight shrinkage per epoch** — 13.5x more than on local 50K-sample test.\n",
        "This systematically shrinks attention Q/K/V weights → softmax goes uniform →\n",
        "mean pooling produces constant output → pred=0. Fix: wd=0 for QT + 0.1x LR\n",
        "on attention params + auto-recovery from collapse.\n",
        "\n",
        "**Runtime**: Select **GPU -> T4** (Runtime -> Change runtime type -> T4 GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOy-KMVIEJFe"
      },
      "source": [
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAVdXGL0EJFf"
      },
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch numpy pandas scikit-learn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZVRVKCfEJFf",
        "outputId": "d25d08ea-3ac0-44ae-f20f-ebd5ca98e3e5"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import time\n",
        "import math\n",
        "import shutil\n",
        "import logging\n",
        "import zipfile\n",
        "from datetime import datetime, timezone\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
        "logger = logging.getLogger('retrain')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory/ 1e9:.1f} GB')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "Memory: 85.1 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1M8P2XlEJFg"
      },
      "source": [
        "## 1. Upload Training Data via Google Drive\n",
        "\n",
        "1. Upload the 6 CSV files to a folder in your Google Drive (e.g., `My Drive/training_data/`)\n",
        "2. Run cell 1a below — it mounts Drive and copies files to local storage\n",
        "3. Run cell 1b — it **automatically fetches** derivatives data from Binance Futures\n",
        "   (works from Colab — blocked in US locally). Saves to Drive for reuse.\n",
        "\n",
        "**Files needed** (from `data/training/`):\n",
        "- `BTC-USD_5m_historical.csv` (~49 MB)\n",
        "- `ETH-USD_5m_historical.csv` (~45 MB)\n",
        "- `SOL-USD_5m_historical.csv` (~27 MB)\n",
        "- `DOGE-USD_5m_historical.csv` (~38 MB)\n",
        "- `AVAX-USD_5m_historical.csv` (~25 MB)\n",
        "- `LINK-USD_5m_historical.csv` (~34 MB)\n",
        "\n",
        "**Derivatives** (auto-fetched by cell 1b, or reused from Drive on subsequent runs):\n",
        "- `BTC-USD_derivatives.csv`, `ETH-USD_derivatives.csv`, etc.\n",
        "- `fear_greed_history.csv`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGO4lq_9EJFg",
        "outputId": "950394ec-7990-4412-b6e7-3bbf6bc9a712"
      },
      "source": [
        "# Mount Google Drive and copy CSV files to local storage\n",
        "# Change DRIVE_FOLDER if you put them somewhere else.\n",
        "\n",
        "DRIVE_FOLDER = 'training_data'  # folder name inside My Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.makedirs('data/training', exist_ok=True)\n",
        "os.makedirs('models/trained', exist_ok=True)\n",
        "\n",
        "# Search for CSV files in the specified folder\n",
        "drive_path = f'/content/drive/My Drive/{DRIVE_FOLDER}'\n",
        "if not os.path.exists(drive_path):\n",
        "    print(f'Folder \"{DRIVE_FOLDER}\" not found in My Drive.')\n",
        "    print('Searching for *_5m_historical.csv files in entire Drive...')\n",
        "    found = glob.glob('/content/drive/My Drive/**/*_5m_historical.csv', recursive=True)\n",
        "    if found:\n",
        "        drive_path = os.path.dirname(found[0])\n",
        "        print(f'Found files in: {drive_path}')\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            'No *_5m_historical.csv files found in Google Drive.\\n'\n",
        "            'Please upload them to a folder in Drive first.'\n",
        "        )\n",
        "\n",
        "# Copy CSV files from Drive to local storage (much faster I/O)\n",
        "csv_files = glob.glob(os.path.join(drive_path, '*_5m_historical.csv'))\n",
        "print(f'\\nFound {len(csv_files)} CSV files in {drive_path}:')\n",
        "for src in sorted(csv_files):\n",
        "    fname = os.path.basename(src)\n",
        "    dst = os.path.join('data/training', fname)\n",
        "    size_mb = os.path.getsize(src) / 1e6\n",
        "    print(f'  Copying {fname} ({size_mb:.1f} MB)...', end=' ')\n",
        "    shutil.copy2(src, dst)\n",
        "    print('done')\n",
        "\n",
        "print(f'\\nAll files copied to data/training/')\n",
        "\n",
        "# Copy derivatives data if available (optional — enables 7 extra features)\n",
        "deriv_drive_path = os.path.join(drive_path, 'derivatives')\n",
        "if os.path.exists(deriv_drive_path):\n",
        "    os.makedirs('data/training/derivatives', exist_ok=True)\n",
        "    deriv_files = [f for f in os.listdir(deriv_drive_path) if f.endswith('.csv')]\n",
        "    for fname in sorted(deriv_files):\n",
        "        src = os.path.join(deriv_drive_path, fname)\n",
        "        shutil.copy2(src, os.path.join('data/training/derivatives', fname))\n",
        "    print(f'\\nCopied {len(deriv_files)} derivatives files to data/training/derivatives/')\n",
        "else:\n",
        "    print(f'\\nNo derivatives/ subfolder found (optional — 7 features will be zero-padded)')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "Found 6 CSV files in /content/drive/My Drive/training_data:\n",
            "  Copying AVAX-USD_5m_historical.csv (26.4 MB)... done\n",
            "  Copying BTC-USD_5m_historical.csv (51.6 MB)... done\n",
            "  Copying DOGE-USD_5m_historical.csv (39.5 MB)... done\n",
            "  Copying ETH-USD_5m_historical.csv (47.6 MB)... done\n",
            "  Copying LINK-USD_5m_historical.csv (35.8 MB)... done\n",
            "  Copying SOL-USD_5m_historical.csv (28.1 MB)... done\n",
            "\n",
            "All files copied to data/training/\n",
            "\n",
            "Copied 7 derivatives files to data/training/derivatives/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZquGlOnWEJFg"
      },
      "source": [
        "### 1b. Fetch Derivatives Data (runs on Colab)\n",
        "\n",
        "Binance Futures API is **geo-restricted in the US** but works from Colab (Google Cloud).\n",
        "This cell fetches funding rate, open interest, long/short ratio, taker volume,\n",
        "and Fear & Greed history — then saves CSVs to `data/training/derivatives/`.\n",
        "\n",
        "**Skip this cell** if you already uploaded derivatives CSVs via Google Drive.\n",
        "Takes ~5-10 minutes for 6 pairs × 2 years of 5-min data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "VfI7eWIREJFh",
        "outputId": "c592097c-8361-46aa-86ac-19f878179ff5"
      },
      "source": [
        "import requests\n",
        "# ── Fetch derivatives data from Binance Futures (works on Colab, blocked in US) ──\n",
        "# Skip this cell if you already uploaded derivatives CSVs to Google Drive.\n",
        "\n",
        "import time as _time\n",
        "\n",
        "DERIV_PAIRS = ['BTC-USD', 'ETH-USD', 'SOL-USD', 'DOGE-USD', 'AVAX-USD', 'LINK-USD']\n",
        "DERIV_DAYS = 180  # ~6 months -- Binance OI/LS endpoints limited to ~30d windows\n",
        "DERIV_PERIOD = '5m'\n",
        "DERIV_DIR = 'data/training/derivatives'\n",
        "os.makedirs(DERIV_DIR, exist_ok=True)\n",
        "\n",
        "BINANCE_FAPI = 'https://fapi.binance.com'\n",
        "BINANCE_FUTURES = 'https://fapi.binance.com/futures/data'\n",
        "FNG_API = 'https://api.alternative.me/fng/'\n",
        "PAIR_MAP = {'BTC-USD':'BTCUSDT','ETH-USD':'ETHUSDT','SOL-USD':'SOLUSDT',\n",
        "            'DOGE-USD':'DOGEUSDT','AVAX-USD':'AVAXUSDT','LINK-USD':'LINKUSDT'}\n",
        "REQ_DELAY = 0.2\n",
        "\n",
        "WINDOW_MS = 30 * 86400 * 1000  # 30-day windows for OI/LS endpoints\n",
        "\n",
        "def _paginate(url, symbol, period, start_ms, end_ms, val_key, label):\n",
        "    # Reverse-windowed pagination -- iterate newest-to-oldest so we get\n",
        "    # recent data first.  Binance OI/LS endpoints only retain ~30 days of\n",
        "    # 5m data; older windows return HTTP 400.  After 2 consecutive 400s\n",
        "    # we stop and return what we have.\n",
        "    rows = []\n",
        "    consecutive_400 = 0\n",
        "    # Build window list (newest first)\n",
        "    windows = []\n",
        "    ws = start_ms\n",
        "    while ws < end_ms:\n",
        "        we = min(ws + WINDOW_MS, end_ms)\n",
        "        windows.append((ws, we))\n",
        "        ws = we\n",
        "    windows.reverse()  # newest first\n",
        "    for window_start, window_end in windows:\n",
        "        if consecutive_400 >= 2:\n",
        "            print(f'    {label}: skipping older windows (data unavailable)')\n",
        "            break\n",
        "        cur = window_start\n",
        "        got_data = False\n",
        "        while cur < window_end:\n",
        "            resp = requests.get(url, params={\n",
        "                'symbol': symbol, 'period': period,\n",
        "                'startTime': int(cur), 'endTime': int(window_end), 'limit': 500,\n",
        "            }, timeout=15)\n",
        "            if resp.status_code == 400:\n",
        "                consecutive_400 += 1\n",
        "                # Try a fallback: fetch latest data without time params\n",
        "                if not rows and consecutive_400 == 1:\n",
        "                    fb = requests.get(url, params={\n",
        "                        'symbol': symbol, 'period': period, 'limit': 500,\n",
        "                    }, timeout=15)\n",
        "                    if fb.status_code == 200:\n",
        "                        data = fb.json()\n",
        "                        for e in data:\n",
        "                            rows.append({'timestamp': int(e['timestamp'])//1000, 'value': float(e[val_key])})\n",
        "                        print(f'    {label}: got {len(data)} entries via latest-only fallback')\n",
        "                        got_data = True\n",
        "                break\n",
        "            if resp.status_code != 200:\n",
        "                print(f'    {label} HTTP {resp.status_code}: {resp.text[:100]}')\n",
        "                break\n",
        "            data = resp.json()\n",
        "            if not data:\n",
        "                break\n",
        "            consecutive_400 = 0  # reset on success\n",
        "            got_data = True\n",
        "            for e in data:\n",
        "                rows.append({'timestamp': int(e['timestamp'])//1000, 'value': float(e[val_key])})\n",
        "            cur = int(data[-1]['timestamp']) + 1\n",
        "            _time.sleep(REQ_DELAY)\n",
        "    return pd.DataFrame(rows).drop_duplicates('timestamp').sort_values('timestamp').reset_index(drop=True) if rows else pd.DataFrame()\n",
        "\n",
        "def _fetch_funding(symbol, start_ms, end_ms):\n",
        "    rows = []\n",
        "    cur = start_ms\n",
        "    while cur < end_ms:\n",
        "        resp = requests.get(f'{BINANCE_FAPI}/fapi/v1/fundingRate',\n",
        "                            params={'symbol':symbol,'startTime':cur,'endTime':end_ms,'limit':1000}, timeout=15)\n",
        "        if resp.status_code != 200:\n",
        "            print(f'    Funding HTTP {resp.status_code}')\n",
        "            break\n",
        "        data = resp.json()\n",
        "        if not data:\n",
        "            break\n",
        "        for e in data:\n",
        "            rows.append({'timestamp': int(e['fundingTime'])//1000, 'value': float(e['fundingRate'])})\n",
        "        cur = int(data[-1]['fundingTime']) + 1\n",
        "        _time.sleep(REQ_DELAY)\n",
        "    return pd.DataFrame(rows).drop_duplicates('timestamp').sort_values('timestamp').reset_index(drop=True) if rows else pd.DataFrame()\n",
        "\n",
        "def _fetch_taker(symbol, period, start_ms, end_ms):\n",
        "    rows = []\n",
        "    cur = start_ms\n",
        "    while cur < end_ms:\n",
        "        resp = requests.get(f'{BINANCE_FUTURES}/takeBuySellVol',\n",
        "                            params={'symbol':symbol,'period':period,'startTime':cur,'endTime':end_ms,'limit':500}, timeout=15)\n",
        "        if resp.status_code != 200:\n",
        "            print(f'    Taker HTTP {resp.status_code}')\n",
        "            break\n",
        "        data = resp.json()\n",
        "        if not data:\n",
        "            break\n",
        "        for e in data:\n",
        "            rows.append({'timestamp': int(e['timestamp'])//1000,\n",
        "                         'taker_buy_vol': float(e['buyVol']), 'taker_sell_vol': float(e['sellVol'])})\n",
        "        cur = int(data[-1]['timestamp']) + 1\n",
        "        _time.sleep(REQ_DELAY)\n",
        "    return pd.DataFrame(rows).drop_duplicates('timestamp').sort_values('timestamp').reset_index(drop=True) if rows else pd.DataFrame()\n",
        "\n",
        "end_ms = int(datetime.now(timezone.utc).timestamp() * 1000)\n",
        "start_ms = end_ms - (DERIV_DAYS * 86400 * 1000)\n",
        "\n",
        "# ── Quick connectivity test ──\n",
        "print('Testing Binance Futures API connectivity...')\n",
        "test_resp = requests.get(f'{BINANCE_FAPI}/fapi/v1/fundingRate',\n",
        "                         params={'symbol':'BTCUSDT','limit':1}, timeout=10)\n",
        "if test_resp.status_code != 200:\n",
        "    print(f'ERROR: Binance Futures returned {test_resp.status_code}.')\n",
        "    print('This API is geo-restricted. If running locally in the US, use Colab or a VPN.')\n",
        "    print('Skipping derivatives fetch — features will be zero-padded.')\n",
        "else:\n",
        "    print(f'OK — connected to Binance Futures\\n')\n",
        "\n",
        "    for pair in DERIV_PAIRS:\n",
        "        symbol = PAIR_MAP[pair]\n",
        "        print(f'\\n{\"=\"*50}')\n",
        "        print(f'{pair} ({symbol}) — {DERIV_DAYS}d, period={DERIV_PERIOD}')\n",
        "        print(f'{\"=\"*50}')\n",
        "\n",
        "        funding_df = _fetch_funding(symbol, start_ms, end_ms)\n",
        "        print(f'  Funding rate: {len(funding_df)} entries')\n",
        "\n",
        "        oi_df = _paginate(f'{BINANCE_FUTURES}/openInterestHist',\n",
        "                          symbol, DERIV_PERIOD, start_ms, end_ms, 'sumOpenInterest', 'OI')\n",
        "        print(f'  Open interest: {len(oi_df)} entries')\n",
        "\n",
        "        ls_df = _paginate(f'{BINANCE_FUTURES}/globalLongShortAccountRatio',\n",
        "                          symbol, DERIV_PERIOD, start_ms, end_ms, 'longShortRatio', 'LS')\n",
        "        print(f'  Long/Short ratio: {len(ls_df)} entries')\n",
        "\n",
        "        taker_df = _fetch_taker(symbol, DERIV_PERIOD, start_ms, end_ms)\n",
        "        print(f'  Taker volume: {len(taker_df)} entries')\n",
        "\n",
        "        # Merge\n",
        "        dfs = []\n",
        "        if not funding_df.empty:\n",
        "            dfs.append(funding_df.rename(columns={'value':'funding_rate'}))\n",
        "        if not oi_df.empty:\n",
        "            dfs.append(oi_df.rename(columns={'value':'open_interest'}))\n",
        "        if not ls_df.empty:\n",
        "            dfs.append(ls_df.rename(columns={'value':'long_short_ratio'}))\n",
        "        if not taker_df.empty:\n",
        "            dfs.append(taker_df)\n",
        "\n",
        "        if dfs:\n",
        "            result = dfs[0]\n",
        "            for df in dfs[1:]:\n",
        "                result = pd.merge(result, df, on='timestamp', how='outer')\n",
        "            result = result.sort_values('timestamp').reset_index(drop=True)\n",
        "            if 'funding_rate' in result.columns:\n",
        "                result['funding_rate'] = result['funding_rate'].ffill()\n",
        "\n",
        "            out_path = os.path.join(DERIV_DIR, f'{pair}_derivatives.csv')\n",
        "            result.to_csv(out_path, index=False)\n",
        "            print(f'  -> Saved {out_path} ({len(result):,} rows, {os.path.getsize(out_path)/1e6:.1f} MB)')\n",
        "        else:\n",
        "            print(f'  -> No data for {pair}')\n",
        "\n",
        "    # ── Fear & Greed ──\n",
        "    print(f'\\n{\"=\"*50}')\n",
        "    print('Fear & Greed Index (all history)')\n",
        "    print(f'{\"=\"*50}')\n",
        "    try:\n",
        "        fng_resp = requests.get(FNG_API, params={'format':'json','limit':0}, timeout=30)\n",
        "        fng_data = fng_resp.json().get('data', [])\n",
        "        if fng_data:\n",
        "            fng_rows = [{'timestamp': int(e['timestamp']), 'fear_greed': int(e['value'])} for e in fng_data]\n",
        "            fng_df = pd.DataFrame(fng_rows).sort_values('timestamp').drop_duplicates('timestamp').reset_index(drop=True)\n",
        "            fng_path = os.path.join(DERIV_DIR, 'fear_greed_history.csv')\n",
        "            fng_df.to_csv(fng_path, index=False)\n",
        "            print(f'  Saved {fng_path} ({len(fng_df):,} daily values)')\n",
        "        else:\n",
        "            print('  No Fear & Greed data returned')\n",
        "    except Exception as e:\n",
        "        print(f'  Fear & Greed fetch failed: {e}')\n",
        "\n",
        "    # ── Summary ──\n",
        "    print(f'\\n{\"=\"*50}')\n",
        "    print('DERIVATIVES FETCH SUMMARY')\n",
        "    print(f'{\"=\"*50}')\n",
        "    total_size = 0\n",
        "    for f in sorted(os.listdir(DERIV_DIR)):\n",
        "        if f.endswith('.csv'):\n",
        "            p = os.path.join(DERIV_DIR, f)\n",
        "            sz = os.path.getsize(p)\n",
        "            total_size += sz\n",
        "            rows = len(pd.read_csv(p))\n",
        "            print(f'  {f}: {rows:,} rows ({sz/1e6:.1f} MB)')\n",
        "    print(f'  Total: {total_size/1e6:.1f} MB')\n",
        "\n",
        "    # Also save to Google Drive for reuse\n",
        "    drive_deriv = f'/content/drive/My Drive/{DRIVE_FOLDER}/derivatives'\n",
        "    os.makedirs(drive_deriv, exist_ok=True)\n",
        "    for f in os.listdir(DERIV_DIR):\n",
        "        if f.endswith('.csv'):\n",
        "            shutil.copy2(os.path.join(DERIV_DIR, f), os.path.join(drive_deriv, f))\n",
        "    print(f'\\nAlso saved to Google Drive: {drive_deriv}/')\n",
        "    print('Next time, cell 1a will auto-copy these from Drive (no re-fetch needed).')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Binance Futures API connectivity...\n",
            "OK — connected to Binance Futures\n",
            "\n",
            "\n",
            "==================================================\n",
            "BTC-USD (BTCUSDT) — 180d, period=5m\n",
            "==================================================\n",
            "  Funding rate: 540 entries\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2492/3582462302.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'  Funding rate: {len(funding_df)} entries'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         oi_df = _paginate(f'{BINANCE_FUTURES}/openInterestHist',\n\u001b[0m\u001b[1;32m    137\u001b[0m                           symbol, DERIV_PERIOD, start_ms, end_ms, 'sumOpenInterest', 'OI')\n\u001b[1;32m    138\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'  Open interest: {len(oi_df)} entries'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2492/3582462302.py\u001b[0m in \u001b[0;36m_paginate\u001b[0;34m(url, symbol, period, start_ms, end_ms, val_key, label)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'value'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0m_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREQ_DELAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvJ6JAsEEJFh",
        "outputId": "d7a852f3-fddf-4504-9a6d-99a09d32f7f7"
      },
      "source": [
        "# Load and validate data\n",
        "ALL_PAIRS = ['BTC-USD', 'ETH-USD', 'SOL-USD', 'DOGE-USD', 'AVAX-USD', 'LINK-USD']\n",
        "\n",
        "pair_dfs = {}\n",
        "for pair in ALL_PAIRS:\n",
        "    csv_path = f'data/training/{pair}_5m_historical.csv'\n",
        "    if os.path.exists(csv_path):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        pair_dfs[pair] = df\n",
        "        first_ts = datetime.fromtimestamp(\n",
        "            df['timestamp'].iloc[0]/1000 if df['timestamp'].iloc[0] > 1e12 else df['timestamp'].iloc[0],\n",
        "            tz=timezone.utc)\n",
        "        last_ts = datetime.fromtimestamp(\n",
        "            df['timestamp'].iloc[-1]/1000 if df['timestamp'].iloc[-1] > 1e12 else df['timestamp'].iloc[-1],\n",
        "            tz=timezone.utc)\n",
        "        print(f'{pair}: {len(df):>10,} bars  ({first_ts.strftime(\"%Y-%m-%d\")} -> {last_ts.strftime(\"%Y-%m-%d\")})')\n",
        "    else:\n",
        "        print(f'{pair}: NOT FOUND')\n",
        "\n",
        "total = sum(len(df) for df in pair_dfs.values())\n",
        "print(f'\\nTotal: {total:,} bars across {len(pair_dfs)} pairs')\n",
        "assert len(pair_dfs) >= 2, 'Need at least 2 pairs for cross-asset features'\n",
        "\n",
        "# Load derivatives data (optional — 7 features: funding_rate_z, oi_change_pct,\n",
        "# long_short_ratio, taker_buy_sell_ratio, fear_greed_norm, fear_greed_roc, has_derivatives_data)\n",
        "derivatives_dfs = {}\n",
        "fear_greed_df = None\n",
        "deriv_dir = 'data/training/derivatives'\n",
        "\n",
        "if os.path.exists(deriv_dir):\n",
        "    for fname in sorted(os.listdir(deriv_dir)):\n",
        "        if fname.endswith('_derivatives.csv'):\n",
        "            pair = fname.replace('_derivatives.csv', '')\n",
        "            ddf = pd.read_csv(os.path.join(deriv_dir, fname))\n",
        "            if len(ddf) > 0:\n",
        "                derivatives_dfs[pair] = ddf\n",
        "                print(f'  Derivatives {pair}: {len(ddf):,} rows')\n",
        "\n",
        "    fng_path = os.path.join(deriv_dir, 'fear_greed_history.csv')\n",
        "    if os.path.exists(fng_path):\n",
        "        fear_greed_df = pd.read_csv(fng_path)\n",
        "        if len(fear_greed_df) > 0:\n",
        "            print(f'  Fear & Greed: {len(fear_greed_df):,} daily values')\n",
        "        else:\n",
        "            fear_greed_df = None\n",
        "\n",
        "    if derivatives_dfs or fear_greed_df is not None:\n",
        "        print(f'\\nDerivatives: {len(derivatives_dfs)} pairs + {\"yes\" if fear_greed_df is not None else \"no\"} Fear & Greed')\n",
        "    else:\n",
        "        print('\\nDerivatives CSVs empty — 7 derivatives features will be zero-padded')\n",
        "else:\n",
        "    print(f'\\nNo derivatives data directory — 7 derivatives features will be zero-padded')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BTC-USD:    888,637 bars  (2017-09-01 -> 2026-02-17)\n",
            "ETH-USD:    888,639 bars  (2017-09-01 -> 2026-02-17)\n",
            "SOL-USD:    574,424 bars  (2020-09-01 -> 2026-02-17)\n",
            "DOGE-USD:    696,063 bars  (2019-07-05 -> 2026-02-17)\n",
            "AVAX-USD:    568,299 bars  (2020-09-22 -> 2026-02-17)\n",
            "LINK-USD:    744,842 bars  (2019-01-16 -> 2026-02-17)\n",
            "\n",
            "Total: 4,360,904 bars across 6 pairs\n",
            "  Derivatives AVAX-USD: 2,190 rows\n",
            "  Derivatives BTC-USD: 2,190 rows\n",
            "  Derivatives DOGE-USD: 2,190 rows\n",
            "  Derivatives ETH-USD: 2,190 rows\n",
            "  Derivatives LINK-USD: 2,190 rows\n",
            "  Derivatives SOL-USD: 2,190 rows\n",
            "  Fear & Greed: 2,936 daily values\n",
            "\n",
            "Derivatives: 6 pairs + yes Fear & Greed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAbpkDdIEJFi"
      },
      "source": [
        "## 2. Constants & Feature Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tuDN55_EJFi",
        "outputId": "e75b7148-fa49-49d1-9b9a-dd741584a54c"
      },
      "source": [
        "# Constants\n",
        "INPUT_DIM = 98\n",
        "N_CROSS_FEATURES = 15\n",
        "N_DERIVATIVES_FEATURES = 7\n",
        "SEQ_LEN = 30\n",
        "N_BASE_MODELS = 6\n",
        "BASE_MODEL_NAMES = ['quantum_transformer', 'bidirectional_lstm', 'dilated_cnn', 'cnn', 'gru', 'lightgbm']\n",
        "\n",
        "# Label configuration — predict 30-min forward return, not next-bar direction\n",
        "LABEL_HORIZON = 6   # 6 bars × 5 min = 30 min lookahead\n",
        "LABEL_SCALE = 100   # Scaling: 0.5% return → label 0.5, clipped to [-1, 1]\n",
        "\n",
        "LEAD_SIGNALS = {\n",
        "    'BTC-USD':  {'primary': 'ETH-USD',  'secondary': 'SOL-USD'},\n",
        "    'ETH-USD':  {'primary': 'BTC-USD',  'secondary': 'LINK-USD'},\n",
        "    'SOL-USD':  {'primary': 'BTC-USD',  'secondary': 'ETH-USD'},\n",
        "    'LINK-USD': {'primary': 'ETH-USD',  'secondary': 'BTC-USD'},\n",
        "    'AVAX-USD': {'primary': 'ETH-USD',  'secondary': 'BTC-USD'},\n",
        "    'DOGE-USD': {'primary': 'BTC-USD',  'secondary': 'ETH-USD'},\n",
        "}\n",
        "\n",
        "print(f'INPUT_DIM = {INPUT_DIM}  (46 single-pair + 15 cross-asset + 7 derivatives = 68 real, padded to 98)')\n",
        "print(f'SEQ_LEN = {SEQ_LEN}')\n",
        "print(f'LABEL_HORIZON = {LABEL_HORIZON} bars ({LABEL_HORIZON * 5} min)')\n",
        "print(f'LABEL_SCALE = {LABEL_SCALE}')\n",
        "print(f'Lead signals: {list(LEAD_SIGNALS.keys())}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT_DIM = 98  (46 single-pair + 15 cross-asset + 7 derivatives = 68 real, padded to 98)\n",
            "SEQ_LEN = 30\n",
            "LABEL_HORIZON = 6 bars (30 min)\n",
            "LABEL_SCALE = 100\n",
            "Lead signals: ['BTC-USD', 'ETH-USD', 'SOL-USD', 'LINK-USD', 'AVAX-USD', 'DOGE-USD']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq9812pXEJFi",
        "outputId": "56ec1982-4f7d-4fae-b6f1-c3fdd2d1cce6"
      },
      "source": [
        "# ================================================================\n",
        "# CROSS-ASSET FEATURE BUILDER (15 features)\n",
        "# ================================================================\n",
        "\n",
        "def _build_cross_features(close, volume, cross_data, pair_name):\n",
        "    \"\"\"Compute 15 cross-asset features (all returns/correlations/z-scores).\"\"\"\n",
        "    feats = {}\n",
        "    log_ret = np.log(close / close.shift(1))\n",
        "\n",
        "    lead_cfg = LEAD_SIGNALS.get(pair_name, {})\n",
        "    for role, leader_pair, horizons in [\n",
        "        ('primary', lead_cfg.get('primary'), [1, 3, 6]),\n",
        "        ('secondary', lead_cfg.get('secondary'), [1, 3]),\n",
        "    ]:\n",
        "        if leader_pair and leader_pair in cross_data:\n",
        "            lc = cross_data[leader_pair]['close'].astype(float)\n",
        "            lr = np.log(lc / lc.shift(1))\n",
        "            for h in horizons:\n",
        "                feats[f'lead_{role}_ret_{h}'] = lr.rolling(h).sum()\n",
        "        else:\n",
        "            for h in ([1, 3, 6] if role == 'primary' else [1, 3]):\n",
        "                feats[f'lead_{role}_ret_{h}'] = pd.Series(0.0, index=close.index)\n",
        "\n",
        "    for ref_name, ref_label in [('BTC-USD', 'btc'), ('ETH-USD', 'eth')]:\n",
        "        if ref_name in cross_data and ref_name != pair_name:\n",
        "            rc = cross_data[ref_name]['close'].astype(float)\n",
        "            rr = np.log(rc / rc.shift(1))\n",
        "            corr_50 = log_ret.rolling(50).corr(rr)\n",
        "            feats[f'corr_{ref_label}_50'] = corr_50\n",
        "            cm = corr_50.rolling(200).mean()\n",
        "            cs = corr_50.rolling(200).std()\n",
        "            feats[f'corr_z_{ref_label}'] = (corr_50 - cm) / (cs + 1e-10)\n",
        "        else:\n",
        "            feats[f'corr_{ref_label}_50'] = pd.Series(0.0, index=close.index)\n",
        "            feats[f'corr_z_{ref_label}'] = pd.Series(0.0, index=close.index)\n",
        "\n",
        "    for ref_name, ref_label in [('BTC-USD', 'btc'), ('ETH-USD', 'eth')]:\n",
        "        if ref_name in cross_data and ref_name != pair_name:\n",
        "            rc = cross_data[ref_name]['close'].astype(float)\n",
        "            ls = np.log(close / (rc + 1e-10))\n",
        "            sm = ls.rolling(100).mean()\n",
        "            ss = ls.rolling(100).std()\n",
        "            feats[f'spread_{ref_label}_z'] = (ls - sm) / (ss + 1e-10)\n",
        "        else:\n",
        "            feats[f'spread_{ref_label}_z'] = pd.Series(0.0, index=close.index)\n",
        "\n",
        "    all_rets, all_vz = [], []\n",
        "    for p, cdf in cross_data.items():\n",
        "        c = cdf['close'].astype(float)\n",
        "        all_rets.append(np.log(c / c.shift(1)))\n",
        "        if 'volume' in cdf.columns:\n",
        "            v = cdf['volume'].astype(float)\n",
        "            vm = v.rolling(20).mean()\n",
        "            all_vz.append(v / (vm + 1e-10) - 1.0)\n",
        "    all_rets.append(log_ret)\n",
        "    if volume is not None:\n",
        "        vm = volume.rolling(20).mean()\n",
        "        all_vz.append(volume / (vm + 1e-10) - 1.0)\n",
        "\n",
        "    if all_rets:\n",
        "        rdf = pd.concat(all_rets, axis=1)\n",
        "        feats['mkt_avg_ret'] = rdf.mean(axis=1)\n",
        "        feats['mkt_dispersion'] = rdf.std(axis=1)\n",
        "        feats['mkt_breadth'] = (rdf > 0).mean(axis=1)\n",
        "    else:\n",
        "        feats['mkt_avg_ret'] = pd.Series(0.0, index=close.index)\n",
        "        feats['mkt_dispersion'] = pd.Series(0.0, index=close.index)\n",
        "        feats['mkt_breadth'] = pd.Series(0.5, index=close.index)\n",
        "    feats['mkt_avg_vol_z'] = pd.concat(all_vz, axis=1).mean(axis=1) if all_vz else pd.Series(0.0, index=close.index)\n",
        "\n",
        "    return feats\n",
        "\n",
        "print('Cross-asset feature builder defined (15 features)')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-asset feature builder defined (15 features)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LkBB1B4EJFi",
        "outputId": "2283206f-8ba3-435f-ddf8-315b72662caf"
      },
      "source": [
        "# ================================================================\n",
        "# SCALE-INVARIANT FEATURE PIPELINE (46 single-pair + 15 cross-asset + 7 derivatives)\n",
        "#\n",
        "# CRITICAL: No feature depends on absolute price level or raw volume.\n",
        "# Every feature is a return, ratio, z-score, or bounded indicator.\n",
        "# This ensures identical feature distributions whether BTC is $3K or $130K.\n",
        "# ================================================================\n",
        "\n",
        "def _compute_single_pair_features(df):\n",
        "    \"\"\"46 scale-invariant features from OHLCV data.\"\"\"\n",
        "    close = df['close'].astype(float)\n",
        "    _open = df['open'].astype(float) if 'open' in df.columns else close\n",
        "    high = df['high'].astype(float) if 'high' in df.columns else close\n",
        "    low = df['low'].astype(float) if 'low' in df.columns else close\n",
        "    vol = df['volume'].astype(float) if 'volume' in df.columns else None\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    # Group 1: Candle shape (5)\n",
        "    features['open_gap'] = np.log(_open / (close.shift(1) + 1e-10))\n",
        "    features['upper_wick'] = (high - np.maximum(_open, close)) / (close + 1e-10)\n",
        "    features['lower_wick'] = (np.minimum(_open, close) - low) / (close + 1e-10)\n",
        "    features['body'] = (close - _open) / (close + 1e-10)\n",
        "    if vol is not None:\n",
        "        vm = vol.rolling(100, min_periods=10).mean()\n",
        "        vs = vol.rolling(100, min_periods=10).std()\n",
        "        features['volume_z'] = (vol - vm) / (vs + 1e-10)\n",
        "    else:\n",
        "        features['volume_z'] = close * 0.0\n",
        "\n",
        "    # Group 2: Returns (7)\n",
        "    for w in [1, 2, 3, 5, 10, 20]:\n",
        "        features[f'ret_{w}'] = close.pct_change(w)\n",
        "    features['log_ret'] = np.log(close / close.shift(1))\n",
        "\n",
        "    # Group 3: SMA distance + slope (8)\n",
        "    for w in [5, 10, 20, 50]:\n",
        "        sma = close.rolling(w).mean()\n",
        "        features[f'sma_dist_{w}'] = (close - sma) / (sma + 1e-10)\n",
        "        features[f'sma_slope_{w}'] = sma.pct_change(3)\n",
        "\n",
        "    # Group 4: EMA distance + slope (6)\n",
        "    for w in [5, 10, 20]:\n",
        "        ema = close.ewm(span=w, adjust=False).mean()\n",
        "        features[f'ema_dist_{w}'] = (close - ema) / (ema + 1e-10)\n",
        "        features[f'ema_slope_{w}'] = ema.pct_change(3)\n",
        "\n",
        "    # Group 5: Realized volatility (3)\n",
        "    pct_ret = close.pct_change()\n",
        "    for w in [5, 10, 20]:\n",
        "        features[f'vol_{w}'] = pct_ret.rolling(w).std()\n",
        "\n",
        "    # Group 6: RSI [-1, 1] (1)\n",
        "    delta = close.diff()\n",
        "    gain = delta.clip(lower=0).rolling(14).mean()\n",
        "    loss_s = (-delta.clip(upper=0)).rolling(14).mean()\n",
        "    rs = gain / (loss_s + 1e-10)\n",
        "    features['rsi_norm'] = (100 - (100 / (1 + rs)) - 50) / 50\n",
        "\n",
        "    # Group 7: MACD / price (3)\n",
        "    ema12 = close.ewm(span=12, adjust=False).mean()\n",
        "    ema26 = close.ewm(span=26, adjust=False).mean()\n",
        "    macd = ema12 - ema26\n",
        "    macd_signal = macd.ewm(span=9, adjust=False).mean()\n",
        "    features['macd_pct'] = macd / (close + 1e-10)\n",
        "    features['macd_signal_pct'] = macd_signal / (close + 1e-10)\n",
        "    features['macd_hist_pct'] = (macd - macd_signal) / (close + 1e-10)\n",
        "\n",
        "    # Group 8: Bollinger Bands (4)\n",
        "    sma20 = close.rolling(20).mean()\n",
        "    std20 = close.rolling(20).std()\n",
        "    bb_upper = sma20 + 2 * std20\n",
        "    bb_lower = sma20 - 2 * std20\n",
        "    bb_range = bb_upper - bb_lower + 1e-10\n",
        "    features['bb_pct'] = (close - bb_lower) / bb_range\n",
        "    features['bb_width'] = bb_range / (sma20 + 1e-10)\n",
        "    features['bb_upper_dist'] = (bb_upper - close) / (close + 1e-10)\n",
        "    features['bb_lower_dist'] = (close - bb_lower) / (close + 1e-10)\n",
        "\n",
        "    # Group 9: ATR / price (1)\n",
        "    tr = pd.concat([\n",
        "        high - low,\n",
        "        (high - close.shift(1)).abs(),\n",
        "        (low - close.shift(1)).abs(),\n",
        "    ], axis=1).max(axis=1)\n",
        "    features['atr_pct'] = tr.rolling(14).mean() / (close + 1e-10)\n",
        "\n",
        "    # Group 10: Volume ratios (3)\n",
        "    if vol is not None:\n",
        "        features['vol_ratio'] = vol / (vol.rolling(10, min_periods=1).mean() + 1e-10)\n",
        "        features['vol_change'] = vol.pct_change()\n",
        "        features['vol_trend'] = vol.rolling(5, min_periods=1).mean() / (vol.rolling(20, min_periods=1).mean() + 1e-10)\n",
        "    else:\n",
        "        features['vol_ratio'] = close * 0.0\n",
        "        features['vol_change'] = close * 0.0\n",
        "        features['vol_trend'] = close * 0.0 + 1.0\n",
        "\n",
        "    # Group 11: Momentum (3)\n",
        "    features['momentum_5'] = close / close.shift(5) - 1\n",
        "    features['momentum_10'] = close / close.shift(10) - 1\n",
        "    features['momentum_20'] = close / close.shift(20) - 1\n",
        "\n",
        "    # Group 12: Range (2)\n",
        "    features['hl_range'] = (high - low) / (close + 1e-10)\n",
        "    features['hl_range_norm'] = features['hl_range'] / (features['hl_range'].rolling(10, min_periods=1).mean() + 1e-10)\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def _build_derivatives_features(n_rows, derivatives_data=None):\n",
        "    \"\"\"Compute 7 derivatives + sentiment features.\n",
        "\n",
        "    Args:\n",
        "        n_rows: Number of rows in the price DataFrame\n",
        "        derivatives_data: Dict with optional keys:\n",
        "            - 'funding_rate': pd.Series of raw funding rates\n",
        "            - 'open_interest': pd.Series of open interest values\n",
        "            - 'long_short_ratio': pd.Series of long/short account ratios\n",
        "            - 'taker_buy_vol': pd.Series of taker buy volume\n",
        "            - 'taker_sell_vol': pd.Series of taker sell volume\n",
        "            - 'fear_greed': pd.Series of Fear & Greed index (0-100)\n",
        "\n",
        "    Returns:\n",
        "        Dict of 7 feature_name -> pd.Series\n",
        "    \"\"\"\n",
        "    idx = pd.RangeIndex(n_rows)\n",
        "    feats = {}\n",
        "    has_deriv = False\n",
        "\n",
        "    if derivatives_data is not None:\n",
        "        # Funding rate z-score (50-bar window)\n",
        "        fr = derivatives_data.get('funding_rate')\n",
        "        if fr is not None and len(fr) > 0:\n",
        "            fr = fr.astype(float)\n",
        "            fr_mean = fr.rolling(50, min_periods=5).mean()\n",
        "            fr_std = fr.rolling(50, min_periods=5).std()\n",
        "            feats['funding_rate_z'] = (fr - fr_mean) / (fr_std + 1e-10)\n",
        "            has_deriv = True\n",
        "        else:\n",
        "            feats['funding_rate_z'] = pd.Series(0.0, index=idx)\n",
        "\n",
        "        # Open interest 5-bar % change\n",
        "        oi = derivatives_data.get('open_interest')\n",
        "        if oi is not None and len(oi) > 0:\n",
        "            oi = oi.astype(float)\n",
        "            feats['oi_change_pct'] = oi.pct_change(5)\n",
        "            has_deriv = True\n",
        "        else:\n",
        "            feats['oi_change_pct'] = pd.Series(0.0, index=idx)\n",
        "\n",
        "        # Long/short ratio (raw, already scale-invariant)\n",
        "        ls = derivatives_data.get('long_short_ratio')\n",
        "        if ls is not None and len(ls) > 0:\n",
        "            feats['long_short_ratio'] = ls.astype(float)\n",
        "            has_deriv = True\n",
        "        else:\n",
        "            feats['long_short_ratio'] = pd.Series(0.0, index=idx)\n",
        "\n",
        "        # Taker buy/sell ratio\n",
        "        buy_vol = derivatives_data.get('taker_buy_vol')\n",
        "        sell_vol = derivatives_data.get('taker_sell_vol')\n",
        "        if buy_vol is not None and sell_vol is not None and len(buy_vol) > 0:\n",
        "            bv = buy_vol.astype(float)\n",
        "            sv = sell_vol.astype(float)\n",
        "            feats['taker_buy_sell_ratio'] = bv / (sv + 1e-10)\n",
        "            has_deriv = True\n",
        "        else:\n",
        "            feats['taker_buy_sell_ratio'] = pd.Series(0.0, index=idx)\n",
        "\n",
        "        # Fear & Greed (normalized + 3-day ROC)\n",
        "        fg = derivatives_data.get('fear_greed')\n",
        "        if fg is not None and len(fg) > 0:\n",
        "            fg = fg.astype(float)\n",
        "            feats['fear_greed_norm'] = fg / 100.0\n",
        "            feats['fear_greed_roc'] = fg.diff(864) / 100.0  # 3 days in 5-min bars\n",
        "        else:\n",
        "            feats['fear_greed_norm'] = pd.Series(0.0, index=idx)\n",
        "            feats['fear_greed_roc'] = pd.Series(0.0, index=idx)\n",
        "    else:\n",
        "        feats['funding_rate_z'] = pd.Series(0.0, index=idx)\n",
        "        feats['oi_change_pct'] = pd.Series(0.0, index=idx)\n",
        "        feats['long_short_ratio'] = pd.Series(0.0, index=idx)\n",
        "        feats['taker_buy_sell_ratio'] = pd.Series(0.0, index=idx)\n",
        "        feats['fear_greed_norm'] = pd.Series(0.0, index=idx)\n",
        "        feats['fear_greed_roc'] = pd.Series(0.0, index=idx)\n",
        "\n",
        "    # Binary flag: model knows when derivatives data is present\n",
        "    feats['has_derivatives_data'] = pd.Series(1.0 if has_deriv else 0.0, index=idx)\n",
        "\n",
        "    return feats\n",
        "\n",
        "\n",
        "def _align_derivatives(price_df, pair, derivatives_dfs, fear_greed_df):\n",
        "    \"\"\"Align derivatives data to price DataFrame timestamps via merge_asof.\n",
        "\n",
        "    Returns dict suitable for _build_derivatives_features(), or None if no data.\n",
        "    \"\"\"\n",
        "    if not derivatives_dfs and fear_greed_df is None:\n",
        "        return None\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    # Align per-pair derivatives (funding_rate, OI, LS, taker volumes)\n",
        "    if derivatives_dfs and pair in derivatives_dfs:\n",
        "        deriv_df = derivatives_dfs[pair].copy()\n",
        "        if 'timestamp' in deriv_df.columns and 'timestamp' in price_df.columns:\n",
        "            price_ts = price_df[['timestamp']].copy()\n",
        "            price_ts['timestamp'] = price_ts['timestamp'].astype(int)\n",
        "            deriv_df['timestamp'] = deriv_df['timestamp'].astype(int)\n",
        "            price_ts = price_ts.sort_values('timestamp')\n",
        "            deriv_df = deriv_df.sort_values('timestamp')\n",
        "            merged = pd.merge_asof(price_ts, deriv_df, on='timestamp', direction='backward')\n",
        "            for col in ['funding_rate', 'open_interest', 'long_short_ratio',\n",
        "                        'taker_buy_vol', 'taker_sell_vol']:\n",
        "                if col in merged.columns:\n",
        "                    result[col] = merged[col].reset_index(drop=True)\n",
        "\n",
        "    # Align Fear & Greed (daily -> forward-fill to 5-min bars)\n",
        "    if fear_greed_df is not None and 'timestamp' in price_df.columns:\n",
        "        fng = fear_greed_df.copy()\n",
        "        fng['timestamp'] = fng['timestamp'].astype(int)\n",
        "        fng = fng.sort_values('timestamp')\n",
        "        price_ts = price_df[['timestamp']].copy()\n",
        "        price_ts['timestamp'] = price_ts['timestamp'].astype(int)\n",
        "        price_ts = price_ts.sort_values('timestamp')\n",
        "        merged_fng = pd.merge_asof(price_ts, fng, on='timestamp', direction='backward')\n",
        "        if 'fear_greed' in merged_fng.columns:\n",
        "            result['fear_greed'] = merged_fng['fear_greed'].reset_index(drop=True)\n",
        "\n",
        "    return result if result else None\n",
        "\n",
        "\n",
        "def build_full_feature_matrix(price_df, cross_data=None, pair_name=None, derivatives_data=None):\n",
        "    \"\"\"(N, INPUT_DIM) matrix — all features for entire DataFrame at once.\"\"\"\n",
        "    if price_df is None or len(price_df) < 30:\n",
        "        return None\n",
        "    df = price_df.copy().reset_index(drop=True)\n",
        "    if cross_data is not None:\n",
        "        cross_data = {p: cdf.copy().reset_index(drop=True) for p, cdf in cross_data.items() if p != pair_name}\n",
        "        if not cross_data:\n",
        "            cross_data = None\n",
        "    close = df['close'].astype(float) if 'close' in df.columns else None\n",
        "    if close is None:\n",
        "        return None\n",
        "    vol = df['volume'].astype(float) if 'volume' in df.columns else None\n",
        "\n",
        "    features = _compute_single_pair_features(df)\n",
        "    if cross_data is not None and pair_name is not None:\n",
        "        features.update(_build_cross_features(close, vol, cross_data, pair_name))\n",
        "\n",
        "    # Add derivatives features (7 features — zeros if no data)\n",
        "    features.update(_build_derivatives_features(len(df), derivatives_data))\n",
        "\n",
        "    feat_df = pd.DataFrame(features, index=df.index)\n",
        "    feat_df = feat_df.replace([np.inf, -np.inf], np.nan).ffill().bfill().fillna(0)\n",
        "    feat_arr = feat_df.values.astype(np.float32)\n",
        "    n_feat = feat_arr.shape[1]\n",
        "    if n_feat < INPUT_DIM:\n",
        "        feat_arr = np.concatenate([feat_arr, np.zeros((len(feat_arr), INPUT_DIM - n_feat), dtype=np.float32)], axis=1)\n",
        "    elif n_feat > INPUT_DIM:\n",
        "        feat_arr = feat_arr[:, :INPUT_DIM]\n",
        "    return feat_arr\n",
        "\n",
        "\n",
        "def build_feature_sequence(price_df, seq_len=30, cross_data=None, pair_name=None, derivatives_data=None):\n",
        "    \"\"\"(seq_len, INPUT_DIM) for a single window — with per-window standardization.\"\"\"\n",
        "    if price_df is None or len(price_df) < seq_len:\n",
        "        return None\n",
        "    df = price_df.tail(seq_len + 50).copy()\n",
        "    if cross_data is not None:\n",
        "        cross_data = {p: cdf.tail(seq_len + 50).copy().reset_index(drop=True) for p, cdf in cross_data.items() if p != pair_name}\n",
        "        if not cross_data:\n",
        "            cross_data = None\n",
        "    df = df.reset_index(drop=True)\n",
        "    close = df['close'].astype(float) if 'close' in df.columns else None\n",
        "    if close is None:\n",
        "        return None\n",
        "    vol = df['volume'].astype(float) if 'volume' in df.columns else None\n",
        "\n",
        "    features = _compute_single_pair_features(df)\n",
        "    if cross_data is not None and pair_name is not None:\n",
        "        features.update(_build_cross_features(close, vol, cross_data, pair_name))\n",
        "\n",
        "    # Add derivatives features\n",
        "    features.update(_build_derivatives_features(len(df), derivatives_data))\n",
        "\n",
        "    feat_df = pd.DataFrame(features, index=df.index)\n",
        "    feat_df = feat_df.replace([np.inf, -np.inf], np.nan).ffill().bfill().fillna(0)\n",
        "    feat_arr = feat_df.tail(seq_len).values.astype(np.float32)\n",
        "\n",
        "    # Per-window standardization\n",
        "    mean = feat_arr.mean(axis=0, keepdims=True)\n",
        "    std = feat_arr.std(axis=0, keepdims=True) + 1e-8\n",
        "    feat_arr = (feat_arr - mean) / std\n",
        "\n",
        "    n_feat = feat_arr.shape[1]\n",
        "    if n_feat < INPUT_DIM:\n",
        "        feat_arr = np.concatenate([feat_arr, np.zeros((seq_len, INPUT_DIM - n_feat), dtype=np.float32)], axis=1)\n",
        "    elif n_feat > INPUT_DIM:\n",
        "        feat_arr = feat_arr[:, :INPUT_DIM]\n",
        "    return feat_arr\n",
        "\n",
        "\n",
        "# Verification\n",
        "test_pair = list(pair_dfs.keys())[0]\n",
        "test_df = pair_dfs[test_pair]\n",
        "\n",
        "f = _compute_single_pair_features(test_df.head(200))\n",
        "print(f'Single-pair features: {len(f)} (expected 46)')\n",
        "\n",
        "df = _build_derivatives_features(200, None)\n",
        "print(f'Derivatives features: {len(df)} (expected 7)')\n",
        "\n",
        "feat_seq = build_feature_sequence(test_df.head(200), seq_len=30)\n",
        "print(f'Per-window shape: {feat_seq.shape} (expected (30, {INPUT_DIM}))')\n",
        "feat_full = build_full_feature_matrix(test_df.head(1000))\n",
        "print(f'Full-matrix shape: {feat_full.shape} (expected (1000, {INPUT_DIM}))')\n",
        "\n",
        "early = build_feature_sequence(test_df.head(200), seq_len=30)\n",
        "late = build_feature_sequence(test_df.tail(200), seq_len=30)\n",
        "print(f'\\nScale invariance (per-window standardized):')\n",
        "print(f'  Early abs mean: {np.abs(early).mean():.4f}')\n",
        "print(f'  Late abs mean:  {np.abs(late).mean():.4f}')\n",
        "print(f'  Ratio: {np.abs(late).mean() / np.abs(early).mean():.2f}x (should be ~1.0)')\n",
        "\n",
        "assert feat_seq.shape == (30, INPUT_DIM)\n",
        "assert feat_full.shape[1] == INPUT_DIM\n",
        "print('\\nAll feature pipeline tests passed!')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single-pair features: 46 (expected 46)\n",
            "Derivatives features: 7 (expected 7)\n",
            "Per-window shape: (30, 98) (expected (30, 98))\n",
            "Full-matrix shape: (1000, 98) (expected (1000, 98))\n",
            "\n",
            "Scale invariance (per-window standardized):\n",
            "  Early abs mean: 0.3809\n",
            "  Late abs mean:  0.3730\n",
            "  Ratio: 0.98x (should be ~1.0)\n",
            "\n",
            "All feature pipeline tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvyFI4IwEJFj"
      },
      "source": [
        "## 3. Model Architectures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhEXKTNeEJFk",
        "outputId": "f0ded07e-a2d4-47f8-a384-f8eeb2955242"
      },
      "source": [
        "# ================================================================\n",
        "# ATTENTION & POSITIONAL ENCODING\n",
        "# ================================================================\n",
        "\n",
        "class _TrainedAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, qkv_dim):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = qkv_dim // n_heads\n",
        "        self.w_q = nn.Linear(d_model, qkv_dim)\n",
        "        self.w_k = nn.Linear(d_model, qkv_dim)\n",
        "        self.w_v = nn.Linear(d_model, qkv_dim)\n",
        "        self.w_o = nn.Linear(qkv_dim, d_model)\n",
        "        self.attention_temperature = nn.Parameter(torch.ones(1))\n",
        "        self.quantum_enhancement_scale = nn.Parameter(torch.ones(n_heads))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, S, _ = x.shape\n",
        "        q = self.w_q(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        k = self.w_k(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        v = self.w_v(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        scale = math.sqrt(self.d_head) * self.attention_temperature\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / scale\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        enhancement = self.quantum_enhancement_scale.view(1, self.n_heads, 1, 1)\n",
        "        attn = attn * enhancement\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, S, -1)\n",
        "        return self.w_o(out)\n",
        "\n",
        "\n",
        "class _TrainedPosEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=128):\n",
        "        super().__init__()\n",
        "        self.quantum_phase = nn.Parameter(torch.zeros(d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[0, :, 0::2] = torch.sin(pos * div)\n",
        "        pe[0, :, 1::2] = torch.cos(pos * div[:d_model // 2])\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :] * torch.cos(self.quantum_phase)\n",
        "\n",
        "\n",
        "class _TrainedTransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, qkv_dim, d_ff, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.attention = _TrainedAttention(d_model, n_heads, qkv_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model), nn.Dropout(dropout))\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.skip_enhancement = nn.Parameter(torch.tensor(1.0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm1(x + self.skip_enhancement * self.attention(x))\n",
        "        x = self.norm2(x + self.feed_forward(x))\n",
        "        return x\n",
        "\n",
        "print('Attention & positional encoding defined')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention & positional encoding defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYxY1cdkEJFk",
        "outputId": "6c1a9716-7ead-406e-e1d8-00c7966099a4"
      },
      "source": [
        "# ================================================================\n",
        "# ALL 7 MODEL ARCHITECTURES\n",
        "# ================================================================\n",
        "\n",
        "class TrainedQuantumTransformer(nn.Module):\n",
        "    def __init__(self, input_dim=INPUT_DIM):\n",
        "        super().__init__()\n",
        "        d_model, n_heads, qkv_dim, d_ff, n_blocks = 288, 8, 328, 1315, 4\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        self.pos_encoding = _TrainedPosEncoding(d_model)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            _TrainedTransformerBlock(d_model, n_heads, qkv_dim, d_ff) for _ in range(n_blocks)])\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.BatchNorm1d(d_model), nn.GELU(),\n",
        "            nn.Linear(d_model, 144), nn.GELU(), nn.Dropout(0.2),\n",
        "            nn.Linear(144, 72), nn.GELU(), nn.Linear(72, 1), nn.Tanh())\n",
        "        self.uncertainty_head = nn.Sequential(\n",
        "            nn.Linear(d_model, 72), nn.ReLU(), nn.Linear(72, 1), nn.Softplus())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_projection(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "        pooled = x.mean(dim=1)\n",
        "        return self.output_head(pooled), self.uncertainty_head(pooled)\n",
        "\n",
        "\n",
        "class _TrainedLSTMCore(nn.Module):\n",
        "    def __init__(self, input_size=INPUT_DIM, hidden_size=292, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        bidir_dim = hidden_size * 2\n",
        "        self.lstm_layers = nn.ModuleList()\n",
        "        self.skip_projections = nn.ModuleList()\n",
        "        self.consciousness_gates = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            in_dim = input_size if i == 0 else bidir_dim\n",
        "            self.lstm_layers.append(nn.LSTM(\n",
        "                input_size=in_dim, hidden_size=hidden_size,\n",
        "                num_layers=1, batch_first=True, bidirectional=True))\n",
        "            self.skip_projections.append(nn.Linear(in_dim, bidir_dim))\n",
        "            self.consciousness_gates.append(nn.Sequential(\n",
        "                nn.Linear(bidir_dim, bidir_dim), nn.Sigmoid()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, lstm_layer in enumerate(self.lstm_layers):\n",
        "            skip = self.skip_projections[i](x)\n",
        "            out, _ = lstm_layer(x)\n",
        "            gate = self.consciousness_gates[i](out)\n",
        "            x = gate * out + (1 - gate) * skip\n",
        "        return x\n",
        "\n",
        "\n",
        "class TrainedBidirectionalLSTM(nn.Module):\n",
        "    def __init__(self, input_dim=INPUT_DIM):\n",
        "        super().__init__()\n",
        "        bidir_dim = 584\n",
        "        self.lstm = _TrainedLSTMCore(input_size=input_dim, hidden_size=292, num_layers=2)\n",
        "        self.prediction_head = nn.Sequential(\n",
        "            nn.BatchNorm1d(bidir_dim), nn.GELU(),\n",
        "            nn.Linear(bidir_dim, 292), nn.GELU(),\n",
        "            nn.BatchNorm1d(292), nn.GELU(),\n",
        "            nn.Linear(292, 146), nn.GELU(), nn.Linear(146, 1), nn.Tanh())\n",
        "        self.confidence_head = nn.Sequential(\n",
        "            nn.Linear(bidir_dim, 73), nn.ReLU(), nn.Linear(73, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        pooled = self.lstm(x).mean(dim=1)\n",
        "        return self.prediction_head(pooled), self.confidence_head(pooled)\n",
        "\n",
        "\n",
        "class _TrainedDilatedBlock(nn.Module):\n",
        "    def __init__(self, channels, dilation):\n",
        "        super().__init__()\n",
        "        self.add_module('0', nn.Conv1d(channels, channels, kernel_size=3, dilation=dilation, padding=dilation))\n",
        "        self.add_module('1', nn.BatchNorm1d(channels))\n",
        "        self.add_module('4', nn.Conv1d(channels, channels, kernel_size=1))\n",
        "        self.add_module('5', nn.BatchNorm1d(channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(getattr(self, '1')(getattr(self, '0')(x)))\n",
        "        h = F.dropout(h, p=0.2, training=self.training)\n",
        "        h = getattr(self, '5')(getattr(self, '4')(h))\n",
        "        return F.relu(h + x)\n",
        "\n",
        "\n",
        "class _TrainedDilatedCNNCore(nn.Module):\n",
        "    def __init__(self, channels=INPUT_DIM, hidden=332, n_blocks=5):\n",
        "        super().__init__()\n",
        "        self.conv_blocks = nn.ModuleList([\n",
        "            _TrainedDilatedBlock(channels, dilation=2**i) for i in range(n_blocks)])\n",
        "        fusion_in = channels * n_blocks\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Conv1d(fusion_in, hidden, kernel_size=1), nn.BatchNorm1d(hidden),\n",
        "            nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Conv1d(hidden, hidden, kernel_size=1), nn.BatchNorm1d(hidden))\n",
        "        self.attention_pool = nn.Sequential(\n",
        "            nn.Softmax(dim=-1),\n",
        "            nn.Conv1d(hidden, channels, kernel_size=1), nn.ReLU(),\n",
        "            nn.Conv1d(channels, hidden, kernel_size=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        block_outputs = []\n",
        "        h = x\n",
        "        for block in self.conv_blocks:\n",
        "            h = block(h)\n",
        "            block_outputs.append(h)\n",
        "        cat = torch.cat(block_outputs, dim=1)\n",
        "        fused = F.relu(self.fusion(cat))\n",
        "        attn = self.attention_pool(fused)\n",
        "        return (fused * F.softmax(attn, dim=-1)).sum(dim=-1)\n",
        "\n",
        "\n",
        "class TrainedDilatedCNN(nn.Module):\n",
        "    def __init__(self, input_dim=INPUT_DIM):\n",
        "        super().__init__()\n",
        "        hidden = 332\n",
        "        self.dilated_cnn = _TrainedDilatedCNNCore(channels=input_dim, hidden=hidden, n_blocks=5)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.BatchNorm1d(hidden), nn.GELU(),\n",
        "            nn.Linear(hidden, 166), nn.BatchNorm1d(166), nn.GELU(), nn.Dropout(0.2),\n",
        "            nn.Linear(166, input_dim), nn.BatchNorm1d(input_dim), nn.GELU(),\n",
        "            nn.Linear(input_dim, 1), nn.Tanh())\n",
        "        self.pattern_strength = nn.Sequential(\n",
        "            nn.Linear(hidden, 41), nn.ReLU(), nn.Linear(41, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        pooled = self.dilated_cnn(x)\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "\n",
        "class TrainedCNN(nn.Module):\n",
        "    def __init__(self, input_dim=INPUT_DIM):\n",
        "        super().__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv1d(input_dim, 128, kernel_size=3, padding=1), nn.BatchNorm1d(128), nn.GELU(),\n",
        "            nn.Conv1d(128, 256, kernel_size=5, padding=2), nn.BatchNorm1d(256), nn.GELU(),\n",
        "            nn.Conv1d(256, 128, kernel_size=7, padding=3), nn.BatchNorm1d(128), nn.GELU(),\n",
        "            nn.Conv1d(128, 64, kernel_size=3, padding=1), nn.BatchNorm1d(64), nn.GELU())\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64, 32), nn.GELU(), nn.Dropout(0.2), nn.Linear(32, 1), nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.conv_layers(x)\n",
        "        return self.classifier(x.mean(dim=-1))\n",
        "\n",
        "\n",
        "class TrainedGRU(nn.Module):\n",
        "    def __init__(self, input_dim=INPUT_DIM):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_size=input_dim, hidden_size=134,\n",
        "                          num_layers=2, batch_first=True, bidirectional=True, dropout=0.2)\n",
        "        bidir_dim = 268\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(bidir_dim, 134), nn.GELU(), nn.Dropout(0.2),\n",
        "            nn.Linear(134, 64), nn.GELU(), nn.Linear(64, 1), nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        return self.classifier(out.mean(dim=1))\n",
        "\n",
        "\n",
        "class TrainedMetaEnsemble(nn.Module):\n",
        "    def __init__(self, input_dim=INPUT_DIM, n_models=N_BASE_MODELS):\n",
        "        super().__init__()\n",
        "        self._input_dim = input_dim\n",
        "        self._n_models = n_models\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128), nn.BatchNorm1d(128), nn.GELU(), nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.GELU())\n",
        "        self.weight_generator = nn.Sequential(\n",
        "            nn.Linear(64, 32), nn.GELU(), nn.Linear(32, n_models))\n",
        "        self.final_predictor = nn.Sequential(\n",
        "            nn.Linear(64 + n_models, 32), nn.GELU(), nn.Dropout(0.1), nn.Linear(32, 1), nn.Tanh())\n",
        "        self.confidence_estimator = nn.Sequential(\n",
        "            nn.Linear(64 + n_models, 16), nn.ReLU(), nn.Linear(16, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = x[:, :self._input_dim]\n",
        "        model_preds = x[:, self._input_dim:]\n",
        "        ctx = self.feature_extractor(features)\n",
        "        weights = F.softmax(self.weight_generator(ctx), dim=-1)\n",
        "        combined = torch.cat([ctx, model_preds], dim=-1)\n",
        "        return self.final_predictor(combined), self.confidence_estimator(combined)\n",
        "\n",
        "\n",
        "class VariationalAutoEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim=32, hidden_dims=None):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        if hidden_dims is None:\n",
        "            hidden_dims = [256, 128, 64]\n",
        "        encoder_layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h in hidden_dims:\n",
        "            encoder_layers.extend([nn.Linear(prev_dim, h), nn.BatchNorm1d(h), nn.LeakyReLU(0.2), nn.Dropout(0.2)])\n",
        "            prev_dim = h\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)\n",
        "        decoder_layers = []\n",
        "        prev_dim = latent_dim\n",
        "        for h in reversed(hidden_dims):\n",
        "            decoder_layers.extend([nn.Linear(prev_dim, h), nn.BatchNorm1d(h), nn.LeakyReLU(0.2), nn.Dropout(0.2)])\n",
        "            prev_dim = h\n",
        "        decoder_layers.append(nn.Linear(hidden_dims[0], input_dim))\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        return self.fc_mu(h), self.fc_logvar(h)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        return mu + std * torch.randn_like(std)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon = self.decode(z)\n",
        "        return recon, mu, logvar\n",
        "\n",
        "\n",
        "# Verify all models\n",
        "x_test = torch.randn(2, 30, INPUT_DIM)\n",
        "for name, cls in [('QT', TrainedQuantumTransformer), ('BiLSTM', TrainedBidirectionalLSTM),\n",
        "                   ('DilatedCNN', TrainedDilatedCNN), ('CNN', TrainedCNN), ('GRU', TrainedGRU)]:\n",
        "    m = cls(input_dim=INPUT_DIM)\n",
        "    out = m(x_test)\n",
        "    shape = out[0].shape if isinstance(out, tuple) else out.shape\n",
        "    print(f'{name}: output {shape}')\n",
        "\n",
        "meta_test = torch.randn(2, INPUT_DIM + N_BASE_MODELS)\n",
        "m = TrainedMetaEnsemble(input_dim=INPUT_DIM, n_models=N_BASE_MODELS)\n",
        "print(f'MetaEnsemble: output {m(meta_test)[0].shape}')\n",
        "\n",
        "vae_test = torch.randn(2, INPUT_DIM)\n",
        "m = VariationalAutoEncoder(input_dim=INPUT_DIM)\n",
        "print(f'VAE: output {m(vae_test)[0].shape}')\n",
        "\n",
        "print('\\nAll 7 model architectures verified!')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QT: output torch.Size([2, 1])\n",
            "BiLSTM: output torch.Size([2, 1])\n",
            "DilatedCNN: output torch.Size([2, 1])\n",
            "CNN: output torch.Size([2, 1])\n",
            "GRU: output torch.Size([2, 1])\n",
            "MetaEnsemble: output torch.Size([2, 1])\n",
            "VAE: output torch.Size([2, 98])\n",
            "\n",
            "All 7 model architectures verified!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTk11NcUEJFl"
      },
      "source": [
        "## 4. Training Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E347KB2EJFl",
        "outputId": "36d521e4-6ff5-47d3-e805-e2d212224f4f"
      },
      "source": [
        "class DirectionalLoss(nn.Module):\n",
        "    \"\"\"v7: Recalibrated for tanh-bounded model outputs in [-1, 1].\n",
        "\n",
        "    v6 was calibrated for unbounded micro-predictions (~0.01-0.05). With models\n",
        "    now outputting tanh-bounded [-1, 1] values, the loss needs recalibration:\n",
        "\n",
        "    - logit_scale: 20.0 -> 3.0 (tanh outputs are already in [-1,1], 3x gives\n",
        "      logits in [-3,3] = ~5%-95% probability range. 20x gave [-20,20] = always\n",
        "      99.99% confident, destroying gradient.)\n",
        "    - margin: 0.10 -> 0.25 (with tanh outputs spanning [-1,1], require 0.25\n",
        "      separation — meaningful directional commitment, not micro-signal)\n",
        "    - mag_floor: 0.01 -> 0.10 (push |pred| above 0.10 — prevents collapse to\n",
        "      zero while leaving room for low-confidence predictions)\n",
        "    - mag_weight: 5.0 -> 3.0 (softer penalty since tanh naturally bounds outputs)\n",
        "\n",
        "    At collapse (all pred=0), v7 penalty:\n",
        "      BCE(0*3, 0.5) = 0.693 (uninformative)\n",
        "      + 10 * relu(0.25 - 0) = 2.5 (strong anti-collapse)\n",
        "      + 3.0 * relu(0.10 - 0) = 0.30\n",
        "      Total = 3.49 (forces model away from zero quickly)\n",
        "    \"\"\"\n",
        "    def __init__(self, logit_scale=3.0, margin=0.25):\n",
        "        super().__init__()\n",
        "        self.logit_scale = logit_scale\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.squeeze(-1) if pred.dim() > 1 else pred\n",
        "        target = target.squeeze(-1) if target.dim() > 1 else target\n",
        "\n",
        "        # 1. BCE direction: logit_scale=3.0 maps tanh output to reasonable probabilities\n",
        "        #    pred=0.5 -> logit=1.5 -> 82% prob, pred=1.0 -> logit=3.0 -> 95% prob\n",
        "        target_pos = (target > 0).float()\n",
        "        bce = F.binary_cross_entropy_with_logits(\n",
        "            pred * self.logit_scale, target_pos)\n",
        "\n",
        "        # 2. Separation margin: require 0.25 gap between up/down predictions\n",
        "        pos_mask = target > 0\n",
        "        neg_mask = target <= 0\n",
        "        if pos_mask.any() and neg_mask.any():\n",
        "            separation = pred[pos_mask].mean() - pred[neg_mask].mean()\n",
        "            sep_loss = F.relu(self.margin - separation)\n",
        "        else:\n",
        "            sep_loss = torch.tensor(0.0, device=pred.device)\n",
        "\n",
        "        # 3. Magnitude floor: push |pred| above 0.10 (was 0.01)\n",
        "        mag_loss = F.relu(0.10 - pred.abs()).mean()\n",
        "\n",
        "        return bce + 10.0 * sep_loss + 3.0 * mag_loss\n",
        "\n",
        "def directional_accuracy(predictions, targets):\n",
        "    return float(np.mean(np.sign(predictions) == np.sign(targets)))\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion, gradient_clip=1.0):\n",
        "    model.train()\n",
        "    total_loss, n = 0.0, 0\n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_batch)\n",
        "        pred = output[0] if isinstance(output, tuple) else output\n",
        "        loss = criterion(pred.squeeze(-1), y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item(); n += 1\n",
        "    return total_loss / max(n, 1)\n",
        "\n",
        "\n",
        "def validate_epoch(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, n = 0.0, 0\n",
        "    all_preds, all_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            output = model(X_batch)\n",
        "            pred = output[0] if isinstance(output, tuple) else output\n",
        "            pred = pred.squeeze(-1)\n",
        "            total_loss += criterion(pred, y_batch).item(); n += 1\n",
        "            all_preds.append(pred.cpu().numpy())\n",
        "            all_targets.append(y_batch.cpu().numpy())\n",
        "    preds = np.concatenate(all_preds) if all_preds else np.array([])\n",
        "    targets = np.concatenate(all_targets) if all_targets else np.array([])\n",
        "    acc = directional_accuracy(preds, targets) if len(preds) > 0 else 0.5\n",
        "    pred_std = float(np.std(preds)) if len(preds) > 0 else 0.0\n",
        "    return total_loss / max(n, 1), acc, pred_std\n",
        "\n",
        "print('Training utilities defined (v7: BCE*3 + 10*sep_margin(0.25) + 3*mag_floor(0.10))')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training utilities defined (v7: BCE*3 + 10*sep_margin(0.25) + 3*mag_floor(0.10))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMGVNYmLEJFl"
      },
      "source": [
        "## 5. Generate Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO7SaUJ0EJFl"
      },
      "source": [
        "def walk_forward_split(pair_dfs, train_frac=0.7, val_frac=0.13):\n",
        "    train_dfs, val_dfs, test_dfs = {}, {}, {}\n",
        "    for pair, df in pair_dfs.items():\n",
        "        n = len(df)\n",
        "        t_end = int(n * train_frac)\n",
        "        v_end = int(n * (train_frac + val_frac))\n",
        "        train_dfs[pair] = df.iloc[:t_end].copy().reset_index(drop=True)\n",
        "        val_dfs[pair] = df.iloc[t_end:v_end].copy().reset_index(drop=True)\n",
        "        test_dfs[pair] = df.iloc[v_end:].copy().reset_index(drop=True)\n",
        "        print(f'  {pair}: train={len(train_dfs[pair]):,}, val={len(val_dfs[pair]):,}, test={len(test_dfs[pair]):,}')\n",
        "    return train_dfs, val_dfs, test_dfs\n",
        "\n",
        "\n",
        "def generate_sequences_fast(pair_dfs, seq_len=SEQ_LEN, stride=1, cross_asset=True,\n",
        "                            derivatives_dfs=None, fear_greed_df=None):\n",
        "    \"\"\"Vectorized sequence generation with 6-bar soft labels.\n",
        "\n",
        "    Labels: 30-min forward return x 100, clipped to [-1, 1].\n",
        "    This gives the model continuous gradient signal proportional to\n",
        "    move magnitude, instead of hard +/-1 that causes dead-zone collapse.\n",
        "\n",
        "    Args:\n",
        "        pair_dfs: Dict of pair -> DataFrame with OHLCV columns\n",
        "        seq_len: Sequence length per sample\n",
        "        stride: Step size between windows\n",
        "        cross_asset: If True, include 15 cross-asset features\n",
        "        derivatives_dfs: Optional dict of pair -> DataFrame with derivatives columns\n",
        "        fear_greed_df: Optional DataFrame with [timestamp, fear_greed]\n",
        "    \"\"\"\n",
        "    all_X, all_y = [], []\n",
        "    warmup = 50\n",
        "\n",
        "    for pair, df in pair_dfs.items():\n",
        "        min_rows = seq_len + warmup + LABEL_HORIZON\n",
        "        if len(df) < min_rows:\n",
        "            print(f'  Skipping {pair}: only {len(df)} bars (need {min_rows})')\n",
        "            continue\n",
        "\n",
        "        cross_data = None\n",
        "        if cross_asset and len(pair_dfs) > 1:\n",
        "            cross_data = {p: odf for p, odf in pair_dfs.items() if p != pair}\n",
        "\n",
        "        # Align derivatives data for this pair\n",
        "        deriv_data = _align_derivatives(df, pair, derivatives_dfs, fear_greed_df)\n",
        "\n",
        "        feat_matrix = build_full_feature_matrix(\n",
        "            df, cross_data=cross_data, pair_name=pair,\n",
        "            derivatives_data=deriv_data,\n",
        "        )\n",
        "        if feat_matrix is None:\n",
        "            print(f'  Skipping {pair}: feature computation failed')\n",
        "            continue\n",
        "\n",
        "        close_vals = df['close'].values.astype(float)\n",
        "        n_samples = 0\n",
        "\n",
        "        for end_idx in range(warmup + seq_len, len(df) - LABEL_HORIZON + 1, stride):\n",
        "            start_idx = end_idx - seq_len\n",
        "\n",
        "            # Future price: LABEL_HORIZON bars after the window ends\n",
        "            future_idx = end_idx + LABEL_HORIZON - 1\n",
        "            if future_idx >= len(df):\n",
        "                break\n",
        "\n",
        "            window = feat_matrix[start_idx:end_idx]\n",
        "\n",
        "            # Per-window standardization\n",
        "            mean = window.mean(axis=0, keepdims=True)\n",
        "            std = window.std(axis=0, keepdims=True) + 1e-8\n",
        "            window = (window - mean) / std\n",
        "\n",
        "            current_close = close_vals[end_idx - 1]\n",
        "            future_close = close_vals[future_idx]\n",
        "            if current_close <= 0:\n",
        "                continue\n",
        "\n",
        "            # Soft label: 6-bar forward return, scaled and clipped to [-1, 1]\n",
        "            ret = future_close / current_close - 1.0\n",
        "            label = float(np.clip(ret * LABEL_SCALE, -1.0, 1.0))\n",
        "\n",
        "            all_X.append(window)\n",
        "            all_y.append(label)\n",
        "            n_samples += 1\n",
        "\n",
        "        print(f'  {pair}: {n_samples:,} sequences')\n",
        "\n",
        "    if not all_X:\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    X = np.array(all_X, dtype=np.float32)\n",
        "    y = np.array(all_y, dtype=np.float32)\n",
        "    up = (y > 0).sum()\n",
        "    down = (y < 0).sum()\n",
        "    print(f'Total: {len(X):,} sequences (dim={X.shape[-1]}), '\n",
        "          f'balance: {up:,} up / {down:,} down '\n",
        "          f'({up/len(y)*100:.1f}% / {down/len(y)*100:.1f}%)')\n",
        "    print(f'Label stats: mean={y.mean():.4f}, std={y.std():.4f}, '\n",
        "          f'min={y.min():.4f}, max={y.max():.4f}')\n",
        "    return X, y"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoReC-tmEJFm",
        "outputId": "51cfc194-3be9-46a6-fd5a-8a56d5c0dd7a"
      },
      "source": [
        "%%time\n",
        "print('Splitting data (walk-forward 70/13/17)...')\n",
        "train_dfs, val_dfs, test_dfs = walk_forward_split(pair_dfs)\n",
        "\n",
        "# Auto-adjust stride to fit in Colab RAM (~12 GB)\n",
        "total_train_bars = sum(len(df) for df in train_dfs.values())\n",
        "bytes_per_seq = SEQ_LEN * INPUT_DIM * 4\n",
        "max_ram_gb = 8.0\n",
        "max_sequences = int(max_ram_gb * 1e9 / bytes_per_seq)\n",
        "stride = max(1, total_train_bars // max_sequences)\n",
        "stride = max(stride, 3)\n",
        "\n",
        "print(f'\\nTotal train bars: {total_train_bars:,}')\n",
        "print(f'Auto stride={stride} (targets <{max_sequences:,} sequences to fit {max_ram_gb}GB RAM)')\n",
        "has_deriv = bool(derivatives_dfs) or fear_greed_df is not None\n",
        "print(f'Derivatives data: {\"yes\" if has_deriv else \"no (7 features zero-padded)\"}')\n",
        "\n",
        "print('\\nGenerating training sequences...')\n",
        "X_train, y_train = generate_sequences_fast(\n",
        "    train_dfs, stride=stride, cross_asset=True,\n",
        "    derivatives_dfs=derivatives_dfs, fear_greed_df=fear_greed_df,\n",
        ")\n",
        "\n",
        "print('\\nGenerating validation sequences...')\n",
        "X_val, y_val = generate_sequences_fast(\n",
        "    val_dfs, stride=stride, cross_asset=True,\n",
        "    derivatives_dfs=derivatives_dfs, fear_greed_df=fear_greed_df,\n",
        ")\n",
        "\n",
        "print('\\nGenerating test sequences...')\n",
        "X_test, y_test = generate_sequences_fast(\n",
        "    test_dfs, stride=stride, cross_asset=True,\n",
        "    derivatives_dfs=derivatives_dfs, fear_greed_df=fear_greed_df,\n",
        ")\n",
        "\n",
        "print(f'\\nDataset shapes: train={X_train.shape}, val={X_val.shape}, test={X_test.shape}')\n",
        "assert X_train.shape[-1] == INPUT_DIM\n",
        "mem_gb = (X_train.nbytes + X_val.nbytes + X_test.nbytes) / 1e9\n",
        "print(f'Total feature array memory: {mem_gb:.2f} GB')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting data (walk-forward 70/13/17)...\n",
            "  BTC-USD: train=622,045, val=115,523, test=151,069\n",
            "  ETH-USD: train=622,047, val=115,523, test=151,069\n",
            "  SOL-USD: train=402,096, val=74,675, test=97,653\n",
            "  DOGE-USD: train=487,244, val=90,488, test=118,331\n",
            "  AVAX-USD: train=397,809, val=73,879, test=96,611\n",
            "  LINK-USD: train=521,389, val=96,829, test=126,624\n",
            "\n",
            "Total train bars: 3,052,630\n",
            "Auto stride=4 (targets <680,272 sequences to fit 8.0GB RAM)\n",
            "Derivatives data: yes\n",
            "\n",
            "Generating training sequences...\n",
            "  BTC-USD: 155,490 sequences\n",
            "  ETH-USD: 155,491 sequences\n",
            "  SOL-USD: 100,503 sequences\n",
            "  DOGE-USD: 121,790 sequences\n",
            "  AVAX-USD: 99,431 sequences\n",
            "  LINK-USD: 130,326 sequences\n",
            "Total: 763,031 sequences (dim=98), balance: 378,394 up / 374,133 down (49.6% / 49.0%)\n",
            "Label stats: mean=0.0027, std=0.5224, min=-1.0000, max=1.0000\n",
            "\n",
            "Generating validation sequences...\n",
            "  BTC-USD: 28,860 sequences\n",
            "  ETH-USD: 28,860 sequences\n",
            "  SOL-USD: 18,648 sequences\n",
            "  DOGE-USD: 22,601 sequences\n",
            "  AVAX-USD: 18,449 sequences\n",
            "  LINK-USD: 24,186 sequences\n",
            "Total: 141,604 sequences (dim=98), balance: 70,432 up / 69,523 down (49.7% / 49.1%)\n",
            "Label stats: mean=0.0022, std=0.4591, min=-1.0000, max=1.0000\n",
            "\n",
            "Generating test sequences...\n",
            "  BTC-USD: 37,746 sequences\n",
            "  ETH-USD: 37,746 sequences\n",
            "  SOL-USD: 24,392 sequences\n",
            "  DOGE-USD: 29,562 sequences\n",
            "  AVAX-USD: 24,132 sequences\n",
            "  LINK-USD: 31,635 sequences\n",
            "Total: 185,213 sequences (dim=98), balance: 90,982 up / 90,917 down (49.1% / 49.1%)\n",
            "Label stats: mean=-0.0012, std=0.4509, min=-1.0000, max=1.0000\n",
            "\n",
            "Dataset shapes: train=(763031, 30, 98), val=(141604, 30, 98), test=(185213, 30, 98)\n",
            "Total feature array memory: 12.82 GB\n",
            "CPU times: user 1min 46s, sys: 10 s, total: 1min 56s\n",
            "Wall time: 1min 56s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOXugLzVEJFm"
      },
      "source": [
        "## 6. Train Base Models (Phase 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCycgZPwEJFm",
        "outputId": "cd5f05d6-402a-42db-9afc-9a458c3ec4a1"
      },
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "LR = 3e-4        # Reduced from 1e-3 — prevents overshoot on noisy data\n",
        "WARMUP_EPOCHS = 3 # Linear warmup before cosine decay\n",
        "PATIENCE = 15\n",
        "\n",
        "train_ds = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "val_ds = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f'Train batches: {len(train_loader)}, Val batches: {len(val_loader)}')\n",
        "print(f'Label horizon: {LABEL_HORIZON} bars ({LABEL_HORIZON * 5} min)')\n",
        "print(f'Label scale: {LABEL_SCALE} (1% return = label {LABEL_SCALE/100:.1f})')\n",
        "print(f'LR: {LR}, Warmup: {WARMUP_EPOCHS} epochs, Epochs: {EPOCHS}, Patience: {PATIENCE}')\n",
        "print(f'Loss: v6 (BCE*20 + 10*sep_margin(0.10) + 5*mag_floor)')\n",
        "print(f'QT optimizer: weight_decay=0, attention LR={LR*0.1:.1e} (0.1x), other LR={LR:.1e}')\n",
        "print(f'Other models: weight_decay=1e-4, LR={LR:.1e}')\n",
        "results = {}"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 11922, Val batches: 2213\n",
            "Label horizon: 6 bars (30 min)\n",
            "Label scale: 100 (1% return = label 1.0)\n",
            "LR: 0.0003, Warmup: 3 epochs, Epochs: 100, Patience: 15\n",
            "Loss: v6 (BCE*20 + 10*sep_margin(0.10) + 5*mag_floor)\n",
            "QT optimizer: weight_decay=0, attention LR=3.0e-05 (0.1x), other LR=3.0e-04\n",
            "Other models: weight_decay=1e-4, LR=3.0e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOzNAsKyEJFm"
      },
      "source": [
        "def train_base_model(name, model_cls):\n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'Training {name}')\n",
        "    print(f'{\"=\"*60}')\n",
        "\n",
        "    model = model_cls(input_dim=INPUT_DIM).to(device)\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'Parameters: {n_params:,}')\n",
        "\n",
        "    # v7: QT needs special optimizer — weight_decay compounds 13.5x more on\n",
        "    # full data (10K+ batches/epoch vs 780 local). Even 1e-5 per step = 10%\n",
        "    # shrinkage/epoch, which degenerates attention (softmax → uniform → constant).\n",
        "    #\n",
        "    # Fix: wd=0 + differential LR (attention at 0.1x) + collapse recovery.\n",
        "    if name == 'quantum_transformer':\n",
        "        # Split params: attention layers get lower LR and zero weight decay\n",
        "        attn_params = []\n",
        "        other_params = []\n",
        "        for pname, p in model.named_parameters():\n",
        "            if any(k in pname for k in ['attention', 'pos_encoding', 'skip_enhancement']):\n",
        "                attn_params.append(p)\n",
        "            else:\n",
        "                other_params.append(p)\n",
        "\n",
        "        optimizer = torch.optim.AdamW([\n",
        "            {'params': attn_params, 'lr': LR * 0.1, 'weight_decay': 0},\n",
        "            {'params': other_params, 'lr': LR, 'weight_decay': 0},\n",
        "        ])\n",
        "        n_attn = sum(p.numel() for p in attn_params)\n",
        "        n_other = sum(p.numel() for p in other_params)\n",
        "        print(f'Param groups: attention={n_attn:,} (lr={LR*0.1:.1e}, wd=0), '\n",
        "              f'other={n_other:,} (lr={LR:.1e}, wd=0)')\n",
        "    else:\n",
        "        wd = 1e-4\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=wd)\n",
        "        print(f'Weight decay: {wd}')\n",
        "\n",
        "    # Linear warmup + cosine decay\n",
        "    def lr_lambda(epoch):\n",
        "        if epoch < WARMUP_EPOCHS:\n",
        "            return (epoch + 1) / WARMUP_EPOCHS\n",
        "        progress = (epoch - WARMUP_EPOCHS) / max(EPOCHS - WARMUP_EPOCHS, 1)\n",
        "        return 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "    # v7 loss: BCE*3 + 10*separation_margin(0.25) + 3*magnitude_floor(0.10)\n",
        "    # Recalibrated for tanh-bounded model outputs in [-1, 1]\n",
        "    criterion = DirectionalLoss(logit_scale=3.0, margin=0.25)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_acc = 0.0\n",
        "    best_state = None\n",
        "    patience_counter = 0\n",
        "    collapse_recoveries = 0\n",
        "    max_collapse_recoveries = 3\n",
        "    t0 = time.time()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "        val_loss, val_acc, pred_std = validate_epoch(model, val_loader, criterion)\n",
        "        scheduler.step()\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_acc = val_acc\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        collapsed = pred_std < 0.001\n",
        "\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0 or patience_counter == 0 or collapsed:\n",
        "            tag = ' *** COLLAPSED ***' if collapsed else ''\n",
        "            print(f'  Epoch {epoch+1:3d}/{EPOCHS}: train={train_loss:.4f}, val={val_loss:.4f}, '\n",
        "                  f'acc={val_acc:.3f}, pred_std={pred_std:.4f}, '\n",
        "                  f'lr={optimizer.param_groups[0][\"lr\"]:.2e}{tag}')\n",
        "\n",
        "        # Collapse recovery: reload best checkpoint and halve all LRs\n",
        "        if collapsed and epoch >= WARMUP_EPOCHS and best_state is not None:\n",
        "            if collapse_recoveries < max_collapse_recoveries:\n",
        "                collapse_recoveries += 1\n",
        "                model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
        "                for pg in optimizer.param_groups:\n",
        "                    pg['lr'] *= 0.5\n",
        "                print(f'  >>> Collapse recovery #{collapse_recoveries}: reloaded best, '\n",
        "                      f'halved LR to {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
        "                patience_counter = 0  # Reset patience after recovery\n",
        "                continue\n",
        "            else:\n",
        "                print(f'  >>> Max collapse recoveries ({max_collapse_recoveries}) reached, stopping')\n",
        "                break\n",
        "\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f'  Early stopping at epoch {epoch+1}')\n",
        "            break\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    model.eval()\n",
        "\n",
        "    test_ds = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
        "    test_loader = DataLoader(test_ds, batch_size=128, shuffle=False)\n",
        "    test_loss, test_acc, test_std = validate_epoch(model, test_loader, criterion)\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    print(f'\\n  Test: loss={test_loss:.4f}, dir_acc={test_acc:.3f}, pred_std={test_std:.4f}')\n",
        "    print(f'  Time: {elapsed/60:.1f} min, Epochs: {epoch+1}, Collapse recoveries: {collapse_recoveries}')\n",
        "\n",
        "    save_path = f'models/trained/best_{name}_model.pth'\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f'  Saved: {save_path}')\n",
        "\n",
        "    results[name] = {\n",
        "        'val_loss': best_val_loss, 'val_acc': best_acc,\n",
        "        'test_loss': test_loss, 'test_acc': test_acc,\n",
        "        'test_pred_std': test_std,\n",
        "        'epochs': epoch + 1, 'time_min': elapsed / 60,\n",
        "        'collapse_recoveries': collapse_recoveries,\n",
        "    }\n",
        "    return model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fji4QDm-EJFn",
        "outputId": "00ba5b1f-b0ea-4023-c6dd-abb12f0ab598"
      },
      "source": [
        "base_configs = [\n",
        "    ('quantum_transformer', TrainedQuantumTransformer),\n",
        "    ('bidirectional_lstm', TrainedBidirectionalLSTM),\n",
        "    ('dilated_cnn', TrainedDilatedCNN),\n",
        "    ('cnn', TrainedCNN),\n",
        "    ('gru', TrainedGRU),\n",
        "]\n",
        "\n",
        "trained_base_models = {}\n",
        "for name, cls in base_configs:\n",
        "    try:\n",
        "        model = train_base_model(name, cls)\n",
        "        trained_base_models[name] = model\n",
        "    except Exception as e:\n",
        "        print(f'\\nFAILED: {name}: {e}')\n",
        "        results[name] = {'status': 'failed', 'error': str(e)}\n",
        "\n",
        "print(f'\\n{\"=\"*60}')\n",
        "print(f'Base models trained: {len(trained_base_models)}/{len(base_configs)}')\n",
        "print(f'{\"=\"*60}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training quantum_transformer\n",
            "============================================================\n",
            "Parameters: 4,659,718\n",
            "Param groups: attention=1,516,840 (lr=3.0e-05, wd=0), other=3,142,878 (lr=3.0e-04, wd=0)\n",
            "  Epoch   1/100: train=2.6521, val=2.7593, acc=0.512, pred_std=0.5398, lr=2.00e-05\n",
            "  Epoch   2/100: train=2.6079, val=2.6192, acc=0.517, pred_std=0.5653, lr=3.00e-05\n",
            "  Epoch   5/100: train=2.5384, val=2.6722, acc=0.515, pred_std=0.4779, lr=3.00e-05\n",
            "  Epoch  10/100: train=2.5192, val=2.6718, acc=0.514, pred_std=0.5204, lr=2.96e-05\n",
            "  Epoch  11/100: train=2.4697, val=2.6071, acc=0.515, pred_std=0.5098, lr=2.95e-05\n",
            "  Epoch  12/100: train=2.4550, val=2.5977, acc=0.519, pred_std=0.5512, lr=2.94e-05\n",
            "  Epoch  15/100: train=2.4312, val=2.6471, acc=0.518, pred_std=0.5023, lr=2.89e-05\n",
            "  Epoch  20/100: train=2.3645, val=2.6107, acc=0.515, pred_std=0.4991, lr=2.78e-05\n",
            "  Epoch  21/100: train=2.3479, val=2.5767, acc=0.517, pred_std=0.5343, lr=2.75e-05\n",
            "  Epoch  25/100: train=2.3007, val=2.6310, acc=0.516, pred_std=0.4620, lr=2.64e-05\n",
            "  Epoch  26/100: train=2.3001, val=2.5450, acc=0.519, pred_std=0.5046, lr=2.60e-05\n",
            "  Epoch  30/100: train=2.2524, val=2.6312, acc=0.513, pred_std=0.5067, lr=2.46e-05\n",
            "  Epoch  35/100: train=2.1760, val=2.6906, acc=0.514, pred_std=0.5114, lr=2.26e-05\n",
            "  Epoch  40/100: train=2.1248, val=2.6629, acc=0.515, pred_std=0.5083, lr=2.05e-05\n",
            "  Early stopping at epoch 41\n",
            "\n",
            "  Test: loss=2.7605, dir_acc=0.512, pred_std=0.5103\n",
            "  Time: 145.6 min, Epochs: 41, Collapse recoveries: 0\n",
            "  Saved: models/trained/best_quantum_transformer_model.pth\n",
            "\n",
            "============================================================\n",
            "Training bidirectional_lstm\n",
            "============================================================\n",
            "Parameters: 4,307,732\n",
            "Weight decay: 0.0001\n",
            "  Epoch   1/100: train=2.6117, val=2.6680, acc=0.516, pred_std=0.4663, lr=2.00e-04\n",
            "  Epoch   2/100: train=2.5056, val=2.6194, acc=0.517, pred_std=0.5045, lr=3.00e-04\n",
            "  Epoch   3/100: train=2.4578, val=2.6051, acc=0.518, pred_std=0.5011, lr=3.00e-04\n",
            "  Epoch   4/100: train=2.3639, val=2.5627, acc=0.518, pred_std=0.5143, lr=3.00e-04\n",
            "  Epoch   5/100: train=2.2304, val=2.6463, acc=0.518, pred_std=0.4929, lr=3.00e-04\n",
            "  Epoch  10/100: train=0.9876, val=2.8861, acc=0.513, pred_std=0.4309, lr=2.96e-04\n",
            "  Epoch  15/100: train=0.6267, val=2.9820, acc=0.511, pred_std=0.3817, lr=2.89e-04\n",
            "  Early stopping at epoch 19\n",
            "\n",
            "  Test: loss=2.7537, dir_acc=0.512, pred_std=0.5123\n",
            "  Time: 52.4 min, Epochs: 19, Collapse recoveries: 0\n",
            "  Saved: models/trained/best_bidirectional_lstm_model.pth\n",
            "\n",
            "============================================================\n",
            "Training dilated_cnn\n",
            "============================================================\n",
            "Parameters: 622,048\n",
            "Weight decay: 0.0001\n",
            "  Epoch   1/100: train=2.7226, val=2.5986, acc=0.517, pred_std=0.5612, lr=2.00e-04\n",
            "  Epoch   5/100: train=2.4318, val=2.6171, acc=0.516, pred_std=0.5051, lr=3.00e-04\n",
            "  Epoch   7/100: train=2.3438, val=2.5948, acc=0.516, pred_std=0.5018, lr=2.99e-04\n",
            "  Epoch  10/100: train=2.2357, val=2.6172, acc=0.516, pred_std=0.5315, lr=2.96e-04\n",
            "  Epoch  15/100: train=2.0479, val=2.6539, acc=0.517, pred_std=0.4973, lr=2.89e-04\n",
            "  Epoch  20/100: train=1.8735, val=2.7549, acc=0.514, pred_std=0.5047, lr=2.78e-04\n",
            "  Early stopping at epoch 22\n",
            "\n",
            "  Test: loss=2.7885, dir_acc=0.511, pred_std=0.5018\n",
            "  Time: 70.6 min, Epochs: 22, Collapse recoveries: 0\n",
            "  Saved: models/trained/best_dilated_cnn_model.pth\n",
            "\n",
            "============================================================\n",
            "Training cnn\n",
            "============================================================\n",
            "Parameters: 459,265\n",
            "Weight decay: 0.0001\n",
            "  Epoch   1/100: train=2.6572, val=2.6544, acc=0.517, pred_std=0.5239, lr=2.00e-04\n",
            "  Epoch   2/100: train=2.5427, val=2.5827, acc=0.517, pred_std=0.5539, lr=3.00e-04\n",
            "  Epoch   5/100: train=2.3585, val=2.6266, acc=0.518, pred_std=0.5107, lr=3.00e-04\n",
            "  Epoch  10/100: train=1.8592, val=2.7611, acc=0.512, pred_std=0.4837, lr=2.96e-04\n",
            "  Epoch  15/100: train=1.1409, val=2.9322, acc=0.508, pred_std=0.4212, lr=2.89e-04\n",
            "  Early stopping at epoch 17\n",
            "\n",
            "  Test: loss=2.7702, dir_acc=0.511, pred_std=0.5549\n",
            "  Time: 23.0 min, Epochs: 17, Collapse recoveries: 0\n",
            "  Saved: models/trained/best_cnn_model.pth\n",
            "\n",
            "============================================================\n",
            "Training gru\n",
            "============================================================\n",
            "Parameters: 557,703\n",
            "Weight decay: 0.0001\n",
            "  Epoch   1/100: train=2.6432, val=2.6573, acc=0.516, pred_std=0.5066, lr=2.00e-04\n",
            "  Epoch   2/100: train=2.5303, val=2.6190, acc=0.517, pred_std=0.4858, lr=3.00e-04\n",
            "  Epoch   3/100: train=2.4692, val=2.5779, acc=0.520, pred_std=0.6498, lr=3.00e-04\n",
            "  Epoch   5/100: train=2.3062, val=2.5857, acc=0.518, pred_std=0.5019, lr=3.00e-04\n",
            "  Epoch  10/100: train=1.7491, val=2.7883, acc=0.514, pred_std=0.4771, lr=2.96e-04\n",
            "  Epoch  15/100: train=1.2364, val=2.8288, acc=0.513, pred_std=0.4566, lr=2.89e-04\n",
            "  Early stopping at epoch 18\n",
            "\n",
            "  Test: loss=2.7592, dir_acc=0.512, pred_std=0.6517\n",
            "  Time: 32.9 min, Epochs: 18, Collapse recoveries: 0\n",
            "  Saved: models/trained/best_gru_model.pth\n",
            "\n",
            "============================================================\n",
            "Base models trained: 5/5\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FVmQMfeEJFn",
        "outputId": "9b90f1cb-8c88-48f4-d174-a88116cdf933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training LightGBM (gradient-boosted trees)\n",
            "============================================================\n",
            "  LGB features: (763031, 294) (last + mean + std of 98-dim sequence)\n",
            "  Label balance: train=0.496, val=0.497, test=0.491\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "[50]\tval's binary_logloss: 0.691812\n",
            "[100]\tval's binary_logloss: 0.691611\n",
            "[150]\tval's binary_logloss: 0.691563\n",
            "Early stopping, best iteration is:\n",
            "[136]\tval's binary_logloss: 0.691545\n",
            "\n",
            "  LightGBM results:\n",
            "    Val dir_acc:  0.514\n",
            "    Test dir_acc: 0.509\n",
            "    Pred std:     0.0745\n",
            "    Best round:   136\n",
            "    Time:         21.1s\n",
            "\n",
            "  Top 20 features by gain:\n",
            "     1.      last_18: 25121\n",
            "     2.        std_0: 8189\n",
            "     3.       std_35: 6396\n",
            "     4.      last_24: 5794\n",
            "     5.        std_2: 4256\n",
            "     6.      last_20: 4223\n",
            "     7.      last_34: 4174\n",
            "     8.      last_31: 3962\n",
            "     9.       std_36: 3913\n",
            "    10.      last_32: 3904\n",
            "    11.       last_3: 3778\n",
            "    12.      last_35: 3603\n",
            "    13.      last_33: 3423\n",
            "    14.      mean_33: 3211\n",
            "    15.      last_36: 3193\n",
            "    16.      last_12: 3114\n",
            "    17.       std_51: 3076\n",
            "    18.      last_19: 3062\n",
            "    19.       last_2: 2620\n",
            "    20.        std_1: 2457\n",
            "\n",
            "  Saved: models/trained/best_lightgbm_model.pkl (1.0 MB)\n",
            "  Saved: models/trained/lightgbm_meta.json\n",
            "\n",
            "  LightGBM registered as base model #6\n",
            "  trained_base_models keys: ['quantum_transformer', 'bidirectional_lstm', 'dilated_cnn', 'cnn', 'gru', 'lightgbm']\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# Section 6b: Train LightGBM (gradient-boosted trees)\n",
        "# ================================================================\n",
        "# LightGBM is structurally different from neural nets -- trees find\n",
        "# interaction effects and threshold rules that DL models miss.\n",
        "# This makes it an excellent diversifier in the meta-ensemble.\n",
        "# ================================================================\n",
        "\n",
        "import lightgbm as lgb\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "print(f'\\n{\"=\"*60}')\n",
        "print('Training LightGBM (gradient-boosted trees)')\n",
        "print(f'{\"=\"*60}')\n",
        "\n",
        "def _prepare_lgb_features(X_seq):\n",
        "    # Flatten sequence data for LightGBM: [last, mean, std] -> (N, INPUT_DIM*3)\n",
        "    last = X_seq[:, -1, :]           # (N, INPUT_DIM)\n",
        "    mean = X_seq.mean(axis=1)        # (N, INPUT_DIM)\n",
        "    std  = X_seq.std(axis=1)         # (N, INPUT_DIM)\n",
        "    return np.concatenate([last, mean, std], axis=1)  # (N, INPUT_DIM*3)\n",
        "\n",
        "lgb_X_train = _prepare_lgb_features(X_train)\n",
        "lgb_X_val   = _prepare_lgb_features(X_val)\n",
        "lgb_X_test  = _prepare_lgb_features(X_test)\n",
        "\n",
        "# Binary classification: is forward return positive?\n",
        "lgb_y_train = (y_train > 0).astype(int)\n",
        "lgb_y_val   = (y_val > 0).astype(int)\n",
        "lgb_y_test  = (y_test > 0).astype(int)\n",
        "\n",
        "print(f'  LGB features: {lgb_X_train.shape} (last + mean + std of {INPUT_DIM}-dim sequence)')\n",
        "print(f'  Label balance: train={lgb_y_train.mean():.3f}, val={lgb_y_val.mean():.3f}, test={lgb_y_test.mean():.3f}')\n",
        "\n",
        "lgb_train = lgb.Dataset(lgb_X_train, label=lgb_y_train)\n",
        "lgb_val   = lgb.Dataset(lgb_X_val, label=lgb_y_val, reference=lgb_train)\n",
        "\n",
        "lgb_params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.03,\n",
        "    'num_leaves': 63,\n",
        "    'max_depth': 7,\n",
        "    'min_child_samples': 50,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.6,\n",
        "    'reg_alpha': 0.1,\n",
        "    'reg_lambda': 1.0,\n",
        "    'verbose': -1,\n",
        "    'seed': 42,\n",
        "}\n",
        "\n",
        "callbacks = [\n",
        "    lgb.early_stopping(stopping_rounds=30),\n",
        "    lgb.log_evaluation(period=50),\n",
        "]\n",
        "\n",
        "t0 = time.time()\n",
        "lgb_model = lgb.train(\n",
        "    lgb_params,\n",
        "    lgb_train,\n",
        "    valid_sets=[lgb_val],\n",
        "    valid_names=['val'],\n",
        "    num_boost_round=500,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "lgb_elapsed = time.time() - t0\n",
        "\n",
        "# Evaluate\n",
        "lgb_val_prob = lgb_model.predict(lgb_X_val)\n",
        "lgb_test_prob = lgb_model.predict(lgb_X_test)\n",
        "\n",
        "# Convert probabilities to signed predictions: (prob - 0.5) * 2 -> [-1, 1]\n",
        "lgb_val_pred  = np.clip((lgb_val_prob  - 0.5) * 2.0, -1.0, 1.0)\n",
        "lgb_test_pred = np.clip((lgb_test_prob - 0.5) * 2.0, -1.0, 1.0)\n",
        "\n",
        "lgb_val_acc  = directional_accuracy(lgb_val_pred, y_val)\n",
        "lgb_test_acc = directional_accuracy(lgb_test_pred, y_test)\n",
        "lgb_pred_std = float(np.std(lgb_test_pred))\n",
        "\n",
        "print(f'\\n  LightGBM results:')\n",
        "print(f'    Val dir_acc:  {lgb_val_acc:.3f}')\n",
        "print(f'    Test dir_acc: {lgb_test_acc:.3f}')\n",
        "print(f'    Pred std:     {lgb_pred_std:.4f}')\n",
        "print(f'    Best round:   {lgb_model.best_iteration}')\n",
        "print(f'    Time:         {lgb_elapsed:.1f}s')\n",
        "\n",
        "# Feature importance (top 20)\n",
        "importance = lgb_model.feature_importance(importance_type='gain')\n",
        "feat_names = [f'feat_{i}' for i in range(lgb_X_train.shape[1])]\n",
        "# Label the three sections\n",
        "for i in range(INPUT_DIM):\n",
        "    feat_names[i] = f'last_{i}'\n",
        "    feat_names[INPUT_DIM + i] = f'mean_{i}'\n",
        "    feat_names[INPUT_DIM * 2 + i] = f'std_{i}'\n",
        "top_idx = np.argsort(importance)[::-1][:20]\n",
        "print(f'\\n  Top 20 features by gain:')\n",
        "for rank, idx in enumerate(top_idx):\n",
        "    print(f'    {rank+1:2d}. {feat_names[idx]:>12s}: {importance[idx]:.0f}')\n",
        "\n",
        "# Save LightGBM model\n",
        "lgb_pkl_path = 'models/trained/best_lightgbm_model.pkl'\n",
        "lgb_meta_path = 'models/trained/lightgbm_meta.json'\n",
        "\n",
        "with open(lgb_pkl_path, 'wb') as f:\n",
        "    pickle.dump(lgb_model, f)\n",
        "print(f'\\n  Saved: {lgb_pkl_path} ({os.path.getsize(lgb_pkl_path)/1e6:.1f} MB)')\n",
        "\n",
        "# Save metadata for local bot\n",
        "lgb_meta = {\n",
        "    'input_dim': INPUT_DIM,\n",
        "    'n_features': lgb_X_train.shape[1],\n",
        "    'feature_prep': 'last_mean_std',\n",
        "    'objective': 'binary',\n",
        "    'best_iteration': lgb_model.best_iteration,\n",
        "    'val_acc': float(lgb_val_acc),\n",
        "    'test_acc': float(lgb_test_acc),\n",
        "}\n",
        "with open(lgb_meta_path, 'w') as f:\n",
        "    json.dump(lgb_meta, f, indent=2)\n",
        "print(f'  Saved: {lgb_meta_path}')\n",
        "\n",
        "# Register in trained_base_models for meta-ensemble\n",
        "trained_base_models['lightgbm'] = lgb_model\n",
        "\n",
        "results['lightgbm'] = {\n",
        "    'val_loss': float(lgb_model.best_score['val']['binary_logloss']),\n",
        "    'val_acc': lgb_val_acc,\n",
        "    'test_acc': lgb_test_acc,\n",
        "    'test_pred_std': lgb_pred_std,\n",
        "    'epochs': lgb_model.best_iteration,\n",
        "    'time_min': lgb_elapsed / 60,\n",
        "}\n",
        "\n",
        "print(f'\\n  LightGBM registered as base model #{len(trained_base_models)}')\n",
        "print(f'  trained_base_models keys: {list(trained_base_models.keys())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abtqlkh1EJFn"
      },
      "source": [
        "## 7. Train Meta-Ensemble (Phase 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ0y2U7IEJFo"
      },
      "source": [
        "assert len(trained_base_models) == N_BASE_MODELS, f'Need all {N_BASE_MODELS} base models, got {len(trained_base_models)}: {list(trained_base_models.keys())}'\n",
        "\n",
        "def generate_meta_inputs(base_models, X, y):\n",
        "    n = len(X)\n",
        "    all_preds = {name: np.zeros(n) for name in BASE_MODEL_NAMES}\n",
        "    for name in BASE_MODEL_NAMES:\n",
        "        model = base_models[name]\n",
        "        if name == 'lightgbm':\n",
        "            # LightGBM uses flattened features, not sequence tensors\n",
        "            lgb_feats = _prepare_lgb_features(X)\n",
        "            probs = model.predict(lgb_feats)\n",
        "            all_preds[name] = np.clip((probs - 0.5) * 2.0, -1.0, 1.0)\n",
        "            acc = directional_accuracy(all_preds[name], y)\n",
        "            print(f'  {name}: dir_acc={acc:.3f}')\n",
        "            continue\n",
        "        model_preds = []\n",
        "        for i in range(0, n, 128):\n",
        "            batch = torch.FloatTensor(X[i:i+128]).to(device)\n",
        "            with torch.no_grad():\n",
        "                output = model(batch)\n",
        "                pred = output[0] if isinstance(output, tuple) else output\n",
        "                model_preds.append(torch.tanh(pred.squeeze(-1)).cpu().numpy())\n",
        "        all_preds[name] = np.concatenate(model_preds)\n",
        "        acc = directional_accuracy(all_preds[name], y)\n",
        "        print(f'  {name}: dir_acc={acc:.3f}')\n",
        "    last_features = X[:, -1, :]\n",
        "    pred_matrix = np.column_stack([all_preds[name] for name in BASE_MODEL_NAMES])\n",
        "    meta_X = np.concatenate([last_features, pred_matrix], axis=1).astype(np.float32)\n",
        "    print(f'Meta-inputs: {meta_X.shape} (features={INPUT_DIM} + {N_BASE_MODELS} model preds)')\n",
        "    return meta_X, y\n",
        "\n",
        "print('Generating meta-inputs from base model predictions...')\n",
        "print('\\nTraining set:')\n",
        "meta_X_train, meta_y_train = generate_meta_inputs(trained_base_models, X_train, y_train)\n",
        "print('\\nValidation set:')\n",
        "meta_X_val, meta_y_val = generate_meta_inputs(trained_base_models, X_val, y_val)\n",
        "print('\\nTest set:')\n",
        "meta_X_test, meta_y_test = generate_meta_inputs(trained_base_models, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDMGhg9REJFo"
      },
      "source": [
        "print(f'\\n{\"=\"*60}')\n",
        "print('Training Meta-Ensemble')\n",
        "print(f'{\"=\"*60}')\n",
        "\n",
        "META_EPOCHS = 80\n",
        "META_LR = 3e-4\n",
        "META_PATIENCE = 12\n",
        "\n",
        "meta_model = TrainedMetaEnsemble(input_dim=INPUT_DIM).to(device)\n",
        "optimizer = torch.optim.AdamW(meta_model.parameters(), lr=META_LR, weight_decay=1e-4)\n",
        "\n",
        "# Warmup + cosine decay\n",
        "def meta_lr_lambda(epoch):\n",
        "    if epoch < 3:\n",
        "        return (epoch + 1) / 3\n",
        "    progress = (epoch - 3) / max(META_EPOCHS - 3, 1)\n",
        "    return 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, meta_lr_lambda)\n",
        "criterion = DirectionalLoss(logit_scale=20.0, margin=0.10)\n",
        "\n",
        "meta_train_ds = TensorDataset(torch.FloatTensor(meta_X_train), torch.FloatTensor(meta_y_train))\n",
        "meta_val_ds = TensorDataset(torch.FloatTensor(meta_X_val), torch.FloatTensor(meta_y_val))\n",
        "meta_train_loader = DataLoader(meta_train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "meta_val_loader = DataLoader(meta_val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_state = None\n",
        "patience_counter = 0\n",
        "t0 = time.time()\n",
        "\n",
        "for epoch in range(META_EPOCHS):\n",
        "    meta_model.train()\n",
        "    total_loss, n_b = 0.0, 0\n",
        "    for X_b, y_b in meta_train_loader:\n",
        "        X_b, y_b = X_b.to(device), y_b.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred, _ = meta_model(X_b)\n",
        "        loss = criterion(pred.squeeze(-1), y_b)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(meta_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item(); n_b += 1\n",
        "    train_loss = total_loss / max(n_b, 1)\n",
        "\n",
        "    meta_model.eval()\n",
        "    v_loss, v_n = 0.0, 0\n",
        "    v_preds, v_tgts = [], []\n",
        "    with torch.no_grad():\n",
        "        for X_b, y_b in meta_val_loader:\n",
        "            X_b, y_b = X_b.to(device), y_b.to(device)\n",
        "            pred, _ = meta_model(X_b)\n",
        "            v_loss += criterion(pred.squeeze(-1), y_b).item(); v_n += 1\n",
        "            v_preds.append(pred.squeeze(-1).cpu().numpy())\n",
        "            v_tgts.append(y_b.cpu().numpy())\n",
        "    val_loss = v_loss / max(v_n, 1)\n",
        "    val_acc = directional_accuracy(np.concatenate(v_preds), np.concatenate(v_tgts))\n",
        "    pred_std = float(np.std(np.concatenate(v_preds)))\n",
        "    scheduler.step()\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_state = {k: v.cpu().clone() for k, v in meta_model.state_dict().items()}\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if (epoch+1) % 5 == 0 or epoch == 0 or patience_counter == 0:\n",
        "        print(f'  Epoch {epoch+1:3d}/{META_EPOCHS}: train={train_loss:.4f}, val={val_loss:.4f}, '\n",
        "              f'acc={val_acc:.3f}, pred_std={pred_std:.4f}')\n",
        "\n",
        "    if patience_counter >= META_PATIENCE:\n",
        "        print(f'  Early stopping at epoch {epoch+1}')\n",
        "        break\n",
        "\n",
        "meta_model.load_state_dict(best_state)\n",
        "meta_model.eval()\n",
        "\n",
        "# Test\n",
        "meta_test_ds = TensorDataset(torch.FloatTensor(meta_X_test), torch.FloatTensor(meta_y_test))\n",
        "meta_test_loader = DataLoader(meta_test_ds, batch_size=128, shuffle=False)\n",
        "t_preds, t_tgts = [], []\n",
        "with torch.no_grad():\n",
        "    for X_b, y_b in meta_test_loader:\n",
        "        pred, _ = meta_model(X_b.to(device))\n",
        "        t_preds.append(pred.squeeze(-1).cpu().numpy())\n",
        "        t_tgts.append(y_b.numpy())\n",
        "test_acc = directional_accuracy(np.concatenate(t_preds), np.concatenate(t_tgts))\n",
        "simple_avg = meta_X_test[:, INPUT_DIM:].mean(axis=1)\n",
        "simple_acc = directional_accuracy(simple_avg, meta_y_test)\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "print(f'\\n  Test dir_acc: {test_acc:.3f}')\n",
        "print(f'  Simple average baseline: {simple_acc:.3f}')\n",
        "print(f'  Ensemble lift: {(test_acc - simple_acc)*100:+.1f} pp')\n",
        "print(f'  Time: {elapsed/60:.1f} min')\n",
        "\n",
        "torch.save(meta_model.state_dict(), 'models/trained/best_meta_ensemble_model.pth')\n",
        "print('  Saved: models/trained/best_meta_ensemble_model.pth')\n",
        "\n",
        "results['meta_ensemble'] = {\n",
        "    'val_loss': best_val_loss, 'val_acc': val_acc,\n",
        "    'test_acc': test_acc, 'simple_avg_acc': simple_acc,\n",
        "    'epochs': epoch + 1, 'time_min': elapsed / 60,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjX4Kb75EJFo"
      },
      "source": [
        "## 8. Train VAE (Phase 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSN3pYv_EJFo"
      },
      "source": [
        "print(f'\\n{\"=\"*60}')\n",
        "print('Training VAE Anomaly Detector')\n",
        "print(f'{\"=\"*60}')\n",
        "\n",
        "vae_samples = X_train[:, -1, :]\n",
        "print(f'VAE training samples: {vae_samples.shape}')\n",
        "\n",
        "VAE_EPOCHS = 100\n",
        "VAE_LR = 1e-3\n",
        "VAE_PATIENCE = 20\n",
        "\n",
        "vae_model = VariationalAutoEncoder(input_dim=INPUT_DIM, latent_dim=32).to(device)\n",
        "vae_optimizer = torch.optim.Adam(vae_model.parameters(), lr=VAE_LR)\n",
        "\n",
        "n_vae = len(vae_samples)\n",
        "n_vae_train = int(n_vae * 0.85)\n",
        "vae_train_ds = TensorDataset(torch.FloatTensor(vae_samples[:n_vae_train]))\n",
        "vae_val_ds = TensorDataset(torch.FloatTensor(vae_samples[n_vae_train:]))\n",
        "vae_train_loader = DataLoader(vae_train_ds, batch_size=64, shuffle=True)\n",
        "vae_val_loader = DataLoader(vae_val_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "def vae_loss_fn(recon, x, mu, logvar):\n",
        "    recon_loss = F.mse_loss(recon, x, reduction='sum') / x.size(0)\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
        "    return recon_loss + kl_loss\n",
        "\n",
        "best_vae_loss = float('inf')\n",
        "best_vae_state = None\n",
        "vae_patience = 0\n",
        "t0 = time.time()\n",
        "\n",
        "for epoch in range(VAE_EPOCHS):\n",
        "    vae_model.train()\n",
        "    total_loss, n_b = 0.0, 0\n",
        "    for (batch,) in vae_train_loader:\n",
        "        batch = batch.to(device)\n",
        "        vae_optimizer.zero_grad()\n",
        "        recon, mu, logvar = vae_model(batch)\n",
        "        loss = vae_loss_fn(recon, batch, mu, logvar)\n",
        "        loss.backward()\n",
        "        vae_optimizer.step()\n",
        "        total_loss += loss.item(); n_b += 1\n",
        "    train_loss = total_loss / max(n_b, 1)\n",
        "\n",
        "    vae_model.eval()\n",
        "    v_loss, v_n = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for (batch,) in vae_val_loader:\n",
        "            batch = batch.to(device)\n",
        "            recon, mu, logvar = vae_model(batch)\n",
        "            v_loss += vae_loss_fn(recon, batch, mu, logvar).item(); v_n += 1\n",
        "    val_loss = v_loss / max(v_n, 1)\n",
        "\n",
        "    if val_loss < best_vae_loss:\n",
        "        best_vae_loss = val_loss\n",
        "        best_vae_state = {k: v.cpu().clone() for k, v in vae_model.state_dict().items()}\n",
        "        vae_patience = 0\n",
        "    else:\n",
        "        vae_patience += 1\n",
        "\n",
        "    if (epoch+1) % 10 == 0 or epoch == 0 or vae_patience == 0:\n",
        "        print(f'  Epoch {epoch+1:3d}/{VAE_EPOCHS}: train={train_loss:.4f}, val={val_loss:.4f}')\n",
        "\n",
        "    if vae_patience >= VAE_PATIENCE:\n",
        "        print(f'  Early stopping at epoch {epoch+1}')\n",
        "        break\n",
        "\n",
        "vae_model.load_state_dict(best_vae_state)\n",
        "torch.save(vae_model.state_dict(), 'models/trained/vae_anomaly_detector.pth')\n",
        "\n",
        "vae_model.eval()\n",
        "errors = []\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(vae_samples), 128):\n",
        "        batch = torch.FloatTensor(vae_samples[i:i+128]).to(device)\n",
        "        recon, _, _ = vae_model(batch)\n",
        "        err = ((recon - batch) ** 2).mean(dim=1).cpu().numpy()\n",
        "        errors.extend(err)\n",
        "errors = np.array(errors)\n",
        "elapsed = time.time() - t0\n",
        "\n",
        "print(f'\\n  Reconstruction error: p50={np.percentile(errors, 50):.4f}, '\n",
        "      f'p95={np.percentile(errors, 95):.4f}, p99={np.percentile(errors, 99):.4f}')\n",
        "print(f'  Time: {elapsed/60:.1f} min')\n",
        "print(f'  Saved: models/trained/vae_anomaly_detector.pth')\n",
        "\n",
        "results['vae'] = {\n",
        "    'val_loss': best_vae_loss,\n",
        "    'p50': float(np.percentile(errors, 50)),\n",
        "    'p95': float(np.percentile(errors, 95)),\n",
        "    'p99': float(np.percentile(errors, 99)),\n",
        "    'epochs': epoch + 1,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRMCSy3nEJFo"
      },
      "source": [
        "## 9. Summary & Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XempmTp-EJFo"
      },
      "source": [
        "print(f'\\n{\"=\"*60}')\n",
        "print('TRAINING SUMMARY')\n",
        "print(f'{\"=\"*60}')\n",
        "print(f'{\"Model\":<25} {\"Val Loss\":>10} {\"Val Acc\":>10} {\"Test Acc\":>10} {\"PredStd\":>10} {\"Epochs\":>8} {\"Recov\":>6}')\n",
        "print('-' * 81)\n",
        "for name in [n for n, _ in base_configs] + ['lightgbm', 'meta_ensemble', 'vae']:\n",
        "    r = results.get(name, {})\n",
        "    vl = f\"{r.get('val_loss', 0):.4f}\" if 'val_loss' in r else 'N/A'\n",
        "    va = f\"{r.get('val_acc', 0):.3f}\" if 'val_acc' in r else 'N/A'\n",
        "    ta = f\"{r.get('test_acc', 0):.3f}\" if 'test_acc' in r else 'N/A'\n",
        "    ps = f\"{r.get('test_pred_std', 0):.4f}\" if 'test_pred_std' in r else 'N/A'\n",
        "    ep = str(r.get('epochs', 'N/A'))\n",
        "    rec = str(r.get('collapse_recoveries', '-'))\n",
        "    print(f'{name:<25} {vl:>10} {va:>10} {ta:>10} {ps:>10} {ep:>8} {rec:>6}')\n",
        "\n",
        "# Check for collapsed models\n",
        "collapsed = [n for n, r in results.items()\n",
        "             if r.get('test_pred_std', 1.0) < 0.001 and 'test_pred_std' in r]\n",
        "if collapsed:\n",
        "    print(f'\\n*** WARNING: {len(collapsed)} model(s) collapsed to constant predictions: {collapsed}')\n",
        "    print('*** These weights will produce ~0 predictions in production.')\n",
        "else:\n",
        "    print(f'\\nAll models producing varied predictions (no collapse detected)')\n",
        "\n",
        "has_deriv = bool(derivatives_dfs) or fear_greed_df is not None\n",
        "print(f'\\nInput dim: {INPUT_DIM} (46 single-pair + 15 cross-asset + 7 derivatives, padded to 98)')\n",
        "print(f'Derivatives data: {\"yes\" if has_deriv else \"no (7 features zero-padded)\"}')\n",
        "print(f'Label: {LABEL_HORIZON}-bar ({LABEL_HORIZON * 5}-min) forward return, scale={LABEL_SCALE}')\n",
        "print(f'Loss: v6 (BCE*20 + 10*sep_margin(0.10) + 5*mag_floor)')\n",
        "print(f'QT optimizer: wd=0, attention LR=0.1x (v7 — prevents attention weight collapse)')\n",
        "print(f'Other models: wd=1e-4, LR={LR:.1e}')\n",
        "print(f'Training data: {sum(len(df) for df in pair_dfs.values()):,} bars across {len(pair_dfs)} pairs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9MFXGUzEJFp"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# Section 9b: Confidence-Stratified Accuracy Analysis\n",
        "# ================================================================\n",
        "# Does model confidence actually correlate with accuracy?\n",
        "# If high-confidence predictions aren't more accurate, the bot's\n",
        "# confidence filtering is useless. This analysis validates the\n",
        "# signal before deploying.\n",
        "# ================================================================\n",
        "\n",
        "print(f'\\n{\"=\"*60}')\n",
        "print('CONFIDENCE-STRATIFIED ACCURACY ANALYSIS')\n",
        "print(f'{\"=\"*60}')\n",
        "\n",
        "def confidence_stratified_analysis(model_name, predictions, targets, n_quintiles=5):\n",
        "    predictions = np.array(predictions).flatten()\n",
        "    targets = np.array(targets).flatten()\n",
        "    confidence = np.abs(predictions)\n",
        "    correct = ((predictions > 0) & (targets > 0)) | ((predictions < 0) & (targets < 0))\n",
        "    try:\n",
        "        quintile_edges = np.percentile(confidence, np.linspace(0, 100, n_quintiles + 1))\n",
        "        quintile_edges = np.unique(quintile_edges)\n",
        "        if len(quintile_edges) < 3:\n",
        "            print(f'  {model_name}: insufficient confidence spread for quintile analysis')\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f'  {model_name}: quintile computation failed: {e}')\n",
        "        return None\n",
        "    rows = []\n",
        "    for i in range(len(quintile_edges) - 1):\n",
        "        lo, hi = quintile_edges[i], quintile_edges[i + 1]\n",
        "        if i == len(quintile_edges) - 2:\n",
        "            mask = (confidence >= lo) & (confidence <= hi)\n",
        "        else:\n",
        "            mask = (confidence >= lo) & (confidence < hi)\n",
        "        n = mask.sum()\n",
        "        if n < 10:\n",
        "            continue\n",
        "        acc = correct[mask].mean()\n",
        "        avg_conf = confidence[mask].mean()\n",
        "        rows.append({\n",
        "            'quintile': i + 1,\n",
        "            'conf_range': f'{lo:.3f}-{hi:.3f}',\n",
        "            'n_samples': int(n),\n",
        "            'accuracy': float(acc),\n",
        "            'avg_confidence': float(avg_conf),\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "# Collect predictions from all models on test set\n",
        "print('\\nCollecting test-set predictions from all models...\\n')\n",
        "\n",
        "model_test_preds = {}\n",
        "\n",
        "# DL models\n",
        "for name in ['quantum_transformer', 'bidirectional_lstm', 'dilated_cnn', 'cnn', 'gru']:\n",
        "    if name not in trained_base_models:\n",
        "        continue\n",
        "    model = trained_base_models[name]\n",
        "    model.eval()\n",
        "    preds_list = []\n",
        "    for i in range(0, len(X_test), 128):\n",
        "        batch = torch.FloatTensor(X_test[i:i+128]).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(batch)\n",
        "            pred = output[0] if isinstance(output, tuple) else output\n",
        "            preds_list.append(torch.tanh(pred.squeeze(-1)).cpu().numpy())\n",
        "    model_test_preds[name] = np.concatenate(preds_list)\n",
        "\n",
        "# LightGBM\n",
        "if 'lightgbm' in trained_base_models:\n",
        "    lgb_feats_test = _prepare_lgb_features(X_test)\n",
        "    lgb_probs = trained_base_models['lightgbm'].predict(lgb_feats_test)\n",
        "    model_test_preds['lightgbm'] = np.clip((lgb_probs - 0.5) * 2.0, -1.0, 1.0)\n",
        "\n",
        "# Meta-ensemble\n",
        "try:\n",
        "    meta_preds_list = []\n",
        "    meta_test_loader_q = DataLoader(\n",
        "        TensorDataset(torch.FloatTensor(meta_X_test), torch.FloatTensor(meta_y_test)),\n",
        "        batch_size=128, shuffle=False)\n",
        "    meta_model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X_b, y_b in meta_test_loader_q:\n",
        "            pred, conf = meta_model(X_b.to(device))\n",
        "            meta_preds_list.append(torch.tanh(pred.squeeze(-1)).cpu().numpy())\n",
        "    model_test_preds['meta_ensemble'] = np.concatenate(meta_preds_list)\n",
        "except Exception as e:\n",
        "    print(f'  Meta-ensemble predictions failed: {e}')\n",
        "\n",
        "# Run analysis for each model\n",
        "print(f'{\"Model\":<22s} {\"Quintile\":>8s} {\"Conf Range\":>14s} {\"N\":>6s} {\"Accuracy\":>10s} {\"AvgConf\":>8s}')\n",
        "print('-' * 72)\n",
        "\n",
        "all_strat_results = {}\n",
        "for name, preds in model_test_preds.items():\n",
        "    rows = confidence_stratified_analysis(name, preds, y_test)\n",
        "    if rows is None:\n",
        "        continue\n",
        "    all_strat_results[name] = rows\n",
        "    for r in rows:\n",
        "        print(f'{name:<22s} {r[\"quintile\"]:>8d} {r[\"conf_range\"]:>14s} {r[\"n_samples\"]:>6d} '\n",
        "              f'{r[\"accuracy\"]:>10.3f} {r[\"avg_confidence\"]:>8.3f}')\n",
        "    # Check monotonicity\n",
        "    accs = [r['accuracy'] for r in rows]\n",
        "    if len(accs) >= 3:\n",
        "        increasing = sum(1 for i in range(1, len(accs)) if accs[i] > accs[i-1])\n",
        "        total = len(accs) - 1\n",
        "        mono_score = increasing / total if total > 0 else 0\n",
        "        verdict = 'GOOD' if mono_score >= 0.6 else 'WEAK' if mono_score >= 0.3 else 'BAD'\n",
        "        print(f'  -> Monotonicity: {increasing}/{total} ({mono_score:.0%}) -- {verdict}')\n",
        "    print()\n",
        "\n",
        "# Overall summary\n",
        "print(f'\\n{\"=\"*60}')\n",
        "print('CONFIDENCE FILTERING VERDICT')\n",
        "print(f'{\"=\"*60}')\n",
        "good_models, weak_models, bad_models = [], [], []\n",
        "for name, rows in all_strat_results.items():\n",
        "    accs = [r['accuracy'] for r in rows]\n",
        "    if len(accs) < 3:\n",
        "        continue\n",
        "    spread = accs[-1] - accs[0]\n",
        "    if spread > 0.03:\n",
        "        good_models.append((name, spread))\n",
        "    elif spread > 0.01:\n",
        "        weak_models.append((name, spread))\n",
        "    else:\n",
        "        bad_models.append((name, spread))\n",
        "\n",
        "if good_models:\n",
        "    print(f'\\nModels where confidence filtering HELPS (top-bottom spread > 3pp):')\n",
        "    for name, spread in good_models:\n",
        "        print(f'  {name}: {spread*100:+.1f}pp')\n",
        "if weak_models:\n",
        "    print(f'\\nModels with WEAK confidence signal (1-3pp spread):')\n",
        "    for name, spread in weak_models:\n",
        "        print(f'  {name}: {spread*100:+.1f}pp')\n",
        "if bad_models:\n",
        "    print(f'\\nModels where confidence filtering is USELESS (<1pp spread):')\n",
        "    for name, spread in bad_models:\n",
        "        print(f'  {name}: {spread*100:+.1f}pp')\n",
        "\n",
        "print(f'\\nRecommendation: Set confidence threshold based on the meta-ensemble\\'s')\n",
        "print(f'top quintile accuracy. If meta-ensemble shows good monotonicity,')\n",
        "print(f'filter predictions below the 40th percentile confidence.')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXUC3cKeEJFp"
      },
      "source": [
        "# Zip and save to Google Drive + browser download\n",
        "zip_path = 'trained_models_98dim.zip'\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "    for f in os.listdir('models/trained'):\n",
        "        if f.endswith(('.pth', '.pkl', '.json')):\n",
        "            full = os.path.join('models/trained', f)\n",
        "            zf.write(full, f)\n",
        "            size_mb = os.path.getsize(full) / 1e6\n",
        "            print(f'  Added: {f} ({size_mb:.1f} MB)')\n",
        "\n",
        "zip_size = os.path.getsize(zip_path) / 1e6\n",
        "print(f'\\nZip: {zip_path} ({zip_size:.1f} MB)')\n",
        "\n",
        "# Save to Google Drive\n",
        "drive_out = f'/content/drive/My Drive/{DRIVE_FOLDER}/{zip_path}'\n",
        "shutil.copy2(zip_path, drive_out)\n",
        "print(f'Saved to Drive: {drive_out}')\n",
        "\n",
        "# Also try browser download\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(zip_path)\n",
        "    print('Browser download started')\n",
        "except Exception as e:\n",
        "    print(f'Browser download failed ({e})')\n",
        "    print(f'Download from Google Drive instead: {drive_out}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4o6KNz_EJFp"
      },
      "source": [
        "## 10. Post-Download\n",
        "\n",
        "After downloading `trained_models_98dim.zip` from Google Drive, unzip and copy:\n",
        "\n",
        "```bash\n",
        "cd ~/Downloads\n",
        "unzip trained_models_98dim.zip -d trained_models_98dim\n",
        "cp trained_models_98dim/*.pth ~/Downloads/bitcoin-trading-bot-renaissance/models/trained/\n",
        "```\n",
        "\n",
        "Then restart the bot — it will auto-detect the new 98-dim weights via `_detect_input_dim()`."
      ]
    }
  ]
}