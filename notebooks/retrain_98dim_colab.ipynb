{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Renaissance Bot \u2014 Retrain All Models (98-dim Cross-Asset Features)\n\nThis notebook retrains all 7 ML models with the new 98-dimension cross-asset feature pipeline:\n- **46 scale-invariant single-pair features**: returns, ratios, z-scores (no raw prices)\n- **15 cross-asset features**: lead signals, correlations, spreads, market-wide\n- **7 derivatives features**: funding rate, OI, long/short ratio, taker ratio, Fear & Greed\n- **Padded to 98**\n\n## Training order\n1. Upload historical CSVs via Google Drive (6 pairs x 5+ years of 5-min bars)\n2. (Optional) Upload derivatives CSVs for 7 additional features\n3. Phase 1: Train 5 base models (QT, BiLSTM, DilatedCNN, CNN, GRU)\n4. Phase 2: Train Meta-Ensemble (stacking layer over 5 base models)\n5. Phase 3: Train VAE anomaly detector\n6. Download trained `.pth` weight files\n\n## Key training fixes (v7)\n- **Soft labels**: 6-bar (30-min) forward return x 100, clipped to [-1, 1]\n- **v6 loss**: BCE(pred x 20) + 10 x separation_margin(0.10) + 5 x magnitude_floor\n- **v7 QT optimizer**: weight_decay=0 (not 1e-5), differential LR (attention 0.1x), collapse recovery\n- **Other models**: weight_decay=1e-4, LR=3e-4\n- **LR warmup**: 3-epoch linear warmup + cosine decay\n\n**Why v7?** v6 fixed the loss but QT still collapsed at epoch 5 on full data.\nRoot cause: weight_decay=1e-5 per step \u00d7 10,600 batches/epoch = **10.6% total\nweight shrinkage per epoch** \u2014 13.5x more than on local 50K-sample test.\nThis systematically shrinks attention Q/K/V weights \u2192 softmax goes uniform \u2192\nmean pooling produces constant output \u2192 pred=0. Fix: wd=0 for QT + 0.1x LR\non attention params + auto-recovery from collapse.\n\n**Runtime**: Select **GPU -> T4** (Runtime -> Change runtime type -> T4 GPU)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch numpy pandas scikit-learn"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "import logging\n",
    "import zipfile\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "logger = logging.getLogger('retrain')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload Training Data via Google Drive\n",
    "\n",
    "1. Upload the 6 CSV files to a folder in your Google Drive (e.g., `My Drive/training_data/`)\n",
    "2. Run cell 1a below \u2014 it mounts Drive and copies files to local storage\n",
    "3. Run cell 1b \u2014 it **automatically fetches** derivatives data from Binance Futures\n",
    "   (works from Colab \u2014 blocked in US locally). Saves to Drive for reuse.\n",
    "\n",
    "**Files needed** (from `data/training/`):\n",
    "- `BTC-USD_5m_historical.csv` (~49 MB)\n",
    "- `ETH-USD_5m_historical.csv` (~45 MB)\n",
    "- `SOL-USD_5m_historical.csv` (~27 MB)\n",
    "- `DOGE-USD_5m_historical.csv` (~38 MB)\n",
    "- `AVAX-USD_5m_historical.csv` (~25 MB)\n",
    "- `LINK-USD_5m_historical.csv` (~34 MB)\n",
    "\n",
    "**Derivatives** (auto-fetched by cell 1b, or reused from Drive on subsequent runs):\n",
    "- `BTC-USD_derivatives.csv`, `ETH-USD_derivatives.csv`, etc.\n",
    "- `fear_greed_history.csv`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Mount Google Drive and copy CSV files to local storage\n# Change DRIVE_FOLDER if you put them somewhere else.\n\nDRIVE_FOLDER = 'training_data'  # folder name inside My Drive\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nos.makedirs('data/training', exist_ok=True)\nos.makedirs('models/trained', exist_ok=True)\n\n# Search for CSV files in the specified folder\ndrive_path = f'/content/drive/My Drive/{DRIVE_FOLDER}'\nif not os.path.exists(drive_path):\n    print(f'Folder \"{DRIVE_FOLDER}\" not found in My Drive.')\n    print('Searching for *_5m_historical.csv files in entire Drive...')\n    found = glob.glob('/content/drive/My Drive/**/*_5m_historical.csv', recursive=True)\n    if found:\n        drive_path = os.path.dirname(found[0])\n        print(f'Found files in: {drive_path}')\n    else:\n        raise FileNotFoundError(\n            'No *_5m_historical.csv files found in Google Drive.\\n'\n            'Please upload them to a folder in Drive first.'\n        )\n\n# Copy CSV files from Drive to local storage (much faster I/O)\ncsv_files = glob.glob(os.path.join(drive_path, '*_5m_historical.csv'))\nprint(f'\\nFound {len(csv_files)} CSV files in {drive_path}:')\nfor src in sorted(csv_files):\n    fname = os.path.basename(src)\n    dst = os.path.join('data/training', fname)\n    size_mb = os.path.getsize(src) / 1e6\n    print(f'  Copying {fname} ({size_mb:.1f} MB)...', end=' ')\n    shutil.copy2(src, dst)\n    print('done')\n\nprint(f'\\nAll files copied to data/training/')\n\n# Copy derivatives data if available (optional \u2014 enables 7 extra features)\nderiv_drive_path = os.path.join(drive_path, 'derivatives')\nif os.path.exists(deriv_drive_path):\n    os.makedirs('data/training/derivatives', exist_ok=True)\n    deriv_files = [f for f in os.listdir(deriv_drive_path) if f.endswith('.csv')]\n    for fname in sorted(deriv_files):\n        src = os.path.join(deriv_drive_path, fname)\n        shutil.copy2(src, os.path.join('data/training/derivatives', fname))\n    print(f'\\nCopied {len(deriv_files)} derivatives files to data/training/derivatives/')\nelse:\n    print(f'\\nNo derivatives/ subfolder found (optional \u2014 7 features will be zero-padded)')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Fetch Derivatives Data (runs on Colab)\n",
    "\n",
    "Binance Futures API is **geo-restricted in the US** but works from Colab (Google Cloud).\n",
    "This cell fetches funding rate, open interest, long/short ratio, taker volume,\n",
    "and Fear & Greed history \u2014 then saves CSVs to `data/training/derivatives/`.\n",
    "\n",
    "**Skip this cell** if you already uploaded derivatives CSVs via Google Drive.\n",
    "Takes ~5-10 minutes for 6 pairs \u00d7 2 years of 5-min data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import requests\n",
    "# \u2500\u2500 Fetch derivatives data from Binance Futures (works on Colab, blocked in US) \u2500\u2500\n",
    "# Skip this cell if you already uploaded derivatives CSVs to Google Drive.\n",
    "\n",
    "import time as _time\n",
    "\n",
    "DERIV_PAIRS = ['BTC-USD', 'ETH-USD', 'SOL-USD', 'DOGE-USD', 'AVAX-USD', 'LINK-USD']\n",
    "DERIV_DAYS = 730  # 2 years of history\n",
    "DERIV_PERIOD = '5m'\n",
    "DERIV_DIR = 'data/training/derivatives'\n",
    "os.makedirs(DERIV_DIR, exist_ok=True)\n",
    "\n",
    "BINANCE_FAPI = 'https://fapi.binance.com'\n",
    "BINANCE_FUTURES = 'https://fapi.binance.com/futures/data'\n",
    "FNG_API = 'https://api.alternative.me/fng/'\n",
    "PAIR_MAP = {'BTC-USD':'BTCUSDT','ETH-USD':'ETHUSDT','SOL-USD':'SOLUSDT',\n",
    "            'DOGE-USD':'DOGEUSDT','AVAX-USD':'AVAXUSDT','LINK-USD':'LINKUSDT'}\n",
    "REQ_DELAY = 0.2\n",
    "\n",
    "def _paginate(url, symbol, period, start_ms, end_ms, val_key, label):\n",
    "    rows = []\n",
    "    cur = start_ms\n",
    "    while cur < end_ms:\n",
    "        resp = requests.get(url, params={\n",
    "            'symbol': symbol, 'period': period,\n",
    "            'startTime': cur, 'endTime': end_ms, 'limit': 500,\n",
    "        }, timeout=15)\n",
    "        if resp.status_code != 200:\n",
    "            print(f'    {label} HTTP {resp.status_code}: {resp.text[:100]}')\n",
    "            break\n",
    "        data = resp.json()\n",
    "        if not data:\n",
    "            break\n",
    "        for e in data:\n",
    "            rows.append({'timestamp': int(e['timestamp'])//1000, 'value': float(e[val_key])})\n",
    "        cur = int(data[-1]['timestamp']) + 1\n",
    "        _time.sleep(REQ_DELAY)\n",
    "    return pd.DataFrame(rows).drop_duplicates('timestamp').sort_values('timestamp').reset_index(drop=True) if rows else pd.DataFrame()\n",
    "\n",
    "def _fetch_funding(symbol, start_ms, end_ms):\n",
    "    rows = []\n",
    "    cur = start_ms\n",
    "    while cur < end_ms:\n",
    "        resp = requests.get(f'{BINANCE_FAPI}/fapi/v1/fundingRate',\n",
    "                            params={'symbol':symbol,'startTime':cur,'endTime':end_ms,'limit':1000}, timeout=15)\n",
    "        if resp.status_code != 200:\n",
    "            print(f'    Funding HTTP {resp.status_code}')\n",
    "            break\n",
    "        data = resp.json()\n",
    "        if not data:\n",
    "            break\n",
    "        for e in data:\n",
    "            rows.append({'timestamp': int(e['fundingTime'])//1000, 'value': float(e['fundingRate'])})\n",
    "        cur = int(data[-1]['fundingTime']) + 1\n",
    "        _time.sleep(REQ_DELAY)\n",
    "    return pd.DataFrame(rows).drop_duplicates('timestamp').sort_values('timestamp').reset_index(drop=True) if rows else pd.DataFrame()\n",
    "\n",
    "def _fetch_taker(symbol, period, start_ms, end_ms):\n",
    "    rows = []\n",
    "    cur = start_ms\n",
    "    while cur < end_ms:\n",
    "        resp = requests.get(f'{BINANCE_FUTURES}/takeBuySellVol',\n",
    "                            params={'symbol':symbol,'period':period,'startTime':cur,'endTime':end_ms,'limit':500}, timeout=15)\n",
    "        if resp.status_code != 200:\n",
    "            print(f'    Taker HTTP {resp.status_code}')\n",
    "            break\n",
    "        data = resp.json()\n",
    "        if not data:\n",
    "            break\n",
    "        for e in data:\n",
    "            rows.append({'timestamp': int(e['timestamp'])//1000,\n",
    "                         'taker_buy_vol': float(e['buyVol']), 'taker_sell_vol': float(e['sellVol'])})\n",
    "        cur = int(data[-1]['timestamp']) + 1\n",
    "        _time.sleep(REQ_DELAY)\n",
    "    return pd.DataFrame(rows).drop_duplicates('timestamp').sort_values('timestamp').reset_index(drop=True) if rows else pd.DataFrame()\n",
    "\n",
    "end_ms = int(datetime.now(timezone.utc).timestamp() * 1000)\n",
    "start_ms = end_ms - (DERIV_DAYS * 86400 * 1000)\n",
    "\n",
    "# \u2500\u2500 Quick connectivity test \u2500\u2500\n",
    "print('Testing Binance Futures API connectivity...')\n",
    "test_resp = requests.get(f'{BINANCE_FAPI}/fapi/v1/fundingRate',\n",
    "                         params={'symbol':'BTCUSDT','limit':1}, timeout=10)\n",
    "if test_resp.status_code != 200:\n",
    "    print(f'ERROR: Binance Futures returned {test_resp.status_code}.')\n",
    "    print('This API is geo-restricted. If running locally in the US, use Colab or a VPN.')\n",
    "    print('Skipping derivatives fetch \u2014 features will be zero-padded.')\n",
    "else:\n",
    "    print(f'OK \u2014 connected to Binance Futures\\n')\n",
    "\n",
    "    for pair in DERIV_PAIRS:\n",
    "        symbol = PAIR_MAP[pair]\n",
    "        print(f'\\n{\"=\"*50}')\n",
    "        print(f'{pair} ({symbol}) \u2014 {DERIV_DAYS}d, period={DERIV_PERIOD}')\n",
    "        print(f'{\"=\"*50}')\n",
    "\n",
    "        funding_df = _fetch_funding(symbol, start_ms, end_ms)\n",
    "        print(f'  Funding rate: {len(funding_df)} entries')\n",
    "\n",
    "        oi_df = _paginate(f'{BINANCE_FUTURES}/openInterestHist',\n",
    "                          symbol, DERIV_PERIOD, start_ms, end_ms, 'sumOpenInterest', 'OI')\n",
    "        print(f'  Open interest: {len(oi_df)} entries')\n",
    "\n",
    "        ls_df = _paginate(f'{BINANCE_FUTURES}/globalLongShortAccountRatio',\n",
    "                          symbol, DERIV_PERIOD, start_ms, end_ms, 'longShortRatio', 'LS')\n",
    "        print(f'  Long/Short ratio: {len(ls_df)} entries')\n",
    "\n",
    "        taker_df = _fetch_taker(symbol, DERIV_PERIOD, start_ms, end_ms)\n",
    "        print(f'  Taker volume: {len(taker_df)} entries')\n",
    "\n",
    "        # Merge\n",
    "        dfs = []\n",
    "        if not funding_df.empty:\n",
    "            dfs.append(funding_df.rename(columns={'value':'funding_rate'}))\n",
    "        if not oi_df.empty:\n",
    "            dfs.append(oi_df.rename(columns={'value':'open_interest'}))\n",
    "        if not ls_df.empty:\n",
    "            dfs.append(ls_df.rename(columns={'value':'long_short_ratio'}))\n",
    "        if not taker_df.empty:\n",
    "            dfs.append(taker_df)\n",
    "\n",
    "        if dfs:\n",
    "            result = dfs[0]\n",
    "            for df in dfs[1:]:\n",
    "                result = pd.merge(result, df, on='timestamp', how='outer')\n",
    "            result = result.sort_values('timestamp').reset_index(drop=True)\n",
    "            if 'funding_rate' in result.columns:\n",
    "                result['funding_rate'] = result['funding_rate'].ffill()\n",
    "\n",
    "            out_path = os.path.join(DERIV_DIR, f'{pair}_derivatives.csv')\n",
    "            result.to_csv(out_path, index=False)\n",
    "            print(f'  -> Saved {out_path} ({len(result):,} rows, {os.path.getsize(out_path)/1e6:.1f} MB)')\n",
    "        else:\n",
    "            print(f'  -> No data for {pair}')\n",
    "\n",
    "    # \u2500\u2500 Fear & Greed \u2500\u2500\n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print('Fear & Greed Index (all history)')\n",
    "    print(f'{\"=\"*50}')\n",
    "    try:\n",
    "        fng_resp = requests.get(FNG_API, params={'format':'json','limit':0}, timeout=30)\n",
    "        fng_data = fng_resp.json().get('data', [])\n",
    "        if fng_data:\n",
    "            fng_rows = [{'timestamp': int(e['timestamp']), 'fear_greed': int(e['value'])} for e in fng_data]\n",
    "            fng_df = pd.DataFrame(fng_rows).sort_values('timestamp').drop_duplicates('timestamp').reset_index(drop=True)\n",
    "            fng_path = os.path.join(DERIV_DIR, 'fear_greed_history.csv')\n",
    "            fng_df.to_csv(fng_path, index=False)\n",
    "            print(f'  Saved {fng_path} ({len(fng_df):,} daily values)')\n",
    "        else:\n",
    "            print('  No Fear & Greed data returned')\n",
    "    except Exception as e:\n",
    "        print(f'  Fear & Greed fetch failed: {e}')\n",
    "\n",
    "    # \u2500\u2500 Summary \u2500\u2500\n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print('DERIVATIVES FETCH SUMMARY')\n",
    "    print(f'{\"=\"*50}')\n",
    "    total_size = 0\n",
    "    for f in sorted(os.listdir(DERIV_DIR)):\n",
    "        if f.endswith('.csv'):\n",
    "            p = os.path.join(DERIV_DIR, f)\n",
    "            sz = os.path.getsize(p)\n",
    "            total_size += sz\n",
    "            rows = len(pd.read_csv(p))\n",
    "            print(f'  {f}: {rows:,} rows ({sz/1e6:.1f} MB)')\n",
    "    print(f'  Total: {total_size/1e6:.1f} MB')\n",
    "    \n",
    "    # Also save to Google Drive for reuse\n",
    "    drive_deriv = f'/content/drive/My Drive/{DRIVE_FOLDER}/derivatives'\n",
    "    os.makedirs(drive_deriv, exist_ok=True)\n",
    "    for f in os.listdir(DERIV_DIR):\n",
    "        if f.endswith('.csv'):\n",
    "            shutil.copy2(os.path.join(DERIV_DIR, f), os.path.join(drive_deriv, f))\n",
    "    print(f'\\nAlso saved to Google Drive: {drive_deriv}/')\n",
    "    print('Next time, cell 1a will auto-copy these from Drive (no re-fetch needed).')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load and validate data\nALL_PAIRS = ['BTC-USD', 'ETH-USD', 'SOL-USD', 'DOGE-USD', 'AVAX-USD', 'LINK-USD']\n\npair_dfs = {}\nfor pair in ALL_PAIRS:\n    csv_path = f'data/training/{pair}_5m_historical.csv'\n    if os.path.exists(csv_path):\n        df = pd.read_csv(csv_path)\n        pair_dfs[pair] = df\n        first_ts = datetime.fromtimestamp(\n            df['timestamp'].iloc[0]/1000 if df['timestamp'].iloc[0] > 1e12 else df['timestamp'].iloc[0],\n            tz=timezone.utc)\n        last_ts = datetime.fromtimestamp(\n            df['timestamp'].iloc[-1]/1000 if df['timestamp'].iloc[-1] > 1e12 else df['timestamp'].iloc[-1],\n            tz=timezone.utc)\n        print(f'{pair}: {len(df):>10,} bars  ({first_ts.strftime(\"%Y-%m-%d\")} -> {last_ts.strftime(\"%Y-%m-%d\")})')\n    else:\n        print(f'{pair}: NOT FOUND')\n\ntotal = sum(len(df) for df in pair_dfs.values())\nprint(f'\\nTotal: {total:,} bars across {len(pair_dfs)} pairs')\nassert len(pair_dfs) >= 2, 'Need at least 2 pairs for cross-asset features'\n\n# Load derivatives data (optional \u2014 7 features: funding_rate_z, oi_change_pct,\n# long_short_ratio, taker_buy_sell_ratio, fear_greed_norm, fear_greed_roc, has_derivatives_data)\nderivatives_dfs = {}\nfear_greed_df = None\nderiv_dir = 'data/training/derivatives'\n\nif os.path.exists(deriv_dir):\n    for fname in sorted(os.listdir(deriv_dir)):\n        if fname.endswith('_derivatives.csv'):\n            pair = fname.replace('_derivatives.csv', '')\n            ddf = pd.read_csv(os.path.join(deriv_dir, fname))\n            if len(ddf) > 0:\n                derivatives_dfs[pair] = ddf\n                print(f'  Derivatives {pair}: {len(ddf):,} rows')\n\n    fng_path = os.path.join(deriv_dir, 'fear_greed_history.csv')\n    if os.path.exists(fng_path):\n        fear_greed_df = pd.read_csv(fng_path)\n        if len(fear_greed_df) > 0:\n            print(f'  Fear & Greed: {len(fear_greed_df):,} daily values')\n        else:\n            fear_greed_df = None\n\n    if derivatives_dfs or fear_greed_df is not None:\n        print(f'\\nDerivatives: {len(derivatives_dfs)} pairs + {\"yes\" if fear_greed_df is not None else \"no\"} Fear & Greed')\n    else:\n        print('\\nDerivatives CSVs empty \u2014 7 derivatives features will be zero-padded')\nelse:\n    print(f'\\nNo derivatives data directory \u2014 7 derivatives features will be zero-padded')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constants & Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Constants\nINPUT_DIM = 98\nN_CROSS_FEATURES = 15\nN_DERIVATIVES_FEATURES = 7\nSEQ_LEN = 30\nN_BASE_MODELS = 5\nBASE_MODEL_NAMES = ['quantum_transformer', 'bidirectional_lstm', 'dilated_cnn', 'cnn', 'gru']\n\n# Label configuration \u2014 predict 30-min forward return, not next-bar direction\nLABEL_HORIZON = 6   # 6 bars \u00d7 5 min = 30 min lookahead\nLABEL_SCALE = 100   # Scaling: 0.5% return \u2192 label 0.5, clipped to [-1, 1]\n\nLEAD_SIGNALS = {\n    'BTC-USD':  {'primary': 'ETH-USD',  'secondary': 'SOL-USD'},\n    'ETH-USD':  {'primary': 'BTC-USD',  'secondary': 'LINK-USD'},\n    'SOL-USD':  {'primary': 'BTC-USD',  'secondary': 'ETH-USD'},\n    'LINK-USD': {'primary': 'ETH-USD',  'secondary': 'BTC-USD'},\n    'AVAX-USD': {'primary': 'ETH-USD',  'secondary': 'BTC-USD'},\n    'DOGE-USD': {'primary': 'BTC-USD',  'secondary': 'ETH-USD'},\n}\n\nprint(f'INPUT_DIM = {INPUT_DIM}  (46 single-pair + 15 cross-asset + 7 derivatives = 68 real, padded to 98)')\nprint(f'SEQ_LEN = {SEQ_LEN}')\nprint(f'LABEL_HORIZON = {LABEL_HORIZON} bars ({LABEL_HORIZON * 5} min)')\nprint(f'LABEL_SCALE = {LABEL_SCALE}')\nprint(f'Lead signals: {list(LEAD_SIGNALS.keys())}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================\n",
    "# CROSS-ASSET FEATURE BUILDER (15 features)\n",
    "# ================================================================\n",
    "\n",
    "def _build_cross_features(close, volume, cross_data, pair_name):\n",
    "    \"\"\"Compute 15 cross-asset features (all returns/correlations/z-scores).\"\"\"\n",
    "    feats = {}\n",
    "    log_ret = np.log(close / close.shift(1))\n",
    "\n",
    "    lead_cfg = LEAD_SIGNALS.get(pair_name, {})\n",
    "    for role, leader_pair, horizons in [\n",
    "        ('primary', lead_cfg.get('primary'), [1, 3, 6]),\n",
    "        ('secondary', lead_cfg.get('secondary'), [1, 3]),\n",
    "    ]:\n",
    "        if leader_pair and leader_pair in cross_data:\n",
    "            lc = cross_data[leader_pair]['close'].astype(float)\n",
    "            lr = np.log(lc / lc.shift(1))\n",
    "            for h in horizons:\n",
    "                feats[f'lead_{role}_ret_{h}'] = lr.rolling(h).sum()\n",
    "        else:\n",
    "            for h in ([1, 3, 6] if role == 'primary' else [1, 3]):\n",
    "                feats[f'lead_{role}_ret_{h}'] = pd.Series(0.0, index=close.index)\n",
    "\n",
    "    for ref_name, ref_label in [('BTC-USD', 'btc'), ('ETH-USD', 'eth')]:\n",
    "        if ref_name in cross_data and ref_name != pair_name:\n",
    "            rc = cross_data[ref_name]['close'].astype(float)\n",
    "            rr = np.log(rc / rc.shift(1))\n",
    "            corr_50 = log_ret.rolling(50).corr(rr)\n",
    "            feats[f'corr_{ref_label}_50'] = corr_50\n",
    "            cm = corr_50.rolling(200).mean()\n",
    "            cs = corr_50.rolling(200).std()\n",
    "            feats[f'corr_z_{ref_label}'] = (corr_50 - cm) / (cs + 1e-10)\n",
    "        else:\n",
    "            feats[f'corr_{ref_label}_50'] = pd.Series(0.0, index=close.index)\n",
    "            feats[f'corr_z_{ref_label}'] = pd.Series(0.0, index=close.index)\n",
    "\n",
    "    for ref_name, ref_label in [('BTC-USD', 'btc'), ('ETH-USD', 'eth')]:\n",
    "        if ref_name in cross_data and ref_name != pair_name:\n",
    "            rc = cross_data[ref_name]['close'].astype(float)\n",
    "            ls = np.log(close / (rc + 1e-10))\n",
    "            sm = ls.rolling(100).mean()\n",
    "            ss = ls.rolling(100).std()\n",
    "            feats[f'spread_{ref_label}_z'] = (ls - sm) / (ss + 1e-10)\n",
    "        else:\n",
    "            feats[f'spread_{ref_label}_z'] = pd.Series(0.0, index=close.index)\n",
    "\n",
    "    all_rets, all_vz = [], []\n",
    "    for p, cdf in cross_data.items():\n",
    "        c = cdf['close'].astype(float)\n",
    "        all_rets.append(np.log(c / c.shift(1)))\n",
    "        if 'volume' in cdf.columns:\n",
    "            v = cdf['volume'].astype(float)\n",
    "            vm = v.rolling(20).mean()\n",
    "            all_vz.append(v / (vm + 1e-10) - 1.0)\n",
    "    all_rets.append(log_ret)\n",
    "    if volume is not None:\n",
    "        vm = volume.rolling(20).mean()\n",
    "        all_vz.append(volume / (vm + 1e-10) - 1.0)\n",
    "\n",
    "    if all_rets:\n",
    "        rdf = pd.concat(all_rets, axis=1)\n",
    "        feats['mkt_avg_ret'] = rdf.mean(axis=1)\n",
    "        feats['mkt_dispersion'] = rdf.std(axis=1)\n",
    "        feats['mkt_breadth'] = (rdf > 0).mean(axis=1)\n",
    "    else:\n",
    "        feats['mkt_avg_ret'] = pd.Series(0.0, index=close.index)\n",
    "        feats['mkt_dispersion'] = pd.Series(0.0, index=close.index)\n",
    "        feats['mkt_breadth'] = pd.Series(0.5, index=close.index)\n",
    "    feats['mkt_avg_vol_z'] = pd.concat(all_vz, axis=1).mean(axis=1) if all_vz else pd.Series(0.0, index=close.index)\n",
    "\n",
    "    return feats\n",
    "\n",
    "print('Cross-asset feature builder defined (15 features)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ================================================================\n# SCALE-INVARIANT FEATURE PIPELINE (46 single-pair + 15 cross-asset + 7 derivatives)\n#\n# CRITICAL: No feature depends on absolute price level or raw volume.\n# Every feature is a return, ratio, z-score, or bounded indicator.\n# This ensures identical feature distributions whether BTC is $3K or $130K.\n# ================================================================\n\ndef _compute_single_pair_features(df):\n    \"\"\"46 scale-invariant features from OHLCV data.\"\"\"\n    close = df['close'].astype(float)\n    _open = df['open'].astype(float) if 'open' in df.columns else close\n    high = df['high'].astype(float) if 'high' in df.columns else close\n    low = df['low'].astype(float) if 'low' in df.columns else close\n    vol = df['volume'].astype(float) if 'volume' in df.columns else None\n\n    features = {}\n\n    # Group 1: Candle shape (5)\n    features['open_gap'] = np.log(_open / (close.shift(1) + 1e-10))\n    features['upper_wick'] = (high - np.maximum(_open, close)) / (close + 1e-10)\n    features['lower_wick'] = (np.minimum(_open, close) - low) / (close + 1e-10)\n    features['body'] = (close - _open) / (close + 1e-10)\n    if vol is not None:\n        vm = vol.rolling(100, min_periods=10).mean()\n        vs = vol.rolling(100, min_periods=10).std()\n        features['volume_z'] = (vol - vm) / (vs + 1e-10)\n    else:\n        features['volume_z'] = close * 0.0\n\n    # Group 2: Returns (7)\n    for w in [1, 2, 3, 5, 10, 20]:\n        features[f'ret_{w}'] = close.pct_change(w)\n    features['log_ret'] = np.log(close / close.shift(1))\n\n    # Group 3: SMA distance + slope (8)\n    for w in [5, 10, 20, 50]:\n        sma = close.rolling(w).mean()\n        features[f'sma_dist_{w}'] = (close - sma) / (sma + 1e-10)\n        features[f'sma_slope_{w}'] = sma.pct_change(3)\n\n    # Group 4: EMA distance + slope (6)\n    for w in [5, 10, 20]:\n        ema = close.ewm(span=w, adjust=False).mean()\n        features[f'ema_dist_{w}'] = (close - ema) / (ema + 1e-10)\n        features[f'ema_slope_{w}'] = ema.pct_change(3)\n\n    # Group 5: Realized volatility (3)\n    pct_ret = close.pct_change()\n    for w in [5, 10, 20]:\n        features[f'vol_{w}'] = pct_ret.rolling(w).std()\n\n    # Group 6: RSI [-1, 1] (1)\n    delta = close.diff()\n    gain = delta.clip(lower=0).rolling(14).mean()\n    loss_s = (-delta.clip(upper=0)).rolling(14).mean()\n    rs = gain / (loss_s + 1e-10)\n    features['rsi_norm'] = (100 - (100 / (1 + rs)) - 50) / 50\n\n    # Group 7: MACD / price (3)\n    ema12 = close.ewm(span=12, adjust=False).mean()\n    ema26 = close.ewm(span=26, adjust=False).mean()\n    macd = ema12 - ema26\n    macd_signal = macd.ewm(span=9, adjust=False).mean()\n    features['macd_pct'] = macd / (close + 1e-10)\n    features['macd_signal_pct'] = macd_signal / (close + 1e-10)\n    features['macd_hist_pct'] = (macd - macd_signal) / (close + 1e-10)\n\n    # Group 8: Bollinger Bands (4)\n    sma20 = close.rolling(20).mean()\n    std20 = close.rolling(20).std()\n    bb_upper = sma20 + 2 * std20\n    bb_lower = sma20 - 2 * std20\n    bb_range = bb_upper - bb_lower + 1e-10\n    features['bb_pct'] = (close - bb_lower) / bb_range\n    features['bb_width'] = bb_range / (sma20 + 1e-10)\n    features['bb_upper_dist'] = (bb_upper - close) / (close + 1e-10)\n    features['bb_lower_dist'] = (close - bb_lower) / (close + 1e-10)\n\n    # Group 9: ATR / price (1)\n    tr = pd.concat([\n        high - low,\n        (high - close.shift(1)).abs(),\n        (low - close.shift(1)).abs(),\n    ], axis=1).max(axis=1)\n    features['atr_pct'] = tr.rolling(14).mean() / (close + 1e-10)\n\n    # Group 10: Volume ratios (3)\n    if vol is not None:\n        features['vol_ratio'] = vol / (vol.rolling(10, min_periods=1).mean() + 1e-10)\n        features['vol_change'] = vol.pct_change()\n        features['vol_trend'] = vol.rolling(5, min_periods=1).mean() / (vol.rolling(20, min_periods=1).mean() + 1e-10)\n    else:\n        features['vol_ratio'] = close * 0.0\n        features['vol_change'] = close * 0.0\n        features['vol_trend'] = close * 0.0 + 1.0\n\n    # Group 11: Momentum (3)\n    features['momentum_5'] = close / close.shift(5) - 1\n    features['momentum_10'] = close / close.shift(10) - 1\n    features['momentum_20'] = close / close.shift(20) - 1\n\n    # Group 12: Range (2)\n    features['hl_range'] = (high - low) / (close + 1e-10)\n    features['hl_range_norm'] = features['hl_range'] / (features['hl_range'].rolling(10, min_periods=1).mean() + 1e-10)\n\n    return features\n\n\ndef _build_derivatives_features(n_rows, derivatives_data=None):\n    \"\"\"Compute 7 derivatives + sentiment features.\n\n    Args:\n        n_rows: Number of rows in the price DataFrame\n        derivatives_data: Dict with optional keys:\n            - 'funding_rate': pd.Series of raw funding rates\n            - 'open_interest': pd.Series of open interest values\n            - 'long_short_ratio': pd.Series of long/short account ratios\n            - 'taker_buy_vol': pd.Series of taker buy volume\n            - 'taker_sell_vol': pd.Series of taker sell volume\n            - 'fear_greed': pd.Series of Fear & Greed index (0-100)\n\n    Returns:\n        Dict of 7 feature_name -> pd.Series\n    \"\"\"\n    idx = pd.RangeIndex(n_rows)\n    feats = {}\n    has_deriv = False\n\n    if derivatives_data is not None:\n        # Funding rate z-score (50-bar window)\n        fr = derivatives_data.get('funding_rate')\n        if fr is not None and len(fr) > 0:\n            fr = fr.astype(float)\n            fr_mean = fr.rolling(50, min_periods=5).mean()\n            fr_std = fr.rolling(50, min_periods=5).std()\n            feats['funding_rate_z'] = (fr - fr_mean) / (fr_std + 1e-10)\n            has_deriv = True\n        else:\n            feats['funding_rate_z'] = pd.Series(0.0, index=idx)\n\n        # Open interest 5-bar % change\n        oi = derivatives_data.get('open_interest')\n        if oi is not None and len(oi) > 0:\n            oi = oi.astype(float)\n            feats['oi_change_pct'] = oi.pct_change(5)\n            has_deriv = True\n        else:\n            feats['oi_change_pct'] = pd.Series(0.0, index=idx)\n\n        # Long/short ratio (raw, already scale-invariant)\n        ls = derivatives_data.get('long_short_ratio')\n        if ls is not None and len(ls) > 0:\n            feats['long_short_ratio'] = ls.astype(float)\n            has_deriv = True\n        else:\n            feats['long_short_ratio'] = pd.Series(0.0, index=idx)\n\n        # Taker buy/sell ratio\n        buy_vol = derivatives_data.get('taker_buy_vol')\n        sell_vol = derivatives_data.get('taker_sell_vol')\n        if buy_vol is not None and sell_vol is not None and len(buy_vol) > 0:\n            bv = buy_vol.astype(float)\n            sv = sell_vol.astype(float)\n            feats['taker_buy_sell_ratio'] = bv / (sv + 1e-10)\n            has_deriv = True\n        else:\n            feats['taker_buy_sell_ratio'] = pd.Series(0.0, index=idx)\n\n        # Fear & Greed (normalized + 3-day ROC)\n        fg = derivatives_data.get('fear_greed')\n        if fg is not None and len(fg) > 0:\n            fg = fg.astype(float)\n            feats['fear_greed_norm'] = fg / 100.0\n            feats['fear_greed_roc'] = fg.diff(864) / 100.0  # 3 days in 5-min bars\n        else:\n            feats['fear_greed_norm'] = pd.Series(0.0, index=idx)\n            feats['fear_greed_roc'] = pd.Series(0.0, index=idx)\n    else:\n        feats['funding_rate_z'] = pd.Series(0.0, index=idx)\n        feats['oi_change_pct'] = pd.Series(0.0, index=idx)\n        feats['long_short_ratio'] = pd.Series(0.0, index=idx)\n        feats['taker_buy_sell_ratio'] = pd.Series(0.0, index=idx)\n        feats['fear_greed_norm'] = pd.Series(0.0, index=idx)\n        feats['fear_greed_roc'] = pd.Series(0.0, index=idx)\n\n    # Binary flag: model knows when derivatives data is present\n    feats['has_derivatives_data'] = pd.Series(1.0 if has_deriv else 0.0, index=idx)\n\n    return feats\n\n\ndef _align_derivatives(price_df, pair, derivatives_dfs, fear_greed_df):\n    \"\"\"Align derivatives data to price DataFrame timestamps via merge_asof.\n\n    Returns dict suitable for _build_derivatives_features(), or None if no data.\n    \"\"\"\n    if not derivatives_dfs and fear_greed_df is None:\n        return None\n\n    result = {}\n\n    # Align per-pair derivatives (funding_rate, OI, LS, taker volumes)\n    if derivatives_dfs and pair in derivatives_dfs:\n        deriv_df = derivatives_dfs[pair].copy()\n        if 'timestamp' in deriv_df.columns and 'timestamp' in price_df.columns:\n            price_ts = price_df[['timestamp']].copy()\n            price_ts['timestamp'] = price_ts['timestamp'].astype(int)\n            deriv_df['timestamp'] = deriv_df['timestamp'].astype(int)\n            price_ts = price_ts.sort_values('timestamp')\n            deriv_df = deriv_df.sort_values('timestamp')\n            merged = pd.merge_asof(price_ts, deriv_df, on='timestamp', direction='backward')\n            for col in ['funding_rate', 'open_interest', 'long_short_ratio',\n                        'taker_buy_vol', 'taker_sell_vol']:\n                if col in merged.columns:\n                    result[col] = merged[col].reset_index(drop=True)\n\n    # Align Fear & Greed (daily -> forward-fill to 5-min bars)\n    if fear_greed_df is not None and 'timestamp' in price_df.columns:\n        fng = fear_greed_df.copy()\n        fng['timestamp'] = fng['timestamp'].astype(int)\n        fng = fng.sort_values('timestamp')\n        price_ts = price_df[['timestamp']].copy()\n        price_ts['timestamp'] = price_ts['timestamp'].astype(int)\n        price_ts = price_ts.sort_values('timestamp')\n        merged_fng = pd.merge_asof(price_ts, fng, on='timestamp', direction='backward')\n        if 'fear_greed' in merged_fng.columns:\n            result['fear_greed'] = merged_fng['fear_greed'].reset_index(drop=True)\n\n    return result if result else None\n\n\ndef build_full_feature_matrix(price_df, cross_data=None, pair_name=None, derivatives_data=None):\n    \"\"\"(N, INPUT_DIM) matrix \u2014 all features for entire DataFrame at once.\"\"\"\n    if price_df is None or len(price_df) < 30:\n        return None\n    df = price_df.copy().reset_index(drop=True)\n    if cross_data is not None:\n        cross_data = {p: cdf.copy().reset_index(drop=True) for p, cdf in cross_data.items() if p != pair_name}\n        if not cross_data:\n            cross_data = None\n    close = df['close'].astype(float) if 'close' in df.columns else None\n    if close is None:\n        return None\n    vol = df['volume'].astype(float) if 'volume' in df.columns else None\n\n    features = _compute_single_pair_features(df)\n    if cross_data is not None and pair_name is not None:\n        features.update(_build_cross_features(close, vol, cross_data, pair_name))\n\n    # Add derivatives features (7 features \u2014 zeros if no data)\n    features.update(_build_derivatives_features(len(df), derivatives_data))\n\n    feat_df = pd.DataFrame(features, index=df.index)\n    feat_df = feat_df.replace([np.inf, -np.inf], np.nan).ffill().bfill().fillna(0)\n    feat_arr = feat_df.values.astype(np.float32)\n    n_feat = feat_arr.shape[1]\n    if n_feat < INPUT_DIM:\n        feat_arr = np.concatenate([feat_arr, np.zeros((len(feat_arr), INPUT_DIM - n_feat), dtype=np.float32)], axis=1)\n    elif n_feat > INPUT_DIM:\n        feat_arr = feat_arr[:, :INPUT_DIM]\n    return feat_arr\n\n\ndef build_feature_sequence(price_df, seq_len=30, cross_data=None, pair_name=None, derivatives_data=None):\n    \"\"\"(seq_len, INPUT_DIM) for a single window \u2014 with per-window standardization.\"\"\"\n    if price_df is None or len(price_df) < seq_len:\n        return None\n    df = price_df.tail(seq_len + 50).copy()\n    if cross_data is not None:\n        cross_data = {p: cdf.tail(seq_len + 50).copy().reset_index(drop=True) for p, cdf in cross_data.items() if p != pair_name}\n        if not cross_data:\n            cross_data = None\n    df = df.reset_index(drop=True)\n    close = df['close'].astype(float) if 'close' in df.columns else None\n    if close is None:\n        return None\n    vol = df['volume'].astype(float) if 'volume' in df.columns else None\n\n    features = _compute_single_pair_features(df)\n    if cross_data is not None and pair_name is not None:\n        features.update(_build_cross_features(close, vol, cross_data, pair_name))\n\n    # Add derivatives features\n    features.update(_build_derivatives_features(len(df), derivatives_data))\n\n    feat_df = pd.DataFrame(features, index=df.index)\n    feat_df = feat_df.replace([np.inf, -np.inf], np.nan).ffill().bfill().fillna(0)\n    feat_arr = feat_df.tail(seq_len).values.astype(np.float32)\n\n    # Per-window standardization\n    mean = feat_arr.mean(axis=0, keepdims=True)\n    std = feat_arr.std(axis=0, keepdims=True) + 1e-8\n    feat_arr = (feat_arr - mean) / std\n\n    n_feat = feat_arr.shape[1]\n    if n_feat < INPUT_DIM:\n        feat_arr = np.concatenate([feat_arr, np.zeros((seq_len, INPUT_DIM - n_feat), dtype=np.float32)], axis=1)\n    elif n_feat > INPUT_DIM:\n        feat_arr = feat_arr[:, :INPUT_DIM]\n    return feat_arr\n\n\n# Verification\ntest_pair = list(pair_dfs.keys())[0]\ntest_df = pair_dfs[test_pair]\n\nf = _compute_single_pair_features(test_df.head(200))\nprint(f'Single-pair features: {len(f)} (expected 46)')\n\ndf = _build_derivatives_features(200, None)\nprint(f'Derivatives features: {len(df)} (expected 7)')\n\nfeat_seq = build_feature_sequence(test_df.head(200), seq_len=30)\nprint(f'Per-window shape: {feat_seq.shape} (expected (30, {INPUT_DIM}))')\nfeat_full = build_full_feature_matrix(test_df.head(1000))\nprint(f'Full-matrix shape: {feat_full.shape} (expected (1000, {INPUT_DIM}))')\n\nearly = build_feature_sequence(test_df.head(200), seq_len=30)\nlate = build_feature_sequence(test_df.tail(200), seq_len=30)\nprint(f'\\nScale invariance (per-window standardized):')\nprint(f'  Early abs mean: {np.abs(early).mean():.4f}')\nprint(f'  Late abs mean:  {np.abs(late).mean():.4f}')\nprint(f'  Ratio: {np.abs(late).mean() / np.abs(early).mean():.2f}x (should be ~1.0)')\n\nassert feat_seq.shape == (30, INPUT_DIM)\nassert feat_full.shape[1] == INPUT_DIM\nprint('\\nAll feature pipeline tests passed!')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================\n",
    "# ATTENTION & POSITIONAL ENCODING\n",
    "# ================================================================\n",
    "\n",
    "class _TrainedAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, qkv_dim):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = qkv_dim // n_heads\n",
    "        self.w_q = nn.Linear(d_model, qkv_dim)\n",
    "        self.w_k = nn.Linear(d_model, qkv_dim)\n",
    "        self.w_v = nn.Linear(d_model, qkv_dim)\n",
    "        self.w_o = nn.Linear(qkv_dim, d_model)\n",
    "        self.attention_temperature = nn.Parameter(torch.ones(1))\n",
    "        self.quantum_enhancement_scale = nn.Parameter(torch.ones(n_heads))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S, _ = x.shape\n",
    "        q = self.w_q(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = self.w_k(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = self.w_v(x).view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        scale = math.sqrt(self.d_head) * self.attention_temperature\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / scale\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        enhancement = self.quantum_enhancement_scale.view(1, self.n_heads, 1, 1)\n",
    "        attn = attn * enhancement\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, S, -1)\n",
    "        return self.w_o(out)\n",
    "\n",
    "\n",
    "class _TrainedPosEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=128):\n",
    "        super().__init__()\n",
    "        self.quantum_phase = nn.Parameter(torch.zeros(d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[0, :, 0::2] = torch.sin(pos * div)\n",
    "        pe[0, :, 1::2] = torch.cos(pos * div[:d_model // 2])\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :] * torch.cos(self.quantum_phase)\n",
    "\n",
    "\n",
    "class _TrainedTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, qkv_dim, d_ff, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.attention = _TrainedAttention(d_model, n_heads, qkv_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model), nn.Dropout(dropout))\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.skip_enhancement = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.skip_enhancement * self.attention(x))\n",
    "        x = self.norm2(x + self.feed_forward(x))\n",
    "        return x\n",
    "\n",
    "print('Attention & positional encoding defined')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ================================================================\n",
    "# ALL 7 MODEL ARCHITECTURES\n",
    "# ================================================================\n",
    "\n",
    "class TrainedQuantumTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=INPUT_DIM):\n",
    "        super().__init__()\n",
    "        d_model, n_heads, qkv_dim, d_ff, n_blocks = 288, 8, 328, 1315, 4\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoding = _TrainedPosEncoding(d_model)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            _TrainedTransformerBlock(d_model, n_heads, qkv_dim, d_ff) for _ in range(n_blocks)])\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.BatchNorm1d(d_model), nn.GELU(),\n",
    "            nn.Linear(d_model, 144), nn.GELU(), nn.Dropout(0.2),\n",
    "            nn.Linear(144, 72), nn.GELU(), nn.Linear(72, 1))\n",
    "        self.uncertainty_head = nn.Sequential(\n",
    "            nn.Linear(d_model, 72), nn.ReLU(), nn.Linear(72, 1), nn.Softplus())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        pooled = x.mean(dim=1)\n",
    "        return self.output_head(pooled), self.uncertainty_head(pooled)\n",
    "\n",
    "\n",
    "class _TrainedLSTMCore(nn.Module):\n",
    "    def __init__(self, input_size=INPUT_DIM, hidden_size=292, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        bidir_dim = hidden_size * 2\n",
    "        self.lstm_layers = nn.ModuleList()\n",
    "        self.skip_projections = nn.ModuleList()\n",
    "        self.consciousness_gates = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_dim = input_size if i == 0 else bidir_dim\n",
    "            self.lstm_layers.append(nn.LSTM(\n",
    "                input_size=in_dim, hidden_size=hidden_size,\n",
    "                num_layers=1, batch_first=True, bidirectional=True))\n",
    "            self.skip_projections.append(nn.Linear(in_dim, bidir_dim))\n",
    "            self.consciousness_gates.append(nn.Sequential(\n",
    "                nn.Linear(bidir_dim, bidir_dim), nn.Sigmoid()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, lstm_layer in enumerate(self.lstm_layers):\n",
    "            skip = self.skip_projections[i](x)\n",
    "            out, _ = lstm_layer(x)\n",
    "            gate = self.consciousness_gates[i](out)\n",
    "            x = gate * out + (1 - gate) * skip\n",
    "        return x\n",
    "\n",
    "\n",
    "class TrainedBidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=INPUT_DIM):\n",
    "        super().__init__()\n",
    "        bidir_dim = 584\n",
    "        self.lstm = _TrainedLSTMCore(input_size=input_dim, hidden_size=292, num_layers=2)\n",
    "        self.prediction_head = nn.Sequential(\n",
    "            nn.BatchNorm1d(bidir_dim), nn.GELU(),\n",
    "            nn.Linear(bidir_dim, 292), nn.GELU(),\n",
    "            nn.BatchNorm1d(292), nn.GELU(),\n",
    "            nn.Linear(292, 146), nn.GELU(), nn.Linear(146, 1))\n",
    "        self.confidence_head = nn.Sequential(\n",
    "            nn.Linear(bidir_dim, 73), nn.ReLU(), nn.Linear(73, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        pooled = self.lstm(x).mean(dim=1)\n",
    "        return self.prediction_head(pooled), self.confidence_head(pooled)\n",
    "\n",
    "\n",
    "class _TrainedDilatedBlock(nn.Module):\n",
    "    def __init__(self, channels, dilation):\n",
    "        super().__init__()\n",
    "        self.add_module('0', nn.Conv1d(channels, channels, kernel_size=3, dilation=dilation, padding=dilation))\n",
    "        self.add_module('1', nn.BatchNorm1d(channels))\n",
    "        self.add_module('4', nn.Conv1d(channels, channels, kernel_size=1))\n",
    "        self.add_module('5', nn.BatchNorm1d(channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(getattr(self, '1')(getattr(self, '0')(x)))\n",
    "        h = F.dropout(h, p=0.2, training=self.training)\n",
    "        h = getattr(self, '5')(getattr(self, '4')(h))\n",
    "        return F.relu(h + x)\n",
    "\n",
    "\n",
    "class _TrainedDilatedCNNCore(nn.Module):\n",
    "    def __init__(self, channels=INPUT_DIM, hidden=332, n_blocks=5):\n",
    "        super().__init__()\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            _TrainedDilatedBlock(channels, dilation=2**i) for i in range(n_blocks)])\n",
    "        fusion_in = channels * n_blocks\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv1d(fusion_in, hidden, kernel_size=1), nn.BatchNorm1d(hidden),\n",
    "            nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Conv1d(hidden, hidden, kernel_size=1), nn.BatchNorm1d(hidden))\n",
    "        self.attention_pool = nn.Sequential(\n",
    "            nn.Softmax(dim=-1),\n",
    "            nn.Conv1d(hidden, channels, kernel_size=1), nn.ReLU(),\n",
    "            nn.Conv1d(channels, hidden, kernel_size=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        block_outputs = []\n",
    "        h = x\n",
    "        for block in self.conv_blocks:\n",
    "            h = block(h)\n",
    "            block_outputs.append(h)\n",
    "        cat = torch.cat(block_outputs, dim=1)\n",
    "        fused = F.relu(self.fusion(cat))\n",
    "        attn = self.attention_pool(fused)\n",
    "        return (fused * F.softmax(attn, dim=-1)).sum(dim=-1)\n",
    "\n",
    "\n",
    "class TrainedDilatedCNN(nn.Module):\n",
    "    def __init__(self, input_dim=INPUT_DIM):\n",
    "        super().__init__()\n",
    "        hidden = 332\n",
    "        self.dilated_cnn = _TrainedDilatedCNNCore(channels=input_dim, hidden=hidden, n_blocks=5)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden), nn.GELU(),\n",
    "            nn.Linear(hidden, 166), nn.BatchNorm1d(166), nn.GELU(), nn.Dropout(0.2),\n",
    "            nn.Linear(166, input_dim), nn.BatchNorm1d(input_dim), nn.GELU(),\n",
    "            nn.Linear(input_dim, 1))\n",
    "        self.pattern_strength = nn.Sequential(\n",
    "            nn.Linear(hidden, 41), nn.ReLU(), nn.Linear(41, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        pooled = self.dilated_cnn(x)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "\n",
    "class TrainedCNN(nn.Module):\n",
    "    def __init__(self, input_dim=INPUT_DIM):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 128, kernel_size=3, padding=1), nn.BatchNorm1d(128), nn.GELU(),\n",
    "            nn.Conv1d(128, 256, kernel_size=5, padding=2), nn.BatchNorm1d(256), nn.GELU(),\n",
    "            nn.Conv1d(256, 128, kernel_size=7, padding=3), nn.BatchNorm1d(128), nn.GELU(),\n",
    "            nn.Conv1d(128, 64, kernel_size=3, padding=1), nn.BatchNorm1d(64), nn.GELU())\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 32), nn.GELU(), nn.Dropout(0.2), nn.Linear(32, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv_layers(x)\n",
    "        return self.classifier(x.mean(dim=-1))\n",
    "\n",
    "\n",
    "class TrainedGRU(nn.Module):\n",
    "    def __init__(self, input_dim=INPUT_DIM):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=input_dim, hidden_size=134,\n",
    "                          num_layers=2, batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        bidir_dim = 268\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bidir_dim, 134), nn.GELU(), nn.Dropout(0.2),\n",
    "            nn.Linear(134, 64), nn.GELU(), nn.Linear(64, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        return self.classifier(out.mean(dim=1))\n",
    "\n",
    "\n",
    "class TrainedMetaEnsemble(nn.Module):\n",
    "    def __init__(self, input_dim=INPUT_DIM):\n",
    "        super().__init__()\n",
    "        self._input_dim = input_dim\n",
    "        n_models = N_BASE_MODELS\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), nn.BatchNorm1d(128), nn.GELU(), nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.GELU())\n",
    "        self.weight_generator = nn.Sequential(\n",
    "            nn.Linear(64, 32), nn.GELU(), nn.Linear(32, n_models))\n",
    "        self.final_predictor = nn.Sequential(\n",
    "            nn.Linear(64 + n_models, 32), nn.GELU(), nn.Dropout(0.1), nn.Linear(32, 1))\n",
    "        self.confidence_estimator = nn.Sequential(\n",
    "            nn.Linear(64 + n_models, 16), nn.ReLU(), nn.Linear(16, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = x[:, :self._input_dim]\n",
    "        model_preds = x[:, self._input_dim:]\n",
    "        ctx = self.feature_extractor(features)\n",
    "        weights = F.softmax(self.weight_generator(ctx), dim=-1)\n",
    "        combined = torch.cat([ctx, model_preds], dim=-1)\n",
    "        return self.final_predictor(combined), self.confidence_estimator(combined)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=32, hidden_dims=None):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [256, 128, 64]\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            encoder_layers.extend([nn.Linear(prev_dim, h), nn.BatchNorm1d(h), nn.LeakyReLU(0.2), nn.Dropout(0.2)])\n",
    "            prev_dim = h\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for h in reversed(hidden_dims):\n",
    "            decoder_layers.extend([nn.Linear(prev_dim, h), nn.BatchNorm1d(h), nn.LeakyReLU(0.2), nn.Dropout(0.2)])\n",
    "            prev_dim = h\n",
    "        decoder_layers.append(nn.Linear(hidden_dims[0], input_dim))\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        return mu + std * torch.randn_like(std)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "\n",
    "# Verify all models\n",
    "x_test = torch.randn(2, 30, INPUT_DIM)\n",
    "for name, cls in [('QT', TrainedQuantumTransformer), ('BiLSTM', TrainedBidirectionalLSTM),\n",
    "                   ('DilatedCNN', TrainedDilatedCNN), ('CNN', TrainedCNN), ('GRU', TrainedGRU)]:\n",
    "    m = cls(input_dim=INPUT_DIM)\n",
    "    out = m(x_test)\n",
    "    shape = out[0].shape if isinstance(out, tuple) else out.shape\n",
    "    print(f'{name}: output {shape}')\n",
    "\n",
    "meta_test = torch.randn(2, INPUT_DIM + N_BASE_MODELS)\n",
    "m = TrainedMetaEnsemble(input_dim=INPUT_DIM)\n",
    "print(f'MetaEnsemble: output {m(meta_test)[0].shape}')\n",
    "\n",
    "vae_test = torch.randn(2, INPUT_DIM)\n",
    "m = VariationalAutoEncoder(input_dim=INPUT_DIM)\n",
    "print(f'VAE: output {m(vae_test)[0].shape}')\n",
    "\n",
    "print('\\nAll 7 model architectures verified!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class DirectionalLoss(nn.Module):\n    \"\"\"v6: BCE + separation margin + magnitude floor. Strengthened for full-data.\n\n    v5 failed on full Colab data (680K samples) \u2014 QT collapsed at epoch 5:\n    - At pred=0 for all: sep penalty = 5.0 * 0.05 = 0.25, mag = 2.0 * 0.01 = 0.02\n    - Total anti-collapse = 0.27, too weak vs weight_decay=1e-4 over 10K batches/epoch\n    - Loss stuck at 0.9475 (= 0.693 BCE + 0.25 sep + 0.02 mag)\n\n    v6 fix:\n    - margin: 0.05 -> 0.10, sep_weight: 5.0 -> 10.0 (4x stronger: 1.0 vs 0.25)\n    - mag_weight: 2.0 -> 5.0 (2.5x stronger: 0.05 vs 0.02)\n    - At collapse, v6 penalty = 1.05 vs v5's 0.27\n    - Combined with weight_decay=1e-5 for QT (in train_base_model)\n    \"\"\"\n    def __init__(self, logit_scale=20.0, margin=0.10):\n        super().__init__()\n        self.logit_scale = logit_scale\n        self.margin = margin\n\n    def forward(self, pred, target):\n        pred = pred.squeeze(-1) if pred.dim() > 1 else pred\n        target = target.squeeze(-1) if target.dim() > 1 else target\n\n        # 1. BCE direction (primary loss)\n        target_pos = (target > 0).float()\n        bce = F.binary_cross_entropy_with_logits(\n            pred * self.logit_scale, target_pos)\n\n        # 2. Separation margin: mean(pred|up) - mean(pred|down) must exceed margin\n        pos_mask = target > 0\n        neg_mask = target <= 0\n        if pos_mask.any() and neg_mask.any():\n            separation = pred[pos_mask].mean() - pred[neg_mask].mean()\n            sep_loss = F.relu(self.margin - separation)\n        else:\n            sep_loss = torch.tensor(0.0, device=pred.device)\n\n        # 3. Magnitude floor: push |pred| above 0.01\n        mag_loss = F.relu(0.01 - pred.abs()).mean()\n\n        return bce + 10.0 * sep_loss + 5.0 * mag_loss\n\n\ndef directional_accuracy(predictions, targets):\n    return float(np.mean(np.sign(predictions) == np.sign(targets)))\n\n\ndef train_epoch(model, loader, optimizer, criterion, gradient_clip=1.0):\n    model.train()\n    total_loss, n = 0.0, 0\n    for X_batch, y_batch in loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        output = model(X_batch)\n        pred = output[0] if isinstance(output, tuple) else output\n        loss = criterion(pred.squeeze(-1), y_batch)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n        optimizer.step()\n        total_loss += loss.item(); n += 1\n    return total_loss / max(n, 1)\n\n\ndef validate_epoch(model, loader, criterion):\n    model.eval()\n    total_loss, n = 0.0, 0\n    all_preds, all_targets = [], []\n    with torch.no_grad():\n        for X_batch, y_batch in loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            output = model(X_batch)\n            pred = output[0] if isinstance(output, tuple) else output\n            pred = pred.squeeze(-1)\n            total_loss += criterion(pred, y_batch).item(); n += 1\n            all_preds.append(pred.cpu().numpy())\n            all_targets.append(y_batch.cpu().numpy())\n    preds = np.concatenate(all_preds) if all_preds else np.array([])\n    targets = np.concatenate(all_targets) if all_targets else np.array([])\n    acc = directional_accuracy(preds, targets) if len(preds) > 0 else 0.5\n    pred_std = float(np.std(preds)) if len(preds) > 0 else 0.0\n    return total_loss / max(n, 1), acc, pred_std\n\nprint('Training utilities defined (v6: BCE + 10*sep_margin(0.10) + 5*mag_floor)')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def walk_forward_split(pair_dfs, train_frac=0.7, val_frac=0.13):\n    train_dfs, val_dfs, test_dfs = {}, {}, {}\n    for pair, df in pair_dfs.items():\n        n = len(df)\n        t_end = int(n * train_frac)\n        v_end = int(n * (train_frac + val_frac))\n        train_dfs[pair] = df.iloc[:t_end].copy().reset_index(drop=True)\n        val_dfs[pair] = df.iloc[t_end:v_end].copy().reset_index(drop=True)\n        test_dfs[pair] = df.iloc[v_end:].copy().reset_index(drop=True)\n        print(f'  {pair}: train={len(train_dfs[pair]):,}, val={len(val_dfs[pair]):,}, test={len(test_dfs[pair]):,}')\n    return train_dfs, val_dfs, test_dfs\n\n\ndef generate_sequences_fast(pair_dfs, seq_len=SEQ_LEN, stride=1, cross_asset=True,\n                            derivatives_dfs=None, fear_greed_df=None):\n    \"\"\"Vectorized sequence generation with 6-bar soft labels.\n\n    Labels: 30-min forward return x 100, clipped to [-1, 1].\n    This gives the model continuous gradient signal proportional to\n    move magnitude, instead of hard +/-1 that causes dead-zone collapse.\n\n    Args:\n        pair_dfs: Dict of pair -> DataFrame with OHLCV columns\n        seq_len: Sequence length per sample\n        stride: Step size between windows\n        cross_asset: If True, include 15 cross-asset features\n        derivatives_dfs: Optional dict of pair -> DataFrame with derivatives columns\n        fear_greed_df: Optional DataFrame with [timestamp, fear_greed]\n    \"\"\"\n    all_X, all_y = [], []\n    warmup = 50\n\n    for pair, df in pair_dfs.items():\n        min_rows = seq_len + warmup + LABEL_HORIZON\n        if len(df) < min_rows:\n            print(f'  Skipping {pair}: only {len(df)} bars (need {min_rows})')\n            continue\n\n        cross_data = None\n        if cross_asset and len(pair_dfs) > 1:\n            cross_data = {p: odf for p, odf in pair_dfs.items() if p != pair}\n\n        # Align derivatives data for this pair\n        deriv_data = _align_derivatives(df, pair, derivatives_dfs, fear_greed_df)\n\n        feat_matrix = build_full_feature_matrix(\n            df, cross_data=cross_data, pair_name=pair,\n            derivatives_data=deriv_data,\n        )\n        if feat_matrix is None:\n            print(f'  Skipping {pair}: feature computation failed')\n            continue\n\n        close_vals = df['close'].values.astype(float)\n        n_samples = 0\n\n        for end_idx in range(warmup + seq_len, len(df) - LABEL_HORIZON + 1, stride):\n            start_idx = end_idx - seq_len\n\n            # Future price: LABEL_HORIZON bars after the window ends\n            future_idx = end_idx + LABEL_HORIZON - 1\n            if future_idx >= len(df):\n                break\n\n            window = feat_matrix[start_idx:end_idx]\n\n            # Per-window standardization\n            mean = window.mean(axis=0, keepdims=True)\n            std = window.std(axis=0, keepdims=True) + 1e-8\n            window = (window - mean) / std\n\n            current_close = close_vals[end_idx - 1]\n            future_close = close_vals[future_idx]\n            if current_close <= 0:\n                continue\n\n            # Soft label: 6-bar forward return, scaled and clipped to [-1, 1]\n            ret = future_close / current_close - 1.0\n            label = float(np.clip(ret * LABEL_SCALE, -1.0, 1.0))\n\n            all_X.append(window)\n            all_y.append(label)\n            n_samples += 1\n\n        print(f'  {pair}: {n_samples:,} sequences')\n\n    if not all_X:\n        return np.array([]), np.array([])\n\n    X = np.array(all_X, dtype=np.float32)\n    y = np.array(all_y, dtype=np.float32)\n    up = (y > 0).sum()\n    down = (y < 0).sum()\n    print(f'Total: {len(X):,} sequences (dim={X.shape[-1]}), '\n          f'balance: {up:,} up / {down:,} down '\n          f'({up/len(y)*100:.1f}% / {down/len(y)*100:.1f}%)')\n    print(f'Label stats: mean={y.mean():.4f}, std={y.std():.4f}, '\n          f'min={y.min():.4f}, max={y.max():.4f}')\n    return X, y",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "%%time\nprint('Splitting data (walk-forward 70/13/17)...')\ntrain_dfs, val_dfs, test_dfs = walk_forward_split(pair_dfs)\n\n# Auto-adjust stride to fit in Colab RAM (~12 GB)\ntotal_train_bars = sum(len(df) for df in train_dfs.values())\nbytes_per_seq = SEQ_LEN * INPUT_DIM * 4\nmax_ram_gb = 8.0\nmax_sequences = int(max_ram_gb * 1e9 / bytes_per_seq)\nstride = max(1, total_train_bars // max_sequences)\nstride = max(stride, 3)\n\nprint(f'\\nTotal train bars: {total_train_bars:,}')\nprint(f'Auto stride={stride} (targets <{max_sequences:,} sequences to fit {max_ram_gb}GB RAM)')\nhas_deriv = bool(derivatives_dfs) or fear_greed_df is not None\nprint(f'Derivatives data: {\"yes\" if has_deriv else \"no (7 features zero-padded)\"}')\n\nprint('\\nGenerating training sequences...')\nX_train, y_train = generate_sequences_fast(\n    train_dfs, stride=stride, cross_asset=True,\n    derivatives_dfs=derivatives_dfs, fear_greed_df=fear_greed_df,\n)\n\nprint('\\nGenerating validation sequences...')\nX_val, y_val = generate_sequences_fast(\n    val_dfs, stride=stride, cross_asset=True,\n    derivatives_dfs=derivatives_dfs, fear_greed_df=fear_greed_df,\n)\n\nprint('\\nGenerating test sequences...')\nX_test, y_test = generate_sequences_fast(\n    test_dfs, stride=stride, cross_asset=True,\n    derivatives_dfs=derivatives_dfs, fear_greed_df=fear_greed_df,\n)\n\nprint(f'\\nDataset shapes: train={X_train.shape}, val={X_val.shape}, test={X_test.shape}')\nassert X_train.shape[-1] == INPUT_DIM\nmem_gb = (X_train.nbytes + X_val.nbytes + X_test.nbytes) / 1e9\nprint(f'Total feature array memory: {mem_gb:.2f} GB')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Base Models (Phase 1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "EPOCHS = 100\nBATCH_SIZE = 64\nLR = 3e-4        # Reduced from 1e-3 \u2014 prevents overshoot on noisy data\nWARMUP_EPOCHS = 3 # Linear warmup before cosine decay\nPATIENCE = 15\n\ntrain_ds = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\nval_ds = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(f'Train batches: {len(train_loader)}, Val batches: {len(val_loader)}')\nprint(f'Label horizon: {LABEL_HORIZON} bars ({LABEL_HORIZON * 5} min)')\nprint(f'Label scale: {LABEL_SCALE} (1% return = label {LABEL_SCALE/100:.1f})')\nprint(f'LR: {LR}, Warmup: {WARMUP_EPOCHS} epochs, Epochs: {EPOCHS}, Patience: {PATIENCE}')\nprint(f'Loss: v6 (BCE*20 + 10*sep_margin(0.10) + 5*mag_floor)')\nprint(f'QT optimizer: weight_decay=0, attention LR={LR*0.1:.1e} (0.1x), other LR={LR:.1e}')\nprint(f'Other models: weight_decay=1e-4, LR={LR:.1e}')\nresults = {}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def train_base_model(name, model_cls):\n    print(f'\\n{\"=\"*60}')\n    print(f'Training {name}')\n    print(f'{\"=\"*60}')\n\n    model = model_cls(input_dim=INPUT_DIM).to(device)\n    n_params = sum(p.numel() for p in model.parameters())\n    print(f'Parameters: {n_params:,}')\n\n    # v7: QT needs special optimizer \u2014 weight_decay compounds 13.5x more on\n    # full data (10K+ batches/epoch vs 780 local). Even 1e-5 per step = 10%\n    # shrinkage/epoch, which degenerates attention (softmax \u2192 uniform \u2192 constant).\n    #\n    # Fix: wd=0 + differential LR (attention at 0.1x) + collapse recovery.\n    if name == 'quantum_transformer':\n        # Split params: attention layers get lower LR and zero weight decay\n        attn_params = []\n        other_params = []\n        for pname, p in model.named_parameters():\n            if any(k in pname for k in ['attention', 'pos_encoding', 'skip_enhancement']):\n                attn_params.append(p)\n            else:\n                other_params.append(p)\n\n        optimizer = torch.optim.AdamW([\n            {'params': attn_params, 'lr': LR * 0.1, 'weight_decay': 0},\n            {'params': other_params, 'lr': LR, 'weight_decay': 0},\n        ])\n        n_attn = sum(p.numel() for p in attn_params)\n        n_other = sum(p.numel() for p in other_params)\n        print(f'Param groups: attention={n_attn:,} (lr={LR*0.1:.1e}, wd=0), '\n              f'other={n_other:,} (lr={LR:.1e}, wd=0)')\n    else:\n        wd = 1e-4\n        optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=wd)\n        print(f'Weight decay: {wd}')\n\n    # Linear warmup + cosine decay\n    def lr_lambda(epoch):\n        if epoch < WARMUP_EPOCHS:\n            return (epoch + 1) / WARMUP_EPOCHS\n        progress = (epoch - WARMUP_EPOCHS) / max(EPOCHS - WARMUP_EPOCHS, 1)\n        return 0.5 * (1 + math.cos(math.pi * progress))\n\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n    # v6 loss: BCE*20 + 10*separation_margin(0.10) + 5*magnitude_floor\n    criterion = DirectionalLoss(logit_scale=20.0, margin=0.10)\n\n    best_val_loss = float('inf')\n    best_acc = 0.0\n    best_state = None\n    patience_counter = 0\n    collapse_recoveries = 0\n    max_collapse_recoveries = 3\n    t0 = time.time()\n\n    for epoch in range(EPOCHS):\n        train_loss = train_epoch(model, train_loader, optimizer, criterion)\n        val_loss, val_acc, pred_std = validate_epoch(model, val_loader, criterion)\n        scheduler.step()\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_acc = val_acc\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            patience_counter = 0\n        else:\n            patience_counter += 1\n\n        collapsed = pred_std < 0.001\n\n        if (epoch + 1) % 5 == 0 or epoch == 0 or patience_counter == 0 or collapsed:\n            tag = ' *** COLLAPSED ***' if collapsed else ''\n            print(f'  Epoch {epoch+1:3d}/{EPOCHS}: train={train_loss:.4f}, val={val_loss:.4f}, '\n                  f'acc={val_acc:.3f}, pred_std={pred_std:.4f}, '\n                  f'lr={optimizer.param_groups[0][\"lr\"]:.2e}{tag}')\n\n        # Collapse recovery: reload best checkpoint and halve all LRs\n        if collapsed and epoch >= WARMUP_EPOCHS and best_state is not None:\n            if collapse_recoveries < max_collapse_recoveries:\n                collapse_recoveries += 1\n                model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n                for pg in optimizer.param_groups:\n                    pg['lr'] *= 0.5\n                print(f'  >>> Collapse recovery #{collapse_recoveries}: reloaded best, '\n                      f'halved LR to {optimizer.param_groups[0][\"lr\"]:.2e}')\n                patience_counter = 0  # Reset patience after recovery\n                continue\n            else:\n                print(f'  >>> Max collapse recoveries ({max_collapse_recoveries}) reached, stopping')\n                break\n\n        if patience_counter >= PATIENCE:\n            print(f'  Early stopping at epoch {epoch+1}')\n            break\n\n    model.load_state_dict(best_state)\n    model.eval()\n\n    test_ds = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n    test_loader = DataLoader(test_ds, batch_size=128, shuffle=False)\n    test_loss, test_acc, test_std = validate_epoch(model, test_loader, criterion)\n\n    elapsed = time.time() - t0\n    print(f'\\n  Test: loss={test_loss:.4f}, dir_acc={test_acc:.3f}, pred_std={test_std:.4f}')\n    print(f'  Time: {elapsed/60:.1f} min, Epochs: {epoch+1}, Collapse recoveries: {collapse_recoveries}')\n\n    save_path = f'models/trained/best_{name}_model.pth'\n    torch.save(model.state_dict(), save_path)\n    print(f'  Saved: {save_path}')\n\n    results[name] = {\n        'val_loss': best_val_loss, 'val_acc': best_acc,\n        'test_loss': test_loss, 'test_acc': test_acc,\n        'test_pred_std': test_std,\n        'epochs': epoch + 1, 'time_min': elapsed / 60,\n        'collapse_recoveries': collapse_recoveries,\n    }\n    return model",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "base_configs = [\n",
    "    ('quantum_transformer', TrainedQuantumTransformer),\n",
    "    ('bidirectional_lstm', TrainedBidirectionalLSTM),\n",
    "    ('dilated_cnn', TrainedDilatedCNN),\n",
    "    ('cnn', TrainedCNN),\n",
    "    ('gru', TrainedGRU),\n",
    "]\n",
    "\n",
    "trained_base_models = {}\n",
    "for name, cls in base_configs:\n",
    "    try:\n",
    "        model = train_base_model(name, cls)\n",
    "        trained_base_models[name] = model\n",
    "    except Exception as e:\n",
    "        print(f'\\nFAILED: {name}: {e}')\n",
    "        results[name] = {'status': 'failed', 'error': str(e)}\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'Base models trained: {len(trained_base_models)}/5')\n",
    "print(f'{\"=\"*60}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Meta-Ensemble (Phase 2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "assert len(trained_base_models) == 5, f'Need all 5 base models, got {len(trained_base_models)}'\n",
    "\n",
    "def generate_meta_inputs(base_models, X, y):\n",
    "    n = len(X)\n",
    "    all_preds = {name: np.zeros(n) for name in BASE_MODEL_NAMES}\n",
    "    for name in BASE_MODEL_NAMES:\n",
    "        model = base_models[name]\n",
    "        model_preds = []\n",
    "        for i in range(0, n, 128):\n",
    "            batch = torch.FloatTensor(X[i:i+128]).to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(batch)\n",
    "                pred = output[0] if isinstance(output, tuple) else output\n",
    "                model_preds.append(torch.tanh(pred.squeeze(-1)).cpu().numpy())\n",
    "        all_preds[name] = np.concatenate(model_preds)\n",
    "        acc = directional_accuracy(all_preds[name], y)\n",
    "        print(f'  {name}: dir_acc={acc:.3f}')\n",
    "    last_features = X[:, -1, :]\n",
    "    pred_matrix = np.column_stack([all_preds[name] for name in BASE_MODEL_NAMES])\n",
    "    meta_X = np.concatenate([last_features, pred_matrix], axis=1).astype(np.float32)\n",
    "    print(f'Meta-inputs: {meta_X.shape}')\n",
    "    return meta_X, y\n",
    "\n",
    "print('Generating meta-inputs from base model predictions...')\n",
    "print('\\nTraining set:')\n",
    "meta_X_train, meta_y_train = generate_meta_inputs(trained_base_models, X_train, y_train)\n",
    "print('\\nValidation set:')\n",
    "meta_X_val, meta_y_val = generate_meta_inputs(trained_base_models, X_val, y_val)\n",
    "print('\\nTest set:')\n",
    "meta_X_test, meta_y_test = generate_meta_inputs(trained_base_models, X_test, y_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(f'\\n{\"=\"*60}')\nprint('Training Meta-Ensemble')\nprint(f'{\"=\"*60}')\n\nMETA_EPOCHS = 80\nMETA_LR = 3e-4\nMETA_PATIENCE = 12\n\nmeta_model = TrainedMetaEnsemble(input_dim=INPUT_DIM).to(device)\noptimizer = torch.optim.AdamW(meta_model.parameters(), lr=META_LR, weight_decay=1e-4)\n\n# Warmup + cosine decay\ndef meta_lr_lambda(epoch):\n    if epoch < 3:\n        return (epoch + 1) / 3\n    progress = (epoch - 3) / max(META_EPOCHS - 3, 1)\n    return 0.5 * (1 + math.cos(math.pi * progress))\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, meta_lr_lambda)\ncriterion = DirectionalLoss(logit_scale=20.0, margin=0.10)\n\nmeta_train_ds = TensorDataset(torch.FloatTensor(meta_X_train), torch.FloatTensor(meta_y_train))\nmeta_val_ds = TensorDataset(torch.FloatTensor(meta_X_val), torch.FloatTensor(meta_y_val))\nmeta_train_loader = DataLoader(meta_train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\nmeta_val_loader = DataLoader(meta_val_ds, batch_size=BATCH_SIZE, shuffle=False)\n\nbest_val_loss = float('inf')\nbest_state = None\npatience_counter = 0\nt0 = time.time()\n\nfor epoch in range(META_EPOCHS):\n    meta_model.train()\n    total_loss, n_b = 0.0, 0\n    for X_b, y_b in meta_train_loader:\n        X_b, y_b = X_b.to(device), y_b.to(device)\n        optimizer.zero_grad()\n        pred, _ = meta_model(X_b)\n        loss = criterion(pred.squeeze(-1), y_b)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(meta_model.parameters(), 1.0)\n        optimizer.step()\n        total_loss += loss.item(); n_b += 1\n    train_loss = total_loss / max(n_b, 1)\n\n    meta_model.eval()\n    v_loss, v_n = 0.0, 0\n    v_preds, v_tgts = [], []\n    with torch.no_grad():\n        for X_b, y_b in meta_val_loader:\n            X_b, y_b = X_b.to(device), y_b.to(device)\n            pred, _ = meta_model(X_b)\n            v_loss += criterion(pred.squeeze(-1), y_b).item(); v_n += 1\n            v_preds.append(pred.squeeze(-1).cpu().numpy())\n            v_tgts.append(y_b.cpu().numpy())\n    val_loss = v_loss / max(v_n, 1)\n    val_acc = directional_accuracy(np.concatenate(v_preds), np.concatenate(v_tgts))\n    pred_std = float(np.std(np.concatenate(v_preds)))\n    scheduler.step()\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_state = {k: v.cpu().clone() for k, v in meta_model.state_dict().items()}\n        patience_counter = 0\n    else:\n        patience_counter += 1\n\n    if (epoch+1) % 5 == 0 or epoch == 0 or patience_counter == 0:\n        print(f'  Epoch {epoch+1:3d}/{META_EPOCHS}: train={train_loss:.4f}, val={val_loss:.4f}, '\n              f'acc={val_acc:.3f}, pred_std={pred_std:.4f}')\n\n    if patience_counter >= META_PATIENCE:\n        print(f'  Early stopping at epoch {epoch+1}')\n        break\n\nmeta_model.load_state_dict(best_state)\nmeta_model.eval()\n\n# Test\nmeta_test_ds = TensorDataset(torch.FloatTensor(meta_X_test), torch.FloatTensor(meta_y_test))\nmeta_test_loader = DataLoader(meta_test_ds, batch_size=128, shuffle=False)\nt_preds, t_tgts = [], []\nwith torch.no_grad():\n    for X_b, y_b in meta_test_loader:\n        pred, _ = meta_model(X_b.to(device))\n        t_preds.append(pred.squeeze(-1).cpu().numpy())\n        t_tgts.append(y_b.numpy())\ntest_acc = directional_accuracy(np.concatenate(t_preds), np.concatenate(t_tgts))\nsimple_avg = meta_X_test[:, INPUT_DIM:].mean(axis=1)\nsimple_acc = directional_accuracy(simple_avg, meta_y_test)\n\nelapsed = time.time() - t0\nprint(f'\\n  Test dir_acc: {test_acc:.3f}')\nprint(f'  Simple average baseline: {simple_acc:.3f}')\nprint(f'  Ensemble lift: {(test_acc - simple_acc)*100:+.1f} pp')\nprint(f'  Time: {elapsed/60:.1f} min')\n\ntorch.save(meta_model.state_dict(), 'models/trained/best_meta_ensemble_model.pth')\nprint('  Saved: models/trained/best_meta_ensemble_model.pth')\n\nresults['meta_ensemble'] = {\n    'val_loss': best_val_loss, 'val_acc': val_acc,\n    'test_acc': test_acc, 'simple_avg_acc': simple_acc,\n    'epochs': epoch + 1, 'time_min': elapsed / 60,\n}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train VAE (Phase 3)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f'\\n{\"=\"*60}')\n",
    "print('Training VAE Anomaly Detector')\n",
    "print(f'{\"=\"*60}')\n",
    "\n",
    "vae_samples = X_train[:, -1, :]\n",
    "print(f'VAE training samples: {vae_samples.shape}')\n",
    "\n",
    "VAE_EPOCHS = 100\n",
    "VAE_LR = 1e-3\n",
    "VAE_PATIENCE = 20\n",
    "\n",
    "vae_model = VariationalAutoEncoder(input_dim=INPUT_DIM, latent_dim=32).to(device)\n",
    "vae_optimizer = torch.optim.Adam(vae_model.parameters(), lr=VAE_LR)\n",
    "\n",
    "n_vae = len(vae_samples)\n",
    "n_vae_train = int(n_vae * 0.85)\n",
    "vae_train_ds = TensorDataset(torch.FloatTensor(vae_samples[:n_vae_train]))\n",
    "vae_val_ds = TensorDataset(torch.FloatTensor(vae_samples[n_vae_train:]))\n",
    "vae_train_loader = DataLoader(vae_train_ds, batch_size=64, shuffle=True)\n",
    "vae_val_loader = DataLoader(vae_val_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "def vae_loss_fn(recon, x, mu, logvar):\n",
    "    recon_loss = F.mse_loss(recon, x, reduction='sum') / x.size(0)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "    return recon_loss + kl_loss\n",
    "\n",
    "best_vae_loss = float('inf')\n",
    "best_vae_state = None\n",
    "vae_patience = 0\n",
    "t0 = time.time()\n",
    "\n",
    "for epoch in range(VAE_EPOCHS):\n",
    "    vae_model.train()\n",
    "    total_loss, n_b = 0.0, 0\n",
    "    for (batch,) in vae_train_loader:\n",
    "        batch = batch.to(device)\n",
    "        vae_optimizer.zero_grad()\n",
    "        recon, mu, logvar = vae_model(batch)\n",
    "        loss = vae_loss_fn(recon, batch, mu, logvar)\n",
    "        loss.backward()\n",
    "        vae_optimizer.step()\n",
    "        total_loss += loss.item(); n_b += 1\n",
    "    train_loss = total_loss / max(n_b, 1)\n",
    "\n",
    "    vae_model.eval()\n",
    "    v_loss, v_n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for (batch,) in vae_val_loader:\n",
    "            batch = batch.to(device)\n",
    "            recon, mu, logvar = vae_model(batch)\n",
    "            v_loss += vae_loss_fn(recon, batch, mu, logvar).item(); v_n += 1\n",
    "    val_loss = v_loss / max(v_n, 1)\n",
    "\n",
    "    if val_loss < best_vae_loss:\n",
    "        best_vae_loss = val_loss\n",
    "        best_vae_state = {k: v.cpu().clone() for k, v in vae_model.state_dict().items()}\n",
    "        vae_patience = 0\n",
    "    else:\n",
    "        vae_patience += 1\n",
    "\n",
    "    if (epoch+1) % 10 == 0 or epoch == 0 or vae_patience == 0:\n",
    "        print(f'  Epoch {epoch+1:3d}/{VAE_EPOCHS}: train={train_loss:.4f}, val={val_loss:.4f}')\n",
    "\n",
    "    if vae_patience >= VAE_PATIENCE:\n",
    "        print(f'  Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "vae_model.load_state_dict(best_vae_state)\n",
    "torch.save(vae_model.state_dict(), 'models/trained/vae_anomaly_detector.pth')\n",
    "\n",
    "vae_model.eval()\n",
    "errors = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(vae_samples), 128):\n",
    "        batch = torch.FloatTensor(vae_samples[i:i+128]).to(device)\n",
    "        recon, _, _ = vae_model(batch)\n",
    "        err = ((recon - batch) ** 2).mean(dim=1).cpu().numpy()\n",
    "        errors.extend(err)\n",
    "errors = np.array(errors)\n",
    "elapsed = time.time() - t0\n",
    "\n",
    "print(f'\\n  Reconstruction error: p50={np.percentile(errors, 50):.4f}, '\n",
    "      f'p95={np.percentile(errors, 95):.4f}, p99={np.percentile(errors, 99):.4f}')\n",
    "print(f'  Time: {elapsed/60:.1f} min')\n",
    "print(f'  Saved: models/trained/vae_anomaly_detector.pth')\n",
    "\n",
    "results['vae'] = {\n",
    "    'val_loss': best_vae_loss,\n",
    "    'p50': float(np.percentile(errors, 50)),\n",
    "    'p95': float(np.percentile(errors, 95)),\n",
    "    'p99': float(np.percentile(errors, 99)),\n",
    "    'epochs': epoch + 1,\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary & Download"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(f'\\n{\"=\"*60}')\nprint('TRAINING SUMMARY')\nprint(f'{\"=\"*60}')\nprint(f'{\"Model\":<25} {\"Val Loss\":>10} {\"Val Acc\":>10} {\"Test Acc\":>10} {\"PredStd\":>10} {\"Epochs\":>8} {\"Recov\":>6}')\nprint('-' * 81)\nfor name in [n for n, _ in base_configs] + ['meta_ensemble', 'vae']:\n    r = results.get(name, {})\n    vl = f\"{r.get('val_loss', 0):.4f}\" if 'val_loss' in r else 'N/A'\n    va = f\"{r.get('val_acc', 0):.3f}\" if 'val_acc' in r else 'N/A'\n    ta = f\"{r.get('test_acc', 0):.3f}\" if 'test_acc' in r else 'N/A'\n    ps = f\"{r.get('test_pred_std', 0):.4f}\" if 'test_pred_std' in r else 'N/A'\n    ep = str(r.get('epochs', 'N/A'))\n    rec = str(r.get('collapse_recoveries', '-'))\n    print(f'{name:<25} {vl:>10} {va:>10} {ta:>10} {ps:>10} {ep:>8} {rec:>6}')\n\n# Check for collapsed models\ncollapsed = [n for n, r in results.items()\n             if r.get('test_pred_std', 1.0) < 0.001 and 'test_pred_std' in r]\nif collapsed:\n    print(f'\\n*** WARNING: {len(collapsed)} model(s) collapsed to constant predictions: {collapsed}')\n    print('*** These weights will produce ~0 predictions in production.')\nelse:\n    print(f'\\nAll models producing varied predictions (no collapse detected)')\n\nhas_deriv = bool(derivatives_dfs) or fear_greed_df is not None\nprint(f'\\nInput dim: {INPUT_DIM} (46 single-pair + 15 cross-asset + 7 derivatives, padded to 98)')\nprint(f'Derivatives data: {\"yes\" if has_deriv else \"no (7 features zero-padded)\"}')\nprint(f'Label: {LABEL_HORIZON}-bar ({LABEL_HORIZON * 5}-min) forward return, scale={LABEL_SCALE}')\nprint(f'Loss: v6 (BCE*20 + 10*sep_margin(0.10) + 5*mag_floor)')\nprint(f'QT optimizer: wd=0, attention LR=0.1x (v7 \u2014 prevents attention weight collapse)')\nprint(f'Other models: wd=1e-4, LR={LR:.1e}')\nprint(f'Training data: {sum(len(df) for df in pair_dfs.values()):,} bars across {len(pair_dfs)} pairs')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Zip and save to Google Drive + browser download\n",
    "zip_path = 'trained_models_98dim.zip'\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    for f in os.listdir('models/trained'):\n",
    "        if f.endswith('.pth'):\n",
    "            full = os.path.join('models/trained', f)\n",
    "            zf.write(full, f)\n",
    "            size_mb = os.path.getsize(full) / 1e6\n",
    "            print(f'  Added: {f} ({size_mb:.1f} MB)')\n",
    "\n",
    "zip_size = os.path.getsize(zip_path) / 1e6\n",
    "print(f'\\nZip: {zip_path} ({zip_size:.1f} MB)')\n",
    "\n",
    "# Save to Google Drive\n",
    "drive_out = f'/content/drive/My Drive/{DRIVE_FOLDER}/{zip_path}'\n",
    "shutil.copy2(zip_path, drive_out)\n",
    "print(f'Saved to Drive: {drive_out}')\n",
    "\n",
    "# Also try browser download\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(zip_path)\n",
    "    print('Browser download started')\n",
    "except Exception as e:\n",
    "    print(f'Browser download failed ({e})')\n",
    "    print(f'Download from Google Drive instead: {drive_out}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Post-Download\n",
    "\n",
    "After downloading `trained_models_98dim.zip` from Google Drive, unzip and copy:\n",
    "\n",
    "```bash\n",
    "cd ~/Downloads\n",
    "unzip trained_models_98dim.zip -d trained_models_98dim\n",
    "cp trained_models_98dim/*.pth ~/Downloads/bitcoin-trading-bot-renaissance/models/trained/\n",
    "```\n",
    "\n",
    "Then restart the bot \u2014 it will auto-detect the new 98-dim weights via `_detect_input_dim()`."
   ]
  }
 ]
}