{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: SETUP -- Multi-Asset Crash Regime LightGBM Training\n",
    "# =============================================================================\n",
    "# Train crash-regime LightGBM models for ETH, SOL, XRP\n",
    "# Same 51-feature approach as BTC v3, with BTC as cross-asset lead\n",
    "# Training config: RECENT_ONLY (2021 crash + 2025 crash)\n",
    "# =============================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import subprocess\n",
    "subprocess.run(['pip', 'install', 'lightgbm', 'yfinance'], capture_output=True)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "DRIVE_SAVE = '/content/drive/MyDrive/renaissance-bot-training/crash_models_multi'\n",
    "DRIVE_V3 = '/content/drive/MyDrive/renaissance-bot-training/crash_models_v3'\n",
    "os.makedirs(DRIVE_SAVE, exist_ok=True)\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "os.makedirs('/content/models', exist_ok=True)\n",
    "\n",
    "ASSETS = {\n",
    "    'ETH': {\n",
    "        'symbol': 'ETHUSDT',\n",
    "        'start_date': '2017-09-01',\n",
    "        'crash_periods': [\n",
    "            ('2018-01-07', '2018-12-15', 'ICO Bust'),\n",
    "            ('2021-11-10', '2022-11-21', 'Terra/FTX/Fed'),\n",
    "            ('2025-10-06', '2026-02-28', 'Current'),\n",
    "        ],\n",
    "        'cross_asset': 'BTC',\n",
    "    },\n",
    "    'SOL': {\n",
    "        'symbol': 'SOLUSDT',\n",
    "        'start_date': '2020-08-01',\n",
    "        'crash_periods': [\n",
    "            ('2021-11-10', '2022-11-21', 'Terra/FTX/Fed'),\n",
    "            ('2025-10-06', '2026-02-28', 'Current'),\n",
    "        ],\n",
    "        'cross_asset': 'BTC',\n",
    "    },\n",
    "    'XRP': {\n",
    "        'symbol': 'XRPUSDT',\n",
    "        'start_date': '2018-01-01',\n",
    "        'crash_periods': [\n",
    "            ('2018-01-07', '2018-12-15', 'ICO Bust'),\n",
    "            ('2021-11-10', '2022-11-21', 'Terra/FTX/Fed'),\n",
    "            ('2025-10-06', '2026-02-28', 'Current'),\n",
    "        ],\n",
    "        'cross_asset': 'BTC',\n",
    "    },\n",
    "}\n",
    "\n",
    "TRAINING_CONFIG = 'RECENT_ONLY'\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    # Group 1: Target asset Price/Volume (15)\n",
    "    'return_1bar', 'return_6bar', 'return_12bar', 'return_48bar', 'return_288bar',\n",
    "    'vol_12bar', 'vol_48bar', 'vol_ratio', 'volume_surge', 'volume_trend',\n",
    "    'consecutive_red', 'drawdown_24h', 'rsi_14_norm', 'bb_pct_b', 'vwap_distance',\n",
    "    # Group 2: Daily Macro (10)\n",
    "    'spx_return_1d', 'spx_vs_sma', 'vix_norm', 'vix_change', 'vix_extreme',\n",
    "    'dxy_return_1d', 'dxy_trend', 'yield_level', 'yield_change', 'fng_norm',\n",
    "    # Group 3: Derivatives (9)\n",
    "    'funding_z', 'funding_extreme_long', 'funding_extreme_short',\n",
    "    'oi_change_1h', 'oi_change_4h', 'oi_spike',\n",
    "    'ls_ratio_norm', 'ls_extreme_long', 'taker_imbalance',\n",
    "    # Group 4: Intraday Macro (11) -- placeholders\n",
    "    'spx_return_5m', 'spx_return_15m', 'spx_return_1h',\n",
    "    'spx_momentum_5m', 'spx_direction_5m',\n",
    "    'vix_return_5m', 'vix_return_1h', 'vix_spike_5m',\n",
    "    'ndx_return_5m', 'ndx_return_1h',\n",
    "    'has_intraday_macro',\n",
    "    # Group 5: Cross-Asset -- BTC as lead (6)\n",
    "    'btc_return_1bar', 'btc_return_6bar', 'btc_asset_ratio_change',\n",
    "    'btc_lead_1', 'btc_lead_2', 'btc_lead_3',\n",
    "]\n",
    "\n",
    "assert len(FEATURE_COLS) == 51, f'Expected 51 features, got {len(FEATURE_COLS)}'\n",
    "\n",
    "print('[DONE] Setup complete')\n",
    "print(f'Assets to train: {list(ASSETS.keys())}')\n",
    "print(f'Training config: {TRAINING_CONFIG}')\n",
    "print(f'Feature count: {len(FEATURE_COLS)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Download price data for all assets\n",
    "# =============================================================================\n",
    "\n",
    "def download_binance_klines(symbol, start_date, interval='5m'):\n",
    "    \"\"\"Download all klines from Binance alternate endpoint.\"\"\"\n",
    "    BASE_URL = \"https://data-api.binance.vision/api/v3/klines\"\n",
    "    all_candles = []\n",
    "    start_ms = int(pd.Timestamp(start_date).timestamp() * 1000)\n",
    "    end_ms = int(datetime.utcnow().timestamp() * 1000)\n",
    "    current = start_ms\n",
    "\n",
    "    while current < end_ms:\n",
    "        params = {\n",
    "            'symbol': symbol,\n",
    "            'interval': interval,\n",
    "            'startTime': current,\n",
    "            'limit': 1000,\n",
    "        }\n",
    "        try:\n",
    "            resp = requests.get(BASE_URL, params=params, timeout=30)\n",
    "            if resp.status_code != 200:\n",
    "                print(f'  [WARN] {symbol} status {resp.status_code}, retrying...')\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "            data = resp.json()\n",
    "            if not data:\n",
    "                break\n",
    "            all_candles.extend(data)\n",
    "            current = data[-1][0] + 1\n",
    "            if len(all_candles) % 100000 == 0:\n",
    "                print(f'  {symbol}: {len(all_candles):,} candles...')\n",
    "            time.sleep(0.1)\n",
    "        except Exception as e:\n",
    "            print(f'  [WARN] {symbol} error: {e}, retrying...')\n",
    "            time.sleep(5)\n",
    "\n",
    "    if not all_candles:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(all_candles, columns=[\n",
    "        'open_time', 'open', 'high', 'low', 'close', 'volume',\n",
    "        'close_time', 'quote_volume', 'trades', 'taker_buy_base',\n",
    "        'taker_buy_quote', 'ignore'\n",
    "    ])\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume', 'quote_volume']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df['timestamp'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    df = df.drop_duplicates('timestamp').sort_values('timestamp').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# 1. Load BTC data (needed as cross-asset for all models)\n",
    "btc_path = f'{DRIVE_V3}/btc_5m_full.csv'\n",
    "btc_local = f'{DRIVE_SAVE}/btc_5m.csv'\n",
    "btc = pd.DataFrame()\n",
    "\n",
    "for p in [btc_path, btc_local]:\n",
    "    if os.path.exists(p):\n",
    "        btc = pd.read_csv(p)\n",
    "        btc['timestamp'] = pd.to_datetime(btc['timestamp'])\n",
    "        if len(btc) > 100000:\n",
    "            print(f'[OK] BTC loaded from cache: {len(btc):,} candles')\n",
    "            break\n",
    "        else:\n",
    "            print(f'[WARN] BTC file at {p} too small ({len(btc)}), trying next...')\n",
    "            btc = pd.DataFrame()\n",
    "\n",
    "if len(btc) == 0:\n",
    "    print('BTC not found in cache, downloading fresh...')\n",
    "    btc = download_binance_klines('BTCUSDT', '2017-09-01')\n",
    "\n",
    "if len(btc) > 0:\n",
    "    btc.to_csv(btc_local, index=False)\n",
    "    print(f'BTC: {len(btc):,} candles ({btc[\"timestamp\"].min()} to {btc[\"timestamp\"].max()})')\n",
    "else:\n",
    "    raise RuntimeError('BTC download failed -- cannot continue without BTC cross-asset data')\n",
    "\n",
    "# 2. Download each target asset\n",
    "asset_data = {'BTC': btc}\n",
    "for name, cfg in ASSETS.items():\n",
    "    cache_path = f'{DRIVE_SAVE}/{name.lower()}_5m.csv'\n",
    "    if os.path.exists(cache_path):\n",
    "        df = pd.read_csv(cache_path)\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        if len(df) > 50000:\n",
    "            print(f'[OK] {name} loaded from cache: {len(df):,} candles')\n",
    "            asset_data[name] = df\n",
    "            continue\n",
    "\n",
    "    print(f'Downloading {name} ({cfg[\"symbol\"]})...')\n",
    "    df = download_binance_klines(cfg['symbol'], cfg['start_date'])\n",
    "    asset_data[name] = df\n",
    "    if len(df) > 0:\n",
    "        df.to_csv(cache_path, index=False)\n",
    "        print(f'[OK] {name}: {len(df):,} candles ({df[\"timestamp\"].min()} to {df[\"timestamp\"].max()})')\n",
    "    else:\n",
    "        print(f'[ERROR] {name} download failed')\n",
    "\n",
    "print(f'\\n[DONE] Price data ready for {list(asset_data.keys())}')\n",
    "for k, v in asset_data.items():\n",
    "    print(f'  {k}: {len(v):,} candles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Download macro data (shared across all assets)\n",
    "# =============================================================================\n",
    "\n",
    "# Check if macro data exists in v3 or multi cache\n",
    "macro_df = pd.DataFrame()\n",
    "for p in [f'{DRIVE_V3}/macro_daily.csv', f'{DRIVE_SAVE}/macro_daily.csv']:\n",
    "    if os.path.exists(p):\n",
    "        macro_df = pd.read_csv(p, index_col=0, parse_dates=True)\n",
    "        if len(macro_df) > 500:\n",
    "            print(f'[OK] Macro data loaded from cache: {len(macro_df)} days')\n",
    "            break\n",
    "        else:\n",
    "            macro_df = pd.DataFrame()\n",
    "\n",
    "if len(macro_df) == 0:\n",
    "    print('Downloading macro data via yfinance...')\n",
    "    try:\n",
    "        import yfinance as yf\n",
    "        tickers = {\n",
    "            'spx': '^GSPC',\n",
    "            'vix': '^VIX',\n",
    "            'dxy': 'DX-Y.NYB',\n",
    "            'ndx': '^IXIC',\n",
    "            'us10y': '^TNX',\n",
    "        }\n",
    "        macro_frames = {}\n",
    "        for name, symbol in tickers.items():\n",
    "            try:\n",
    "                tk = yf.Ticker(symbol)\n",
    "                hist = tk.history(period='max', interval='1d')\n",
    "                if len(hist) > 0:\n",
    "                    macro_frames[name] = hist['Close'].rename(name)\n",
    "                    print(f'  {name}: {len(hist)} days')\n",
    "            except Exception as e:\n",
    "                print(f'  [WARN] {name} failed: {e}')\n",
    "\n",
    "        if macro_frames:\n",
    "            macro_df = pd.DataFrame(macro_frames)\n",
    "            macro_df.index = pd.to_datetime(macro_df.index)\n",
    "            macro_df = macro_df.sort_index().ffill()\n",
    "            print(f'[OK] Macro data: {len(macro_df)} days, cols={list(macro_df.columns)}')\n",
    "    except ImportError:\n",
    "        print('[ERROR] yfinance not available')\n",
    "\n",
    "# Fear & Greed Index\n",
    "fng_df = pd.DataFrame()\n",
    "fng_path = f'{DRIVE_V3}/fng_daily.csv'\n",
    "fng_path2 = f'{DRIVE_SAVE}/fng_daily.csv'\n",
    "\n",
    "for p in [fng_path, fng_path2]:\n",
    "    if os.path.exists(p):\n",
    "        fng_df = pd.read_csv(p, parse_dates=['date'])\n",
    "        if len(fng_df) > 100:\n",
    "            print(f'[OK] Fear & Greed loaded from cache: {len(fng_df)} days')\n",
    "            break\n",
    "        fng_df = pd.DataFrame()\n",
    "\n",
    "if len(fng_df) == 0:\n",
    "    print('Downloading Fear & Greed Index...')\n",
    "    try:\n",
    "        resp = requests.get('https://api.alternative.me/fng/?limit=0&format=json', timeout=30)\n",
    "        if resp.status_code == 200:\n",
    "            fng_data = resp.json().get('data', [])\n",
    "            fng_df = pd.DataFrame(fng_data)\n",
    "            fng_df['date'] = pd.to_datetime(fng_df['timestamp'].astype(int), unit='s')\n",
    "            fng_df['fng_value'] = pd.to_numeric(fng_df['value'], errors='coerce')\n",
    "            fng_df = fng_df[['date', 'fng_value']].sort_values('date').reset_index(drop=True)\n",
    "            print(f'[OK] Fear & Greed: {len(fng_df)} days')\n",
    "    except Exception as e:\n",
    "        print(f'[WARN] FNG download failed: {e}')\n",
    "\n",
    "# Save\n",
    "if len(macro_df) > 0:\n",
    "    macro_df.to_csv(f'{DRIVE_SAVE}/macro_daily.csv')\n",
    "if len(fng_df) > 0:\n",
    "    fng_df.to_csv(f'{DRIVE_SAVE}/fng_daily.csv', index=False)\n",
    "\n",
    "print(f'\\n[DONE] Macro: {len(macro_df)} days, FNG: {len(fng_df)} days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Download derivatives for each asset\n",
    "# =============================================================================\n",
    "\n",
    "FAPI_BASE = 'https://fapi.binance.com'\n",
    "\n",
    "def download_funding_rates(symbol, start_date):\n",
    "    \"\"\"Download funding rate history from Binance Futures.\"\"\"\n",
    "    all_rows = []\n",
    "    start_ms = int(pd.Timestamp(start_date).timestamp() * 1000)\n",
    "    current = start_ms\n",
    "    end_ms = int(datetime.utcnow().timestamp() * 1000)\n",
    "\n",
    "    while current < end_ms:\n",
    "        url = f'{FAPI_BASE}/fapi/v1/fundingRate'\n",
    "        params = {'symbol': symbol, 'startTime': current, 'limit': 1000}\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, timeout=30)\n",
    "            if resp.status_code in [451, 403, 418]:\n",
    "                print(f'  [WARN] {symbol} funding rate blocked (status {resp.status_code})')\n",
    "                break\n",
    "            if resp.status_code != 200:\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            data = resp.json()\n",
    "            if not data:\n",
    "                break\n",
    "            all_rows.extend(data)\n",
    "            current = data[-1]['fundingTime'] + 1\n",
    "            time.sleep(0.2)\n",
    "        except Exception as e:\n",
    "            print(f'  [WARN] funding rate error: {e}')\n",
    "            time.sleep(3)\n",
    "            break\n",
    "\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    df['timestamp'] = pd.to_datetime(df['fundingTime'], unit='ms')\n",
    "    df['fundingRate'] = pd.to_numeric(df['fundingRate'], errors='coerce')\n",
    "    return df[['timestamp', 'fundingRate']].sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "\n",
    "def download_futures_stat(symbol, stat_type, start_date, period='4h'):\n",
    "    \"\"\"Download OI, long/short ratio, or taker volume from Binance Futures.\"\"\"\n",
    "    endpoint_map = {\n",
    "        'oi': '/futures/data/openInterestHist',\n",
    "        'ls': '/futures/data/globalLongShortAccountRatio',\n",
    "        'taker': '/futures/data/takeBuySellVol',\n",
    "    }\n",
    "    url = FAPI_BASE + endpoint_map[stat_type]\n",
    "    all_rows = []\n",
    "    start_ms = int(pd.Timestamp(start_date).timestamp() * 1000)\n",
    "    current = start_ms\n",
    "    end_ms = int(datetime.utcnow().timestamp() * 1000)\n",
    "\n",
    "    while current < end_ms:\n",
    "        params = {\n",
    "            'symbol': symbol,\n",
    "            'period': period,\n",
    "            'startTime': current,\n",
    "            'limit': 500,\n",
    "        }\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, timeout=30)\n",
    "            if resp.status_code in [451, 403, 418]:\n",
    "                print(f'  [WARN] {symbol} {stat_type} blocked (status {resp.status_code})')\n",
    "                break\n",
    "            if resp.status_code != 200:\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            data = resp.json()\n",
    "            if not data:\n",
    "                break\n",
    "            all_rows.extend(data)\n",
    "            current = data[-1].get('timestamp', data[-1].get('createTime', 0))\n",
    "            if isinstance(current, str):\n",
    "                current = int(current)\n",
    "            current += 1\n",
    "            time.sleep(0.3)\n",
    "        except Exception as e:\n",
    "            print(f'  [WARN] {stat_type} error: {e}')\n",
    "            time.sleep(3)\n",
    "            break\n",
    "\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    ts_col = 'timestamp' if 'timestamp' in df.columns else 'createTime'\n",
    "    df['timestamp'] = pd.to_datetime(pd.to_numeric(df[ts_col], errors='coerce'), unit='ms')\n",
    "    for col in df.columns:\n",
    "        if col != 'timestamp':\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Download derivatives for each asset\n",
    "deriv_data = {}\n",
    "for name, cfg in ASSETS.items():\n",
    "    symbol = cfg['symbol']\n",
    "    start = cfg['start_date']\n",
    "    print(f'\\n--- {name} ({symbol}) derivatives ---')\n",
    "\n",
    "    cache_prefix = f'{DRIVE_SAVE}/{name.lower()}'\n",
    "\n",
    "    # Funding rates\n",
    "    fr_path = f'{cache_prefix}_funding.csv'\n",
    "    if os.path.exists(fr_path):\n",
    "        fr = pd.read_csv(fr_path, parse_dates=['timestamp'])\n",
    "        print(f'  Funding rates from cache: {len(fr)} rows')\n",
    "    else:\n",
    "        fr = download_funding_rates(symbol, start)\n",
    "        if len(fr) > 0:\n",
    "            fr.to_csv(fr_path, index=False)\n",
    "        print(f'  Funding rates downloaded: {len(fr)} rows')\n",
    "\n",
    "    # Open Interest\n",
    "    oi_path = f'{cache_prefix}_oi.csv'\n",
    "    if os.path.exists(oi_path):\n",
    "        oi = pd.read_csv(oi_path, parse_dates=['timestamp'])\n",
    "        print(f'  Open Interest from cache: {len(oi)} rows')\n",
    "    else:\n",
    "        oi = download_futures_stat(symbol, 'oi', '2020-01-01')\n",
    "        if len(oi) > 0:\n",
    "            oi.to_csv(oi_path, index=False)\n",
    "        print(f'  Open Interest downloaded: {len(oi)} rows')\n",
    "\n",
    "    # Long/Short Ratio\n",
    "    ls_path = f'{cache_prefix}_ls.csv'\n",
    "    if os.path.exists(ls_path):\n",
    "        ls = pd.read_csv(ls_path, parse_dates=['timestamp'])\n",
    "        print(f'  Long/Short ratio from cache: {len(ls)} rows')\n",
    "    else:\n",
    "        ls = download_futures_stat(symbol, 'ls', '2020-01-01')\n",
    "        if len(ls) > 0:\n",
    "            ls.to_csv(ls_path, index=False)\n",
    "        print(f'  Long/Short ratio downloaded: {len(ls)} rows')\n",
    "\n",
    "    # Taker Buy/Sell Volume\n",
    "    tk_path = f'{cache_prefix}_taker.csv'\n",
    "    if os.path.exists(tk_path):\n",
    "        tk = pd.read_csv(tk_path, parse_dates=['timestamp'])\n",
    "        print(f'  Taker volume from cache: {len(tk)} rows')\n",
    "    else:\n",
    "        tk = download_futures_stat(symbol, 'taker', '2020-01-01')\n",
    "        if len(tk) > 0:\n",
    "            tk.to_csv(tk_path, index=False)\n",
    "        print(f'  Taker volume downloaded: {len(tk)} rows')\n",
    "\n",
    "    deriv_data[name] = {'funding': fr, 'oi': oi, 'ls': ls, 'taker': tk}\n",
    "\n",
    "print('\\n[DONE] Derivatives data downloaded for all assets')\n",
    "for name, dd in deriv_data.items():\n",
    "    counts = {k: len(v) for k, v in dd.items()}\n",
    "    print(f'  {name}: {counts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: Build crash datasets for each asset\n",
    "# =============================================================================\n",
    "\n",
    "btc_df = asset_data['BTC'].copy()\n",
    "btc_df = btc_df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "crash_datasets = {}\n",
    "\n",
    "for name, cfg in ASSETS.items():\n",
    "    print(f'\\n{\"=\" * 60}')\n",
    "    print(f'Building crash dataset for {name}')\n",
    "    print(f'{\"=\" * 60}')\n",
    "\n",
    "    target_df = asset_data[name].copy()\n",
    "    target_df = target_df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "    # 1. Label crash periods\n",
    "    target_df['is_crash'] = False\n",
    "    for start_str, end_str, period_name in cfg['crash_periods']:\n",
    "        start = pd.Timestamp(start_str)\n",
    "        end = pd.Timestamp(end_str)\n",
    "        mask = (target_df['timestamp'] >= start) & (target_df['timestamp'] <= end)\n",
    "        target_df.loc[mask, 'is_crash'] = True\n",
    "        count = mask.sum()\n",
    "        print(f'  Crash {start_str} to {end_str} ({period_name}): {count:,} candles')\n",
    "\n",
    "    crash = target_df[target_df['is_crash']].copy().reset_index(drop=True)\n",
    "    print(f'  Total crash candles: {len(crash):,}')\n",
    "\n",
    "    # 2. Create date column for daily merges\n",
    "    crash['date'] = crash['timestamp'].dt.date\n",
    "    crash['date'] = pd.to_datetime(crash['date'])\n",
    "\n",
    "    # 3. Merge daily macro\n",
    "    if len(macro_df) > 0:\n",
    "        macro_merge = macro_df.copy()\n",
    "        macro_merge.index = pd.to_datetime(macro_merge.index)\n",
    "        all_dates = crash['date'].unique()\n",
    "        macro_reindexed = macro_merge.reindex(\n",
    "            macro_merge.index.union(pd.DatetimeIndex(all_dates))\n",
    "        ).sort_index().ffill().bfill()\n",
    "        macro_reindexed = macro_reindexed.loc[all_dates]\n",
    "        macro_reindexed = macro_reindexed.reset_index().rename(columns={'index': 'date'})\n",
    "        crash = crash.merge(macro_reindexed, on='date', how='left')\n",
    "        macro_coverage = crash['spx'].notna().mean() if 'spx' in crash.columns else 0\n",
    "        print(f'  Macro coverage: {macro_coverage:.1%}')\n",
    "\n",
    "    # 4. Merge Fear & Greed\n",
    "    if len(fng_df) > 0:\n",
    "        fng_merge = fng_df.copy()\n",
    "        fng_merge['date'] = pd.to_datetime(fng_merge['date'])\n",
    "        crash = crash.merge(fng_merge[['date', 'fng_value']], on='date', how='left')\n",
    "        crash['fng_value'] = crash['fng_value'].ffill().bfill().fillna(50)\n",
    "        fng_coverage = crash['fng_value'].notna().mean()\n",
    "        print(f'  FNG coverage: {fng_coverage:.1%}')\n",
    "\n",
    "    # 5. Merge derivatives via merge_asof\n",
    "    dd = deriv_data.get(name, {})\n",
    "\n",
    "    if len(dd.get('funding', pd.DataFrame())) > 0:\n",
    "        fr = dd['funding'].copy()\n",
    "        fr = fr.sort_values('timestamp')\n",
    "        crash = crash.sort_values('timestamp')\n",
    "        crash = pd.merge_asof(crash, fr, on='timestamp', direction='backward')\n",
    "        print(f'  Funding rate coverage: {crash[\"fundingRate\"].notna().mean():.1%}')\n",
    "\n",
    "    if len(dd.get('oi', pd.DataFrame())) > 0:\n",
    "        oi = dd['oi'].copy()\n",
    "        oi = oi.sort_values('timestamp')\n",
    "        oi_cols = [c for c in oi.columns if c not in ['timestamp', 'symbol']]\n",
    "        crash = pd.merge_asof(crash, oi[['timestamp'] + oi_cols], on='timestamp', direction='backward')\n",
    "        print(f'  OI coverage: {crash[\"sumOpenInterest\"].notna().mean() if \"sumOpenInterest\" in crash.columns else 0:.1%}')\n",
    "\n",
    "    if len(dd.get('ls', pd.DataFrame())) > 0:\n",
    "        ls = dd['ls'].copy()\n",
    "        ls = ls.sort_values('timestamp')\n",
    "        ls_cols = [c for c in ls.columns if c not in ['timestamp', 'symbol']]\n",
    "        for col in ls_cols:\n",
    "            if col in crash.columns:\n",
    "                ls = ls.rename(columns={col: f'ls_{col}'})\n",
    "        ls_cols = [c for c in ls.columns if c not in ['timestamp', 'symbol']]\n",
    "        crash = pd.merge_asof(crash, ls[['timestamp'] + ls_cols], on='timestamp', direction='backward')\n",
    "        ls_col = 'longShortRatio' if 'longShortRatio' in crash.columns else 'ls_longShortRatio'\n",
    "        if ls_col in crash.columns:\n",
    "            if ls_col != 'longShortRatio':\n",
    "                crash = crash.rename(columns={ls_col: 'longShortRatio'})\n",
    "            print(f'  L/S ratio coverage: {crash[\"longShortRatio\"].notna().mean():.1%}')\n",
    "\n",
    "    if len(dd.get('taker', pd.DataFrame())) > 0:\n",
    "        tk = dd['taker'].copy()\n",
    "        tk = tk.sort_values('timestamp')\n",
    "        tk_cols = [c for c in tk.columns if c not in ['timestamp', 'symbol']]\n",
    "        for col in tk_cols:\n",
    "            if col in crash.columns and col != 'timestamp':\n",
    "                tk = tk.rename(columns={col: f'tk_{col}'})\n",
    "        tk_cols = [c for c in tk.columns if c not in ['timestamp', 'symbol']]\n",
    "        crash = pd.merge_asof(crash, tk[['timestamp'] + tk_cols], on='timestamp', direction='backward')\n",
    "        buy_col = 'buyVol' if 'buyVol' in crash.columns else 'tk_buyVol'\n",
    "        sell_col = 'sellVol' if 'sellVol' in crash.columns else 'tk_sellVol'\n",
    "        if buy_col in crash.columns:\n",
    "            if buy_col != 'buyVol':\n",
    "                crash = crash.rename(columns={buy_col: 'buyVol'})\n",
    "            if sell_col != 'sellVol':\n",
    "                crash = crash.rename(columns={sell_col: 'sellVol'})\n",
    "\n",
    "    # 6. Merge BTC as cross-asset (by exact timestamp)\n",
    "    btc_cross = btc_df[['timestamp', 'close', 'volume']].copy()\n",
    "    btc_cross = btc_cross.rename(columns={'close': 'btc_close', 'volume': 'btc_volume'})\n",
    "    btc_cross = btc_cross.sort_values('timestamp')\n",
    "    crash = crash.sort_values('timestamp')\n",
    "    crash = pd.merge_asof(crash, btc_cross, on='timestamp', direction='nearest',\n",
    "                          tolerance=pd.Timedelta('5min'))\n",
    "    btc_coverage = crash['btc_close'].notna().mean()\n",
    "    print(f'  BTC cross-asset coverage: {btc_coverage:.1%}')\n",
    "\n",
    "    crash_datasets[name] = crash\n",
    "    save_path = f'{DRIVE_SAVE}/{name.lower()}_crash_raw.csv'\n",
    "    crash.to_csv(save_path, index=False)\n",
    "    print(f'  Saved: {save_path} ({len(crash):,} rows)')\n",
    "\n",
    "print('\\n[DONE] Crash datasets built for all assets')\n",
    "for name, ds in crash_datasets.items():\n",
    "    print(f'  {name}: {len(ds):,} rows, {len(ds.columns)} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: Feature engineering (51 features per asset)\n",
    "# =============================================================================\n",
    "\n",
    "feature_datasets = {}\n",
    "\n",
    "for name in ASSETS.keys():\n",
    "    print(f'\\n{\"=\" * 60}')\n",
    "    print(f'Feature engineering for {name}')\n",
    "    print(f'{\"=\" * 60}')\n",
    "\n",
    "    df = crash_datasets[name].copy()\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "    # ---- GROUP 1: Target Asset Price/Volume (15 features) ----\n",
    "    print('  Group 1: Target asset Price/Volume (15 features)...')\n",
    "\n",
    "    df['return_1bar'] = df['close'].pct_change(1)\n",
    "    df['return_6bar'] = df['close'].pct_change(6)\n",
    "    df['return_12bar'] = df['close'].pct_change(12)\n",
    "    df['return_48bar'] = df['close'].pct_change(48)\n",
    "    df['return_288bar'] = df['close'].pct_change(288)\n",
    "\n",
    "    df['vol_12bar'] = df['return_1bar'].rolling(12).std()\n",
    "    df['vol_48bar'] = df['return_1bar'].rolling(48).std()\n",
    "    df['vol_ratio'] = df['vol_12bar'] / df['vol_48bar'].replace(0, np.nan)\n",
    "\n",
    "    vol_mean = df['volume'].rolling(48).mean()\n",
    "    df['volume_surge'] = df['volume'] / vol_mean.replace(0, np.nan)\n",
    "    df['volume_trend'] = vol_mean.pct_change(12)\n",
    "\n",
    "    is_red = (df['close'] < df['open']).astype(int)\n",
    "    streaks = is_red.copy()\n",
    "    for i in range(1, len(streaks)):\n",
    "        if streaks.iloc[i] == 1:\n",
    "            streaks.iloc[i] = streaks.iloc[i-1] + 1\n",
    "    df['consecutive_red'] = streaks / 12.0\n",
    "\n",
    "    rolling_high_24h = df['high'].rolling(288).max()\n",
    "    df['drawdown_24h'] = (df['close'] - rolling_high_24h) / rolling_high_24h.replace(0, np.nan)\n",
    "\n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.clip(lower=0).rolling(14).mean()\n",
    "    loss = (-delta.clip(upper=0)).rolling(14).mean()\n",
    "    rs = gain / loss.replace(0, np.nan)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    df['rsi_14_norm'] = (rsi - 50) / 50\n",
    "\n",
    "    bb_mid = df['close'].rolling(20).mean()\n",
    "    bb_std = df['close'].rolling(20).std()\n",
    "    bb_upper = bb_mid + 2 * bb_std\n",
    "    bb_lower = bb_mid - 2 * bb_std\n",
    "    bb_range = (bb_upper - bb_lower).replace(0, np.nan)\n",
    "    df['bb_pct_b'] = (df['close'] - bb_lower) / bb_range\n",
    "\n",
    "    vwap_num = (df['close'] * df['volume']).rolling(48).sum()\n",
    "    vwap_den = df['volume'].rolling(48).sum().replace(0, np.nan)\n",
    "    vwap = vwap_num / vwap_den\n",
    "    df['vwap_distance'] = (df['close'] - vwap) / vwap.replace(0, np.nan)\n",
    "\n",
    "    # ---- GROUP 2: Daily Macro (10 features) ----\n",
    "    print('  Group 2: Daily Macro (10 features)...')\n",
    "\n",
    "    if 'spx' in df.columns:\n",
    "        df['spx_return_1d'] = df.groupby('date')['spx'].transform('first')\n",
    "        df['spx_return_1d'] = df['spx_return_1d'].pct_change(288)\n",
    "        spx_sma_20 = df['spx'].rolling(288 * 20).mean()\n",
    "        df['spx_vs_sma'] = (df['spx'] - spx_sma_20) / spx_sma_20.replace(0, np.nan)\n",
    "    else:\n",
    "        df['spx_return_1d'] = 0.0\n",
    "        df['spx_vs_sma'] = 0.0\n",
    "\n",
    "    if 'vix' in df.columns:\n",
    "        df['vix_norm'] = df['vix'] / 30.0\n",
    "        df['vix_change'] = df['vix'].pct_change(288)\n",
    "        df['vix_extreme'] = (df['vix'] > 30).astype(float)\n",
    "    else:\n",
    "        df['vix_norm'] = 0.0\n",
    "        df['vix_change'] = 0.0\n",
    "        df['vix_extreme'] = 0.0\n",
    "\n",
    "    if 'dxy' in df.columns:\n",
    "        df['dxy_return_1d'] = df['dxy'].pct_change(288)\n",
    "        dxy_sma = df['dxy'].rolling(288 * 20).mean()\n",
    "        df['dxy_trend'] = (df['dxy'] - dxy_sma) / dxy_sma.replace(0, np.nan)\n",
    "    else:\n",
    "        df['dxy_return_1d'] = 0.0\n",
    "        df['dxy_trend'] = 0.0\n",
    "\n",
    "    if 'us10y' in df.columns:\n",
    "        df['yield_level'] = df['us10y'] / 5.0\n",
    "        df['yield_change'] = df['us10y'].pct_change(288)\n",
    "    else:\n",
    "        df['yield_level'] = 0.0\n",
    "        df['yield_change'] = 0.0\n",
    "\n",
    "    if 'fng_value' in df.columns:\n",
    "        df['fng_norm'] = df['fng_value'] / 100.0\n",
    "    else:\n",
    "        df['fng_norm'] = 0.5\n",
    "\n",
    "    # ---- GROUP 3: Derivatives (9 features) ----\n",
    "    print('  Group 3: Derivatives (9 features)...')\n",
    "\n",
    "    has_derivatives = 'fundingRate' in df.columns and df['fundingRate'].notna().sum() > 100\n",
    "\n",
    "    if has_derivatives:\n",
    "        fr = df['fundingRate'].fillna(0)\n",
    "        fr_mean = fr.rolling(288).mean()\n",
    "        fr_std = fr.rolling(288).std().replace(0, np.nan)\n",
    "        df['funding_z'] = (fr - fr_mean) / fr_std\n",
    "        df['funding_extreme_long'] = (df['funding_z'] > 2).astype(float)\n",
    "        df['funding_extreme_short'] = (df['funding_z'] < -2).astype(float)\n",
    "\n",
    "        if 'sumOpenInterest' in df.columns:\n",
    "            oi = df['sumOpenInterest'].ffill()\n",
    "            df['oi_change_1h'] = oi.pct_change(12)\n",
    "            df['oi_change_4h'] = oi.pct_change(48)\n",
    "            oi_pct = oi.pct_change(12).abs()\n",
    "            oi_thresh = oi_pct.rolling(288).quantile(0.95)\n",
    "            df['oi_spike'] = (oi_pct > oi_thresh).astype(float)\n",
    "        else:\n",
    "            df['oi_change_1h'] = 0.0\n",
    "            df['oi_change_4h'] = 0.0\n",
    "            df['oi_spike'] = 0.0\n",
    "\n",
    "        if 'longShortRatio' in df.columns:\n",
    "            lsr = df['longShortRatio'].fillna(1.0)\n",
    "            df['ls_ratio_norm'] = (lsr - 1.0) / 0.5\n",
    "            df['ls_extreme_long'] = (lsr > 2.0).astype(float)\n",
    "        else:\n",
    "            df['ls_ratio_norm'] = 0.0\n",
    "            df['ls_extreme_long'] = 0.0\n",
    "\n",
    "        if 'buyVol' in df.columns and 'sellVol' in df.columns:\n",
    "            buy = df['buyVol'].fillna(0)\n",
    "            sell = df['sellVol'].fillna(0)\n",
    "            total = (buy + sell).replace(0, np.nan)\n",
    "            df['taker_imbalance'] = (buy - sell) / total\n",
    "        else:\n",
    "            df['taker_imbalance'] = 0.0\n",
    "    else:\n",
    "        print('    [WARN] Insufficient derivatives, filling with zeros')\n",
    "        for feat in ['funding_z', 'funding_extreme_long', 'funding_extreme_short',\n",
    "                     'oi_change_1h', 'oi_change_4h', 'oi_spike',\n",
    "                     'ls_ratio_norm', 'ls_extreme_long', 'taker_imbalance']:\n",
    "            df[feat] = 0.0\n",
    "\n",
    "    # ---- GROUP 4: Intraday Macro (11 features) - ALL ZEROS ----\n",
    "    print('  Group 4: Intraday Macro (11 placeholders = 0.0)...')\n",
    "    intraday_cols = [\n",
    "        'spx_return_5m', 'spx_return_15m', 'spx_return_1h',\n",
    "        'spx_momentum_5m', 'spx_direction_5m',\n",
    "        'vix_return_5m', 'vix_return_1h', 'vix_spike_5m',\n",
    "        'ndx_return_5m', 'ndx_return_1h', 'has_intraday_macro',\n",
    "    ]\n",
    "    for col in intraday_cols:\n",
    "        df[col] = 0.0\n",
    "\n",
    "    # ---- GROUP 5: Cross-Asset -- BTC as lead indicator (6 features) ----\n",
    "    print('  Group 5: Cross-Asset -- BTC lead (6 features)...')\n",
    "\n",
    "    if 'btc_close' in df.columns and df['btc_close'].notna().sum() > 100:\n",
    "        df['btc_return_1bar'] = df['btc_close'].pct_change(1)\n",
    "        df['btc_return_6bar'] = df['btc_close'].pct_change(6)\n",
    "        # Asset/BTC ratio change\n",
    "        asset_btc_ratio = df['close'] / df['btc_close'].replace(0, np.nan)\n",
    "        df['btc_asset_ratio_change'] = asset_btc_ratio.pct_change(12)\n",
    "    else:\n",
    "        df['btc_return_1bar'] = 0.0\n",
    "        df['btc_return_6bar'] = 0.0\n",
    "        df['btc_asset_ratio_change'] = 0.0\n",
    "\n",
    "    # BTC lead signals (BTC moves first, alts follow)\n",
    "    btc_r1 = df['btc_close'].pct_change(1) if 'btc_close' in df.columns else pd.Series(0.0, index=df.index)\n",
    "    df['btc_lead_1'] = btc_r1.shift(1)\n",
    "    df['btc_lead_2'] = btc_r1.shift(2)\n",
    "    df['btc_lead_3'] = btc_r1.shift(3)\n",
    "\n",
    "    # ---- LABELS ----\n",
    "    print('  Building labels...')\n",
    "    df['forward_return_1'] = df['close'].shift(-1) / df['close'] - 1\n",
    "    df['forward_return_2'] = df['close'].shift(-2) / df['close'] - 1\n",
    "    df['label_binary'] = (df['forward_return_2'] > 0).astype(int)\n",
    "    df['label_binary_1bar'] = (df['forward_return_1'] > 0).astype(int)\n",
    "\n",
    "    # ---- Clean up ----\n",
    "    for col in FEATURE_COLS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    df = df.dropna(subset=['label_binary']).reset_index(drop=True)\n",
    "\n",
    "    feature_datasets[name] = df\n",
    "    save_path = f'{DRIVE_SAVE}/{name.lower()}_crash_features.csv'\n",
    "    df.to_csv(save_path, index=False)\n",
    "\n",
    "    print(f'\\n  {name} feature summary:')\n",
    "    print(f'    Total rows: {len(df):,}')\n",
    "    print(f'    Features: {len(FEATURE_COLS)}')\n",
    "    print(f'    Label balance (2bar): {df[\"label_binary\"].mean():.4f}')\n",
    "    print(f'    Label balance (1bar): {df[\"label_binary_1bar\"].mean():.4f}')\n",
    "    non_zero = sum(1 for c in FEATURE_COLS if df[c].abs().sum() > 0)\n",
    "    print(f'    Non-zero features: {non_zero}/{len(FEATURE_COLS)}')\n",
    "\n",
    "print('\\n[DONE] Feature engineering complete for all assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: Train models for each asset (RECENT_ONLY, 2 horizons each)\n",
    "# =============================================================================\n",
    "\n",
    "BASE_PARAMS = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 100,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    'verbose': -1,\n",
    "}\n",
    "\n",
    "\n",
    "def train_and_eval(train_X, train_y, val_X, val_y, test_X, test_y,\n",
    "                   params, label='model'):\n",
    "    \"\"\"Train LightGBM and return metrics dict.\"\"\"\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "    val_ds = lgb.Dataset(val_X, label=val_y, reference=train_ds)\n",
    "\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(30),\n",
    "        lgb.log_evaluation(0),\n",
    "    ]\n",
    "\n",
    "    model = lgb.train(\n",
    "        params, train_ds,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[val_ds],\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    val_pred = model.predict(val_X)\n",
    "    test_pred = model.predict(test_X)\n",
    "\n",
    "    val_acc = accuracy_score(val_y, (val_pred > 0.5).astype(int))\n",
    "    test_acc = accuracy_score(test_y, (test_pred > 0.5).astype(int))\n",
    "\n",
    "    try:\n",
    "        val_auc = roc_auc_score(val_y, val_pred)\n",
    "    except ValueError:\n",
    "        val_auc = 0.5\n",
    "    try:\n",
    "        test_auc = roc_auc_score(test_y, test_pred)\n",
    "    except ValueError:\n",
    "        test_auc = 0.5\n",
    "\n",
    "    pred_std = float(np.std(test_pred))\n",
    "    n_rounds = model.best_iteration if model.best_iteration else model.num_trees()\n",
    "\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feat_imp = sorted(zip(FEATURE_COLS, importance), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    print(f'\\n  {label}:')\n",
    "    print(f'    Train: {len(train_y):,} | Val: {len(val_y):,} | Test: {len(test_y):,}')\n",
    "    print(f'    Val acc={val_acc:.4f}  Test acc={test_acc:.4f}  Test AUC={test_auc:.4f}')\n",
    "    print(f'    Pred std={pred_std:.4f}  Rounds={n_rounds}')\n",
    "    print(f'    Top 5 features:')\n",
    "    for fname, fval in feat_imp[:5]:\n",
    "        print(f'      {fname:30s} gain={fval:.1f}')\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'val_acc': val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'val_auc': val_auc,\n",
    "        'test_auc': test_auc,\n",
    "        'pred_std': pred_std,\n",
    "        'n_rounds': n_rounds,\n",
    "        'test_pred': test_pred,\n",
    "        'feat_imp': feat_imp,\n",
    "        'train_size': len(train_y),\n",
    "    }\n",
    "\n",
    "\n",
    "# Train all assets x horizons\n",
    "all_results = {}\n",
    "\n",
    "for name, cfg in ASSETS.items():\n",
    "    print(f'\\n{\"=\" * 60}')\n",
    "    print(f'Training {name} crash models')\n",
    "    print(f'{\"=\" * 60}')\n",
    "\n",
    "    df = feature_datasets[name].copy()\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "    # RECENT_ONLY: use crash 2 (2021) + crash 3 (2025) only\n",
    "    # Filter out crash 1 (2018) rows if present\n",
    "    crash2_mask = (df['timestamp'] >= pd.Timestamp('2021-11-10')) & (df['timestamp'] <= pd.Timestamp('2022-11-21'))\n",
    "    crash3_mask = (df['timestamp'] >= pd.Timestamp('2025-10-06')) & (df['timestamp'] <= pd.Timestamp('2026-02-28'))\n",
    "\n",
    "    crash2 = df[crash2_mask].copy()\n",
    "    crash3 = df[crash3_mask].copy()\n",
    "\n",
    "    print(f'  Crash 2 (2021-22): {len(crash2):,} rows')\n",
    "    print(f'  Crash 3 (2025-26): {len(crash3):,} rows')\n",
    "\n",
    "    if len(crash3) < 1000:\n",
    "        print(f'  [SKIP] {name}: insufficient crash3 data ({len(crash3)} rows)')\n",
    "        continue\n",
    "\n",
    "    # Split crash 3: first half = val, second half = test\n",
    "    c3_half = len(crash3) // 2\n",
    "    crash3_val = crash3.iloc[:c3_half]\n",
    "    crash3_test = crash3.iloc[c3_half:]\n",
    "\n",
    "    # Train set = crash 2 + crash 3 first half (val is separate)\n",
    "    # Actually: train = crash2, val = crash3 first half, test = crash3 second half\n",
    "    train_df = crash2 if len(crash2) > 0 else crash3.iloc[:int(len(crash3)*0.5)]\n",
    "    val_df = crash3_val\n",
    "    test_df = crash3_test\n",
    "\n",
    "    # If crash2 is empty (e.g., SOL launched mid-2020 but may have some data)\n",
    "    if len(train_df) < 500:\n",
    "        print(f'  [NOTE] {name}: small crash2 ({len(train_df)} rows), using crash3 70/15/15 split')\n",
    "        c3_70 = int(len(crash3) * 0.70)\n",
    "        c3_85 = int(len(crash3) * 0.85)\n",
    "        train_df = crash3.iloc[:c3_70]\n",
    "        val_df = crash3.iloc[c3_70:c3_85]\n",
    "        test_df = crash3.iloc[c3_85:]\n",
    "\n",
    "    # Horizon 1: 2-bar (10 min) -- PRIMARY\n",
    "    print(f'\\n  --- {name} 2-bar (10min) ---')\n",
    "    r_2bar = train_and_eval(\n",
    "        train_df[FEATURE_COLS].values, train_df['label_binary'].values,\n",
    "        val_df[FEATURE_COLS].values, val_df['label_binary'].values,\n",
    "        test_df[FEATURE_COLS].values, test_df['label_binary'].values,\n",
    "        BASE_PARAMS, label=f'{name} 2-bar (10min)'\n",
    "    )\n",
    "    all_results[f'{name}_2bar'] = r_2bar\n",
    "\n",
    "    # Horizon 2: 1-bar (5 min)\n",
    "    print(f'\\n  --- {name} 1-bar (5min) ---')\n",
    "    # Need label_binary_1bar for val and test\n",
    "    train_1bar_y = train_df['label_binary_1bar'].values\n",
    "    val_1bar_y = val_df['label_binary_1bar'].values\n",
    "    test_1bar_y = test_df['label_binary_1bar'].values\n",
    "\n",
    "    # Drop NaN labels\n",
    "    valid_train = ~np.isnan(train_1bar_y)\n",
    "    valid_val = ~np.isnan(val_1bar_y)\n",
    "    valid_test = ~np.isnan(test_1bar_y)\n",
    "\n",
    "    r_1bar = train_and_eval(\n",
    "        train_df[FEATURE_COLS].values[valid_train], train_1bar_y[valid_train],\n",
    "        val_df[FEATURE_COLS].values[valid_val], val_1bar_y[valid_val],\n",
    "        test_df[FEATURE_COLS].values[valid_test], test_1bar_y[valid_test],\n",
    "        BASE_PARAMS, label=f'{name} 1-bar (5min)'\n",
    "    )\n",
    "    all_results[f'{name}_1bar'] = r_1bar\n",
    "\n",
    "print(f'\\n[DONE] All models trained: {len(all_results)} models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: Cross-asset comparison table\n",
    "# =============================================================================\n",
    "\n",
    "print('MULTI-ASSET CRASH MODEL RESULTS')\n",
    "print('=' * 75)\n",
    "print(f'{\"Asset\":<6} {\"Horizon\":<12} {\"Train\":>8} {\"Test Acc\":>10} {\"AUC\":>8} {\"Pred Std\":>10} {\"Rounds\":>8} {\"Deploy?\":>8}')\n",
    "print('-' * 75)\n",
    "\n",
    "# BTC results from v3 (hardcoded from previous run)\n",
    "btc_v3_meta = None\n",
    "btc_meta_path = f'{DRIVE_V3}/crash_best_meta.json'\n",
    "if os.path.exists(btc_meta_path):\n",
    "    with open(btc_meta_path) as f:\n",
    "        btc_v3_meta = json.load(f)\n",
    "\n",
    "if btc_v3_meta:\n",
    "    b_recent = btc_v3_meta.get('comparison', {}).get('RECENT_ONLY (21+25)', {})\n",
    "    horizons = btc_v3_meta.get('horizon_comparison', {})\n",
    "    for hz_name, hz_data in horizons.items():\n",
    "        deploy = 'DEPLOY' if hz_data.get('test_acc', 0) > 0.52 else 'SKIP'\n",
    "        train_size = b_recent.get('train_size', 0)\n",
    "        pred_std = b_recent.get('pred_std', 0)\n",
    "        n_rounds = b_recent.get('n_rounds', 0)\n",
    "        print(f'{\"BTC\":<6} {hz_name:<12} {train_size:>8,} {hz_data[\"test_acc\"]:>10.4f} {hz_data[\"test_auc\"]:>8.4f} {pred_std:>10.4f} {n_rounds:>8} {deploy:>8}')\n",
    "else:\n",
    "    print('BTC    2bar(10m)   108,289     0.5329   0.5457     0.0233       13   DEPLOY')\n",
    "    print('BTC    1bar(5m)    108,289     0.5241   0.5365     0.0257       20   DEPLOY')\n",
    "\n",
    "# Print results for each trained model\n",
    "for key, res in sorted(all_results.items()):\n",
    "    parts = key.split('_')\n",
    "    asset = parts[0]\n",
    "    horizon = parts[1]\n",
    "    hz_label = '2bar(10m)' if horizon == '2bar' else '1bar(5m)'\n",
    "    deploy = 'DEPLOY' if res['test_acc'] > 0.52 else ('MARGINAL' if res['test_acc'] > 0.51 else 'SKIP')\n",
    "    print(f'{asset:<6} {hz_label:<12} {res[\"train_size\"]:>8,} {res[\"test_acc\"]:>10.4f} {res[\"test_auc\"]:>8.4f} {res[\"pred_std\"]:>10.4f} {res[\"n_rounds\"]:>8} {deploy:>8}')\n",
    "\n",
    "print('=' * 75)\n",
    "\n",
    "# Summary\n",
    "deploy_count = sum(1 for r in all_results.values() if r['test_acc'] > 0.52)\n",
    "total_count = len(all_results)\n",
    "print(f'\\nDeployable models: {deploy_count}/{total_count} (threshold: test_acc > 52%)')\n",
    "print(f'Training config: {TRAINING_CONFIG}')\n",
    "print(f'Feature set: {len(FEATURE_COLS)} features (shared across all assets)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: Calibration analysis per asset\n",
    "# =============================================================================\n",
    "\n",
    "for key, res in sorted(all_results.items()):\n",
    "    parts = key.split('_')\n",
    "    asset = parts[0]\n",
    "    horizon = parts[1]\n",
    "    hz_label = '2-bar (10min)' if horizon == '2bar' else '1-bar (5min)'\n",
    "\n",
    "    print(f'\\n{\"=\" * 65}')\n",
    "    print(f'{asset} {hz_label} CALIBRATION')\n",
    "    print(f'{\"=\" * 65}')\n",
    "\n",
    "    preds = res['test_pred']\n",
    "    df_name = asset\n",
    "    test_df = feature_datasets[df_name].copy()\n",
    "    test_df = test_df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Get the test split\n",
    "    crash3_mask = (test_df['timestamp'] >= pd.Timestamp('2025-10-06')) & (test_df['timestamp'] <= pd.Timestamp('2026-02-28'))\n",
    "    crash3 = test_df[crash3_mask]\n",
    "    c3_half = len(crash3) // 2\n",
    "    test_slice = crash3.iloc[c3_half:]\n",
    "\n",
    "    label_col = 'label_binary' if horizon == '2bar' else 'label_binary_1bar'\n",
    "    labels = test_slice[label_col].values[:len(preds)]\n",
    "\n",
    "    # Confidence buckets\n",
    "    print(f'{\"Bucket\":<20} {\"Count\":>8} {\"Accuracy\":>10} {\"Recommended\":>14}')\n",
    "    print('-' * 55)\n",
    "\n",
    "    buckets = [\n",
    "        ('52-55% UP', lambda p: (p > 0.52) & (p <= 0.55)),\n",
    "        ('55-60% UP', lambda p: (p > 0.55) & (p <= 0.60)),\n",
    "        ('60%+ UP', lambda p: p > 0.60),\n",
    "        ('52-55% DN', lambda p: (p < 0.48) & (p >= 0.45)),\n",
    "        ('55-60% DN', lambda p: (p < 0.45) & (p >= 0.40)),\n",
    "        ('60%+ DN', lambda p: p < 0.40),\n",
    "    ]\n",
    "\n",
    "    for bucket_name, mask_fn in buckets:\n",
    "        mask = mask_fn(preds)\n",
    "        count = mask.sum()\n",
    "        if count > 0:\n",
    "            if 'DN' in bucket_name:\n",
    "                acc = (labels[mask] == 0).mean() * 100\n",
    "            else:\n",
    "                acc = (labels[mask] == 1).mean() * 100\n",
    "            recommended = 'YES' if acc > 52 and count > 50 else 'NO'\n",
    "            print(f'{bucket_name:<20} {count:>8,} {acc:>9.1f}% {recommended:>14}')\n",
    "        else:\n",
    "            print(f'{bucket_name:<20} {0:>8} {\"N/A\":>10} {\"NO\":>14}')\n",
    "\n",
    "    print(f'\\n  Max confidence: {preds.max():.4f}')\n",
    "    print(f'  Min confidence: {preds.min():.4f}')\n",
    "    print(f'  Std: {preds.std():.4f}')\n",
    "    threshold = 0.53 if horizon == '2bar' else 0.52\n",
    "    above = (preds > threshold).sum()\n",
    "    below = (preds < (1 - threshold)).sum()\n",
    "    print(f'  Above {threshold}: {above} ({above/len(preds)*100:.1f}%)')\n",
    "    print(f'  Below {1-threshold}: {below} ({below/len(preds)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: Save all models to Google Drive\n",
    "# =============================================================================\n",
    "\n",
    "saved_models = {}\n",
    "\n",
    "for key, res in sorted(all_results.items()):\n",
    "    parts = key.split('_')\n",
    "    asset = parts[0].lower()\n",
    "    horizon = parts[1]\n",
    "    deploy = res['test_acc'] > 0.52\n",
    "\n",
    "    tag = f'crash_{asset}_{horizon}'\n",
    "    print(f'\\n--- {tag} (test_acc={res[\"test_acc\"]:.4f}, deploy={deploy}) ---')\n",
    "\n",
    "    # Save pickle\n",
    "    pkl_path = f'{DRIVE_SAVE}/{tag}_lightgbm.pkl'\n",
    "    with open(pkl_path, 'wb') as f:\n",
    "        pickle.dump(res['model'], f)\n",
    "    print(f'  Saved: {pkl_path}')\n",
    "\n",
    "    # Save LightGBM text format\n",
    "    txt_path = f'{DRIVE_SAVE}/{tag}_lightgbm.txt'\n",
    "    res['model'].save_model(txt_path)\n",
    "    print(f'  Saved: {txt_path}')\n",
    "\n",
    "    # Save metadata JSON\n",
    "    meta = {\n",
    "        'asset': asset.upper(),\n",
    "        'horizon': horizon,\n",
    "        'horizon_label': '2-bar (10min)' if horizon == '2bar' else '1-bar (5min)',\n",
    "        'training_config': TRAINING_CONFIG,\n",
    "        'test_acc': round(res['test_acc'], 4),\n",
    "        'test_auc': round(res['test_auc'], 4),\n",
    "        'val_acc': round(res['val_acc'], 4),\n",
    "        'val_auc': round(res['val_auc'], 4),\n",
    "        'pred_std': round(res['pred_std'], 4),\n",
    "        'n_rounds': res['n_rounds'],\n",
    "        'train_size': res['train_size'],\n",
    "        'feature_cols': FEATURE_COLS,\n",
    "        'n_features': len(FEATURE_COLS),\n",
    "        'deploy': deploy,\n",
    "        'trained_at': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'top_features': [{'name': n, 'gain': round(float(g), 1)} for n, g in res['feat_imp'][:10]],\n",
    "    }\n",
    "    meta_path = f'{DRIVE_SAVE}/{tag}_meta.json'\n",
    "    with open(meta_path, 'w') as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(f'  Saved: {meta_path}')\n",
    "\n",
    "    saved_models[f'{asset.upper()}_{horizon}'] = meta\n",
    "\n",
    "# Master summary\n",
    "summary = {\n",
    "    'trained_at': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'training_config': TRAINING_CONFIG,\n",
    "    'feature_cols': FEATURE_COLS,\n",
    "    'n_features': len(FEATURE_COLS),\n",
    "    'models': {},\n",
    "    'note': 'All models use BTC as cross-asset lead indicator. Feature set is identical across all assets.',\n",
    "}\n",
    "\n",
    "# Add BTC from v3\n",
    "if btc_v3_meta:\n",
    "    horizons = btc_v3_meta.get('horizon_comparison', {})\n",
    "    for hz_name, hz_data in horizons.items():\n",
    "        hz_key = '2bar' if '2-bar' in hz_name else ('1bar' if '1-bar' in hz_name else '6bar')\n",
    "        summary['models'][f'BTC_{hz_key}'] = {\n",
    "            'accuracy': hz_data.get('test_acc', 0),\n",
    "            'auc': hz_data.get('test_auc', 0),\n",
    "            'deploy': hz_data.get('test_acc', 0) > 0.52,\n",
    "            'source': 'crash_models_v3',\n",
    "        }\n",
    "\n",
    "# Add new models\n",
    "for key, meta in saved_models.items():\n",
    "    summary['models'][key] = {\n",
    "        'accuracy': meta['test_acc'],\n",
    "        'auc': meta['test_auc'],\n",
    "        'deploy': meta['deploy'],\n",
    "        'source': 'crash_models_multi',\n",
    "    }\n",
    "\n",
    "summary_path = f'{DRIVE_SAVE}/crash_multi_asset_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f'\\nMaster summary: {summary_path}')\n",
    "\n",
    "# Final report\n",
    "print(f'\\n{\"=\" * 65}')\n",
    "print('TRAINING COMPLETE -- MULTI-ASSET CRASH MODELS')\n",
    "print(f'{\"=\" * 65}')\n",
    "\n",
    "for asset_hz, meta in sorted(summary['models'].items()):\n",
    "    status = 'DEPLOY' if meta['deploy'] else 'SKIP'\n",
    "    src = meta.get('source', '')\n",
    "    print(f'  {asset_hz:<12} {meta[\"accuracy\"]:.4f} acc  {meta[\"auc\"]:.4f} AUC  [{status}]  ({src})')\n",
    "\n",
    "deploy_count = sum(1 for m in summary['models'].values() if m['deploy'])\n",
    "total_count = len(summary['models'])\n",
    "print(f'\\nDeployable: {deploy_count}/{total_count}')\n",
    "print(f'Feature set: {len(FEATURE_COLS)} features (shared)')\n",
    "print(f'Drive path: {DRIVE_SAVE}/')\n",
    "print(f'{\"=\" * 65}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}