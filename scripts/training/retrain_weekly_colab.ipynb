{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Renaissance Trading Bot — ML Model Training (Colab)\n\nGPU-accelerated training for all 7 ML models:\n- **Quantum Transformer** (83→288→4 blocks→1)\n- **Bidirectional LSTM** (83→292 bidir→1)\n- **Dilated CNN** (83 channels→5 blocks→332→1)\n- **Simple CNN** + **Bidirectional GRU**\n- **Meta-Ensemble** (stacking layer)\n- **VAE** (anomaly detector)\n\nSupports two data modes:\n- **30-day**: Quick retrain with recent Coinbase data\n- **Historical (5+ years)**: Full training with pre-downloaded Binance data from `data/training/`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Install dependencies\n!pip install torch numpy pandas ccxt\n\nimport torch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Using device: {device}')\nif device == 'cuda':\n    print(f'GPU: {torch.cuda.get_device_name(0)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Upload project files\n",
    "# Option A: Mount Google Drive (if project is in Drive)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/bitcoin-trading-bot-renaissance\n",
    "\n",
    "# Option B: Upload a zip of the project\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload project.zip\n",
    "# !unzip project.zip -d /content/project\n",
    "# %cd /content/project\n",
    "\n",
    "# Option C: Clone from git (if available)\n",
    "# !git clone <your-repo-url> /content/project\n",
    "# %cd /content/project\n",
    "\n",
    "import sys, os\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "print(f'Project root: {PROJECT_ROOT}')\n",
    "print(f'Files: {os.listdir(\".\")[:20]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Choose data source and load training data\nimport os, logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n\n# ── Option A: Use pre-downloaded historical CSVs (5+ years, ~240MB) ──\n# Upload CSVs to data/training/ (e.g., BTC-USD_5m_historical.csv)\n# These should have been downloaded locally via:\n#   python -m scripts.training.fetch_historical_data\n\nUSE_HISTORICAL = True  # Set to False for 30-day Coinbase data\n\nif USE_HISTORICAL:\n    from scripts.training.fetch_historical_data import load_historical_csvs\n    pair_dfs = load_historical_csvs()\n    if not pair_dfs:\n        print(\"No historical CSVs found in data/training/. Falling back to 30-day download.\")\n        USE_HISTORICAL = False\n\nif not USE_HISTORICAL:\n    # ── Option B: Download 30 days from Coinbase (quick retrain) ──\n    from scripts.training.fetch_training_data import download_all\n    PAIRS = ['BTC-USD', 'ETH-USD', 'SOL-USD', 'DOGE-USD', 'AVAX-USD', 'LINK-USD']\n    pair_dfs = download_all(PAIRS, days=30)\n\nprint(f'\\nLoaded data:')\ntotal = 0\nfor pair, df in sorted(pair_dfs.items()):\n    days_of_data = len(df) * 5 / 60 / 24\n    total += len(df)\n    print(f'  {pair}: {len(df):,} candles ({days_of_data:.0f} days)')\nprint(f'\\nTotal: {total:,} candles')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Train all models (GPU-accelerated)\nfrom scripts.training.train_all import train_all\n\nEPOCHS = 100  # Reduce for quick test: EPOCHS = 5\n\n# Use --historical mode if we loaded historical CSVs\nresults = train_all(\n    days=30,\n    epochs=EPOCHS,\n    historical=USE_HISTORICAL,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Display results\n",
    "import json\n",
    "\n",
    "print('Training Results')\n",
    "print('=' * 60)\n",
    "for model_name, r in results.items():\n",
    "    print(f'\\n{model_name}:')\n",
    "    for k, v in r.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f'  {k}: {v:.4f}')\n",
    "        else:\n",
    "            print(f'  {k}: {v}')\n",
    "\n",
    "# Check model files\n",
    "import os\n",
    "models_dir = 'models/trained'\n",
    "print(f'\\nModel files in {models_dir}:')\n",
    "for f in sorted(os.listdir(models_dir)):\n",
    "    if f.endswith('.pth'):\n",
    "        size_kb = os.path.getsize(os.path.join(models_dir, f)) / 1024\n",
    "        print(f'  {f}: {size_kb:.1f} KB')\n",
    "\n",
    "# Verify models load correctly\n",
    "from ml_model_loader import load_trained_models\n",
    "models = load_trained_models()\n",
    "print(f'\\nLoaded models: {list(models.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Download trained .pth files\n",
    "# Option A: Download directly (Colab)\n",
    "# from google.colab import files\n",
    "# for f in os.listdir('models/trained'):\n",
    "#     if f.endswith('.pth'):\n",
    "#         files.download(os.path.join('models/trained', f))\n",
    "\n",
    "# Option B: Copy to Google Drive\n",
    "# import shutil\n",
    "# drive_dest = '/content/drive/MyDrive/trained_models/'\n",
    "# os.makedirs(drive_dest, exist_ok=True)\n",
    "# for f in os.listdir('models/trained'):\n",
    "#     if f.endswith('.pth'):\n",
    "#         shutil.copy2(os.path.join('models/trained', f), drive_dest)\n",
    "#         print(f'Copied {f} to Drive')\n",
    "\n",
    "print('Uncomment the download method above to save your trained models.')\n",
    "print('\\nModel files:')\n",
    "for f in sorted(os.listdir('models/trained')):\n",
    "    if f.endswith('.pth'):\n",
    "        print(f'  {f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}